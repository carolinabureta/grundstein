{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Introduction", "text": ""}, {"location": "#home-sweet-home", "title": "Home sweet home", "text": "<p>this is my personal place to order my knowledge. Some knowledge might be useful for you.</p> <p>I keep stuff here to remember hopefully. I am the real Carolina Bureta, La Bastelaria and this is, what I know and how I do it. This book is forked from the Blue Book.</p>"}, {"location": "abstract_syntax_trees/", "title": "Abstract syntax trees", "text": "<p>Abstract syntax trees (AST) is a tree representation of the abstract syntactic structure of text (often source code) written in a formal language. Each node of the tree denotes a construct occurring in the text.</p> <p>The syntax is \"abstract\" in the sense that it does not represent every detail appearing in the real syntax, but rather just the structural or content-related details. For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then statement may be denoted by means of a single node with three branches.</p> <p>This distinguishes abstract syntax trees from concrete syntax trees, traditionally designated parse trees. Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis.</p> <p>Abstract syntax trees are also used in program analysis and program transformation systems.</p>"}, {"location": "abstract_syntax_trees/#how-to-construct-an-ast", "title": "How to construct an AST", "text": "<p>TBD</p> <p><code>pyparsing</code> looks to be a good candidate to implement ASTs.</p>"}, {"location": "activitywatch/", "title": "ActivityWatch", "text": "<p>ActivityWatch is a bundle of software that tracks your computer activity. You are, by default, the sole owner of your data.</p> <p>ActivityWatch is:</p> <ul> <li>A set of watchers that record relevant information about what you do and what     happens on your computer (such as if you are AFK or not, or which window is     currently active).</li> <li>A way of storing data collected by the watchers.</li> <li>A dataformat accomodating most logging needs due to its flexibility.</li> <li>An ecosystem of tools to help users extend the software to fit their needs.</li> </ul>"}, {"location": "activitywatch/#installation", "title": "Installation", "text": "<ul> <li>Download the latest release</li> <li>Unpack it and move it for example to <code>~/.local/bin/activitywatch</code>.</li> <li>Add the <code>aw-qt</code> executable to the autostart.</li> </ul> <p>It will start the web interface at http://localhost:5600 and will capture the data.</p>"}, {"location": "activitywatch/#configuration", "title": "Configuration", "text": "<p>First go to the <code>settings</code> page of the Web UI, you can define there the rules for the categories.</p> <p>More advanced settings can be changed on the files, but I had no need to go there yet.</p> <p>The used directories are:</p> <ul> <li>Data: <code>~/.local/share/activitywatch</code>.</li> <li>Config: <code>~/.config/activitywatch</code>.</li> <li>Logs: <code>~/.cache/activitywatch/log</code>.</li> <li>Cache: <code>~/.cache/activitywatch</code>.</li> </ul>"}, {"location": "activitywatch/#watchers", "title": "Watchers", "text": "<p>By default ActivityWatch comes with the next watchers:</p> <ul> <li>aw-watcher-afk: Watches for     mouse &amp; keyboard activity to detect if the user is active.</li> <li>aw-watcher-window:     Watches the active window and its title.</li> </ul> <p>But you can add more, such as:</p> <ul> <li> <p>aw-watcher-web: The     official browser extension, supports Chrome and Firefox. Watches properties     of the active tab like title, URL, and incognito state.</p> <p>It doesn't work if you Configure it to Never remember history, or if you use incognito mode</p> <p>It's known not to be very accurate. The overall time spent in the browser shown by the <code>aw-watcher-window</code> is greater than the one shown in <code>aw-watcher-web-firefox</code>.</p> </li> <li> <p>aw-watcher-vim: Watches the     actively edited file and associated metadata like path, language, and     project name (folder name of git root).</p> <p>It's impressive, plug and play:</p> <p></p> <p>It still doesn't add the branch information, it could be useful to give hints of what task you're working on inside a project.</p> </li> </ul> <p>They even show you how to create your own watcher.</p>"}, {"location": "activitywatch/#syncing", "title": "Syncing", "text": "<p>There is currently no syncing support. You'll need to export the data (under <code>Raw Data</code>, <code>Export all buckets as JSON</code>), and either tweak it so it can be imported, or analyze the data through other processes.</p>"}, {"location": "activitywatch/#issues", "title": "Issues", "text": "<ul> <li>Syncing support:     See how to merge the data from the different devices.</li> <li>Firefox not logging     data: Once it's     solved, try it again.</li> <li>Making it work in incognito     mode: Try it once     it's solved.</li> <li>Add branch information in vim     watcher: try it     once it's out.</li> <li>Web tracking is not     accurate: Test     the solution once it's implemented.</li> <li>Physical Activity Monitor Integration     (gadgetbridge):     Try it once there is a solution.</li> </ul>"}, {"location": "activitywatch/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "adr/", "title": "ADR", "text": "<p>ADR are short text documents that captures an important architectural decision made along with its context and consequences.</p> <p>The whole document should be one or two pages long. Written as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments.</p> <p>Pros:</p> <ul> <li>We have a clear log of the different decisions taken, which can help newcomers     to understand past decisions.</li> <li>It can help in the discussion of such changes.</li> <li>Architecture decisions recorded in small modular readable documents.</li> </ul> <p>Cons:</p> <ul> <li>More time is required for each change, as we need to document and discuss it.</li> </ul>"}, {"location": "adr/#how-to-use-them", "title": "How to use them", "text": "<p>We will keep a collection of architecturally significant decisions, those that affect the structure, non-functional characteristics, dependencies, interfaces or construction techniques.</p> <p>There are different templates you can start with, being the most popular Michael Nygard's one.</p> <p>The documents are stored in the project repository under the <code>doc/arch</code> directory, with a name convention of <code>NNN-title_with_underscores.md</code>, where <code>NNN</code> is a monotonically increasing number.</p> <p>If a decision is reversed, we'll keep the old one around, but mark it as superseded, as it's still relevant to know that it was a decision, but is no longer.</p>"}, {"location": "adr/#adr-template", "title": "ADR template", "text": "<p>Using Michael Nygard's template as a starting point, I'm going to use these sections:</p> <ul> <li>Title: A short noun phrase that describes the change. For example, \"ADR 1:     Deployment on Ruby on Rails 3.0.10\".</li> <li>Date: Creation date of the document.</li> <li>Status: The ADRs go through the following phases:<ul> <li>Draft: We are using the ADR to build the idea, so everything can change.</li> <li>Proposed: We have a solid proposal on the Decision to solve the Context.</li> <li>Accepted: We have agreed on the proposal, from now on the document can't     be changed!</li> <li>Rejected: We have agreed not to solve the Context.</li> <li>Deprecated: The Context no longer applies, so the solution is no longer     needed.</li> <li>Superseded: We have found another ADR that better solves the Context.</li> </ul> </li> <li>Context: This section describes the situation we're trying to solve,     including technological, political, social, and project local aspects. The     language in this section is neutral, simply describing facts.</li> <li>Proposals: Analysis of the different solutions for the situation defined     in the Context.</li> <li>Decision: Clear summary of the selected proposal. It is stated in full     sentences, with active voice.</li> <li>Consequences: Description of the resulting context, after applying     the decision. All consequences should be listed here, not just the positive     ones.</li> </ul> <p>I'm using the following Ultisnip vim snippet:</p> <pre><code>snippet adr \"ADR\"\nDate: `date +%Y-%m-%d`\n\n# Status\n&lt;!-- What is the status? Draft, Proposed, Accepted, Rejected, Deprecated or Superseded?\n--&gt;\n$1\n\n# Context\n&lt;!-- What is the issue that we're seeing that is motivating this decision or change? --&gt;\n$0\n\n# Proposals\n&lt;!-- What are the possible solutions to the problem described in the context --&gt;\n\n# Decision\n&lt;!-- What is the change that we're proposing and/or doing? --&gt;\n\n# Consequences\n&lt;!-- What becomes easier or more difficult to do because of this change? --&gt;\nendsnippet\n</code></pre>"}, {"location": "adr/#usage-in-a-project", "title": "Usage in a project", "text": "<p>When starting a project, I'll do it by the ADRs, that way you evaluate the problem, structure the idea and leave a record of your initial train of thought.</p> <p>I found useful to:</p> <ul> <li> <p>Define the general problem at high level in     <code>001-high_level_problem_analysis.md</code>.</p> <ul> <li>Describe the problem you want to solve in the Context.</li> <li>Reflect the key points to solve the problem at the start of the Proposals     section. Go one by one analyzing possible outcomes trying not to dive deep     into details and having at least two proposals for each key point (hard!).</li> <li>Build an initial proposal in the Decision section by reviewing that all     the Context points have been addressed and summarizing each of the     Proposal key points' outcomes.</li> <li>Review the positive and negative Consequences for each actor involved with     the solution, such as:<ul> <li>The final user that is going to consume the outcome.</li> <li>The middle user that is going to host and maintain the solution.</li> <li>Ourselves as developers.</li> </ul> </li> <li>Use the problem definition of <code>001</code> and draft the phases of the solution at <code>002</code>.</li> <li>Create another ADR for each of the phases, getting a level closer to the final implementation.</li> </ul> </li> <li> <p>Use <code>00X</code> for the early drafts. Once you give it a number try not to change     the file name, or you'll need to manually update the references you make.</p> </li> </ul> <p>As the project starts to grow, the relationships between the ADRs will get more complex, it's useful to create an ADR landing page, where the user can follow the logic between them. MermaidJS can be used to create a nice diagram that shows this information.</p> <p>In the mkdocs-newsletter I've used the next structure:</p> <pre><code>graph TD\n    001[001: High level analysis]\n    002[002: Initial MkDocs plugin design]\n    003[003: Selected changes to record]\n    004[004: Article newsletter structure]\n    005[005: Article newsletter creation]\n\n    001 -- Extended --&gt; 002\n    002 -- Extended --&gt; 003\n    002 -- Extended --&gt; 004\n    002 -- Extended --&gt; 005\n    003 -- Extended --&gt; 004\n    004 -- Extended --&gt; 005\n\n    click 001 \"https://lyz-code.github.io/mkdocs-newsletter/adr/001-initial_approach\" _blank\n    click 002 \"https://lyz-code.github.io/mkdocs-newsletter/adr/002-initial_plugin_design\" _blank\n    click 003 \"https://lyz-code.github.io/mkdocs-newsletter/adr/003-select_the_changes_to_record\" _blank\n    click 004 \"https://lyz-code.github.io/mkdocs-newsletter/adr/004-article_newsletter_structure\" _blank\n    click 005 \"https://lyz-code.github.io/mkdocs-newsletter/adr/005-create_the_newsletter_articles\" _blank\n\n    001:::accepted\n    002:::accepted\n    003:::accepted\n    004:::accepted\n    005:::accepted\n\n    classDef draft fill:#CDBFEA;\n    classDef proposed fill:#B1CCE8;\n    classDef accepted fill:#B1E8BA;\n    classDef rejected fill:#E8B1B1;\n    classDef deprecated fill:#E8B1B1;\n    classDef superseeded fill:#E8E5B1;\n</code></pre> <p>Where we define:</p> <ul> <li>The nodes with their title.</li> <li>The relationship between the ADRs.</li> <li>The link to the ADR article so it can be clicked.</li> <li>The state of the ADR.</li> </ul>"}, {"location": "adr/#tools", "title": "Tools", "text": "<p>Although adr-tools exist, I feel it's an overkill to create new documents and search on an existing codebase. We are now used to using other tools for the similar purpose, like Vim or grep.</p>"}, {"location": "adr/#references", "title": "References", "text": "<ul> <li>Joel Parker guide on ADRs</li> <li>Michael Nygard post on ARDs</li> </ul>"}, {"location": "aerial_silk/", "title": "Aerial Silk", "text": "<p>Aerial Silk is a type of performance in which one or more artists perform aerial acrobatics while hanging from a fabric. The fabric may be hung as two pieces, or a single piece, folded to make a loop, classified as hammock silks. Performers climb the suspended fabric without the use of safety lines and rely only on their training and skill to ensure safety. They use the fabric to wrap, suspend, drop, swing, and spiral their bodies into and out of various positions. Aerial silks may be used to fly through the air, striking poses and figures while flying. Some performers use dried or spray rosin on their hands and feet to increase the friction and grip on the fabric.</p>"}, {"location": "aerial_silk/#warming-up", "title": "Warming up", "text": ""}, {"location": "aerial_silk/#arm-twist", "title": "Arm twist", "text": "<p>. Leave the silk at your left, take it with the left hand with your arm straight up and you thumb pointing away from you. . Start twisting in <code>z &gt; 0</code> your arm from your shoulder until the thumb points to your front. . Keep on twisting and follow the movement with the rest of your upper body until you're hanging from that arm and stretching. You shouldn't move your feet in the whole process. . Repeat with the other arm.</p>"}, {"location": "aerial_silk/#ball-controlled-inversions", "title": "Ball controlled inversions", "text": "<p>. From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Bend your knees so that you become a ball and from that position. . While alive: . Keep on rotating 90 degrees more until your shins are parallel to the ground facing down. . Change the direction of the rotation and rotate 180 degrees until your shins are parallel to the ground but facing up.</p>"}, {"location": "aerial_silk/#inverted-arm-twist-warmup", "title": "Inverted arm twist warmup", "text": "<p>. From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide, while doing so start twisting your arms at shoulder level so that your chest goes to your back in a cat like position until your body limit. . Go back up doing the twist in the other direction till you're back up.</p>"}, {"location": "aerial_silk/#inverted-knee-to-elbow", "title": "Inverted knee to elbow", "text": "<p>. From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide for your knees until they are at elbow level. Don't bend your knees! . Go back up.</p>"}, {"location": "aerial_silk/#horizontal-pull-ups", "title": "Horizontal pull-ups", "text": "<p>. From standing position with each hand in a tissue, ask for a partner to take your legs until you're horizontal hanging from your hands and feet. . While alive: . Keeping your body straight, do a pull up with your arms. . Slowly unbend your elbows and stretch back your arms.</p> <p>The next level would be that you use your leg strength to grip your partner's hips instead of her holding your feet.</p>"}, {"location": "aerial_silk/#basic-movements", "title": "Basic movements", "text": ""}, {"location": "aerial_silk/#splitting-the-tissues", "title": "Splitting the tissues", "text": "<p>. Go to the desired height and get into a comfortable position, such as seated over your feet on a Russian climb. . Open the palm of one hand at eye level holding the whole tissue . Keep your bellybutton close to the tissue . With the other hand pinch the side of the silk and start walking with your fingers till you find the break between tissues.</p>"}, {"location": "aerial_silk/#figures", "title": "Figures", "text": ""}, {"location": "aerial_silk/#waist-lock", "title": "Waist lock", "text": "<p>. Climb to the desired height . Leave the tissue to the side you want to do the lock to, for example the left (try the other side as well). . Leave your right hand above your left, with your arm straight. Your left hand should be at breast level with your elbow bent. . With straight legs with pointed toes, bring your left leg a little bit to the front, while the right one catches the silk . Bright the right leg up trying to get the silk as close to your waist as possible. . Rotate your body in <code>z&gt;0</code> towards your left hand until you're looking down . Release your hands.</p>"}, {"location": "aerial_silk/#ideas", "title": "Ideas", "text": ""}, {"location": "aerial_silk/#locking-shoulder-blades-when-gripping-the-silks", "title": "Locking shoulder blades when gripping the silks", "text": "<p>When you are going to hang yourself from your hands:</p> <p>. Unlock your shoulder blades moving your chest to the back. . Embrace the silk with your arms twisting your hands inwards . Grip the silk and lock back your shoulder blades together as if you were holding an apple between them. That movement will make your hands twist in the other direction until your wrists are between you and the tissue.</p>"}, {"location": "aerial_silk/#safety", "title": "Safety", "text": "<ul> <li>When rolling up silk over your legs, always leave room for the knee, do loops     above and below but never over.</li> </ul>"}, {"location": "afew/", "title": "afew", "text": "<p>afew is an initial tagging script for notmuch mail.</p> <p>Its basic task is to provide automatic tagging each time new mail is registered with <code>notmuch</code>. In a classic setup, you might call it after <code>notmuch new</code> in an offlineimap post sync hook.</p> <p>It can do basic thing such as adding tags based on email headers or maildir folders, handling killed threads and spam.</p> <p>In move mode, afew will move mails between maildir folders according to configurable rules that can contain arbitrary notmuch queries to match against any searchable attributes.</p>"}, {"location": "afew/#installation", "title": "Installation", "text": "<p>First install the requirements:</p> <pre><code>sudo apt-get install notmuch python-notmuch python-dev python-setuptools\n</code></pre> <p>Then configure <code>notmuch</code>.</p> <p>Finally install the program:</p> <pre><code>pip3 install afew\n</code></pre>"}, {"location": "afew/#usage", "title": "Usage", "text": "<p>To tag new emails use:</p> <pre><code>afew -v --tag --new\n</code></pre>"}, {"location": "afew/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "aleph/", "title": "Aleph", "text": "<p>Aleph is a tool for indexing large amounts of both documents (PDF, Word, HTML) and structured (CSV, XLS, SQL) data for easy browsing and search. It is built with investigative reporting as a primary use case. Aleph allows cross-referencing mentions of well-known entities (such as people and companies) against watchlists, e.g. from prior research or public datasets.</p>"}, {"location": "aleph/#install-the-development-environment", "title": "Install the development environment", "text": "<p>As a first step, check out the source code of Aleph from GitHub:</p> <pre><code>git clone https://github.com/alephdata/aleph.git\ncd aleph/\n</code></pre> <p>Also, please execute the following command to allow ElasticSearch to map its memory:</p> <pre><code>sysctl -w vm.max_map_count=262144\n</code></pre> <p>Then enable the use of <code>pdb</code> by adding the next lines into the <code>docker-compose.dev.yml</code> file, under the <code>api</code> service configuration.</p> <pre><code>stdin_open: true\ntty: true\n</code></pre> <p>With the settings in place, you can use <code>make all</code> to set everything up and launch the web service. This is equivalent to the following steps:</p> <ul> <li><code>make build</code> to build the docker images for the application and relevant   services.</li> <li><code>make upgrade</code> to run the latest database migrations and create/update the   search index.</li> <li><code>make web</code> to run the web-based API server and the user interface.</li> <li>In a separate shell, run <code>make worker</code> to start a worker. If you do not start   a worker, background jobs (for example ingesting new documents) won\u2019t be   processed.</li> </ul> <p>Open http://localhost:8080/ in your browser to visit the web frontend.</p> <ul> <li>Create a shell to do the operations with <code>make shell</code>.</li> <li>Create the main user within that shell running   <pre><code>aleph createuser --name=\"demo\" \\\n    --admin \\\n    --password=demo \\\n    demo@demo.com\n</code></pre></li> <li>Load some sample data by running <code>aleph crawldir /aleph/contrib/testdata</code></li> </ul>"}, {"location": "aleph/#debugging-the-code", "title": "Debugging the code", "text": "<p>To debug the code, you can create <code>pdb</code> breakpoints in the code you cloned, and run the actions that trigger the breakpoint. To be able to act on it, you need to be attached to the api by running:</p> <pre><code>docker attach aleph_api_1\n</code></pre> <p>You don't need to reload the page for it to load the changes, it does it dynamically.</p>"}, {"location": "aleph/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "aleph/#problems-accessing-redis-locally", "title": "Problems accessing redis locally", "text": "<p>If you're with the VPN connected, turn it off.</p>"}, {"location": "aleph/#pdb-behaves-weird", "title": "PDB behaves weird", "text": "<p>Sometimes you have two traces at the same time, so each time you run a PDB command it jumps from pdb trace. Quite confusing. Try to <code>c</code> the one you don't want so that you're left with the one you want. Or put the <code>pdb</code> trace in a conditional that only matches one of both threads.</p>"}, {"location": "aleph/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "alot/", "title": "alot", "text": "<p>alot is a terminal-based mail user agent based on the notmuch mail indexer. It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience.</p>"}, {"location": "alot/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install alot\n</code></pre>"}, {"location": "alot/#configuration", "title": "Configuration", "text": "<p>Alot reads the INI config file <code>~/.config/alot/config</code>. That file is not created by default, if you don't want to start from scratch, you can use pazz's alot configuration, in particular the <code>[accounts]</code> section.</p>"}, {"location": "alot/#ui-interaction", "title": "UI interaction", "text": "<p>Basic movement is done with:</p> <ul> <li>Move up and down: <code>j</code>/<code>k</code>, arrows and page up and page down.</li> <li>Cancel prompts: <code>Escape</code></li> <li>Select highlighted element: <code>Enter</code>.</li> <li>Update buffer: <code>@</code>.</li> </ul> <p>The interface shows one buffer at a time, basic buffer management is done with:</p> <ul> <li>Change buffer: <code>Tab</code> and <code>Shift-Tab</code>.</li> <li>Close the current buffer: <code>d</code></li> <li>List all buffers: <code>;</code>.</li> </ul> <p>The buffer type or mode (displayed at the bottom left) determines which prompt commands are available. Usage information on any command can be listed by typing <code>help YOURCOMMAND</code> to the prompt. The key bindings for the current mode are listed upon pressing <code>?</code>.</p> <p>You can always run commands with <code>:</code>.</p>"}, {"location": "alot/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "alot/#remove-emails", "title": "Remove emails", "text": "<p>Say you want to remove emails from the provider's server but keep them in the notmuch database. There is no straight way to do it, you need to tag them with a special tag like <code>deleted</code> and then remove them from the server with a post-hook.</p>"}, {"location": "alot/#theme-not-found", "title": "Theme not found", "text": "<p>I don't know why but <code>apt-get</code> didn't install the default themes, you need to create the <code>~/.config/alot/themes</code> and copy the contents of the themes directory.</p>"}, {"location": "alot/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Wiki</li> <li>FAQ</li> </ul>"}, {"location": "amazfit_band_5/", "title": "Amazfit Band 5", "text": "<p>Amazfit Band 5 it's the affordable fitness tracker I chose to buy because:</p> <ul> <li>It's supported by gadgetbridge.</li> <li>It has a SpO2 sensor which enhances the quality of the sleep metrics</li> <li>It has sleep metrics, not only time but also type of sleep (light, deep, REM).</li> <li>It has support with Alexa, not that I'd use that, but it would be cool if once     I've got my personal voice assistant, I can use it     through the band.</li> </ul>"}, {"location": "amazfit_band_5/#sleep-detection-quality", "title": "Sleep detection quality", "text": "<p>The sleep tracking using Gadgetbridge is not good at all. After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results.</p> <p>If it doesn't work either, I might think of getting a specific device like withings sleep analyzer which seems to have much more accuracy and useful insights. I've sent them an email to see if it's possible to extract the data before it reach their servers, and they confirmed that there is no way. Maybe you can route the requests to their servers to one of your own, bring up an http server and reverse engineer the communication.</p> <p>Karlicoss, the author of the awesome HPI uses the Emfit QS, so that could be another option.</p>"}, {"location": "amazfit_band_5/#firmware-updates", "title": "Firmware updates", "text": "<p>Gadgetbridge people have a guide on how to upgrade the firmware, you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post.</p>"}, {"location": "android_tips/", "title": "Android tips", "text": ""}, {"location": "android_tips/#extend-the-life-of-your-battery", "title": "Extend the life of your battery", "text": "<p>Research has shown that keeping your battery charged between 0% and 80% can make your battery's lifespan last 2x longer than when you use a full battery cycle from 0-100%.</p> <p>As a non root user you can install Accubattery (not in F-droid :( ) to get an alarm when the battery reaches 80% so that you can manually unplug it. Instead of leaving the mobile charge in the night and stay connected at 100% a lot of hours until you unplug, charge it throughout the day.</p>"}, {"location": "anki/", "title": "Anki", "text": "<p>Anki is a program which makes remembering things easy. Because it's a lot more efficient than traditional study methods, you can either greatly decrease your time spent studying, or greatly increase the amount you learn.</p> <p>Anyone who needs to remember things in their daily life can benefit from Anki. Since it is content-agnostic and supports images, audio, videos and scientific markup (via LaTeX), the possibilities are endless.</p>"}, {"location": "anki/#interacting-with-python", "title": "Interacting with python", "text": ""}, {"location": "anki/#configuration", "title": "Configuration", "text": "<p>Although there are some python libraries:</p> <ul> <li>genanki</li> <li>py-anki</li> </ul> <p>I think the best way is to use AnkiConnect</p> <p>The installation process is similar to other Anki plugins and can be accomplished in three steps:</p> <ul> <li>Open the Install Add-on dialog by selecting Tools | Add-ons | Get     Add-ons... in Anki.</li> <li>Input <code>2055492159</code> into the text box labeled Code and press the OK button to     proceed.</li> <li>Restart Anki when prompted to do so in order to complete the installation of     Anki-Connect.</li> </ul> <p>Anki must be kept running in the background in order for other applications to be able to use Anki-Connect. You can verify that Anki-Connect is running at any time by accessing <code>localhost:8765</code> in your browser. If the server is running, you will see the message Anki-Connect displayed in your browser window.</p>"}, {"location": "anki/#usage", "title": "Usage", "text": "<p>Every request consists of a JSON-encoded object containing an <code>action</code>, a <code>version</code>, contextual <code>params</code>, and a <code>key</code> value used for authentication (which is optional and can be omitted by default). Anki-Connect will respond with an object containing two fields: <code>result</code> and <code>error</code>. The <code>result</code> field contains the return value of the executed API, and the <code>error</code> field is a description of any exception thrown during API execution (the value <code>null</code> is used if execution completed successfully).</p> <p>Sample successful response:</p> <pre><code>{\"result\": [\"Default\", \"Filtered Deck 1\"], \"error\": null}\n</code></pre> <p>Samples of failed responses:</p> <pre><code>{\"result\": null, \"error\": \"unsupported action\"}\n\n{\"result\": null, \"error\": \"guiBrowse() got an unexpected keyword argument 'foobar'\"}\n</code></pre> <p>For compatibility with clients designed to work with older versions of Anki-Connect, failing to provide a version field in the request will make the version default to 4.</p> <p>To make the interaction with the API easier, I'm using the next adapter:</p> <pre><code>class Anki:\n    \"\"\"Define the Anki adapter.\"\"\"\n\n    def __init__(self, url: str = \"http://localhost:8765\") -&gt; None:\n        \"\"\"Initialize the adapter.\"\"\"\n        self.url = url\n\n    def requests(\n        self, action: str, params: Optional[Dict[str, str]] = None\n    ) -&gt; Response:\n        \"\"\"Do a request to the server.\"\"\"\n        if params is None:\n            params = {}\n\n        response = requests.post(\n            self.url, json={\"action\": action, \"params\": params, \"version\": 6}\n        ).json()\n        if len(response) != 2:\n            raise Exception(\"response has an unexpected number of fields\")\n        if \"error\" not in response:\n            raise Exception(\"response is missing required error field\")\n        if \"result\" not in response:\n            raise Exception(\"response is missing required result field\")\n        if response[\"error\"] is not None:\n            raise Exception(response[\"error\"])\n        return response[\"result\"]\n</code></pre> <p>You can find the full adapter in the fala project.</p>"}, {"location": "anki/#decks", "title": "Decks", "text": ""}, {"location": "anki/#get-all-decks", "title": "Get all decks", "text": "<p>With the adapter:</p> <pre><code>self.requests(\"deckNames\")\n</code></pre> <p>Or with <code>curl</code>:</p> <pre><code>curl localhost:8765 -X POST -d '{\"action\": \"deckNames\", \"version\": 6}'\n</code></pre>"}, {"location": "anki/#create-a-new-deck", "title": "Create a new deck", "text": "<pre><code>self.requests(\"createDeck\", {\"deck\": deck})\n</code></pre>"}, {"location": "anki/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Anki-Connect reference</li> </ul>"}, {"location": "anonymous_feedback/", "title": "Anonymous Feedback", "text": "<p>Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities.</p>"}, {"location": "anonymous_feedback/#why-would-you-need-anonymous-feedback", "title": "Why would you need anonymous feedback?", "text": "<p>Ideally, everyone in your company should be able to give feedback publicly and not anonymously. They should share constructive criticism and not shy away from direct feedback if they believe and trust that their opinions will be heard and addressed.</p> <p>However, to achieve this ideal, people need to feel that they are in a safe space, a place or environment in which they feel confident that they will not be exposed to discrimination, criticism, harassment, or any other emotional or physical harm. The work place is usually not considered a safe space by the employees because they may:</p> <ul> <li> <p>Fear of being judged: We want people to like us and not just in our     personal lives, but in our professional lives as well. It also seems to bear     a bigger importance that our supervisor likes us because he holds the power     over our career and financial security. So we live in a constant state of     anxiety of what might happen if our manager doesn't like us.</p> </li> <li> <p>Fear of losing their job: It\u2019s a form of self-preservation, abstaining from     saying something that may be perceived as wrong to someone in a position of     authority.</p> </li> <li> <p>Fear of being singled out: Giving direct feedback puts you in the spotlight.     Being highlighted against the rest of the employees might be seen as     a threat, especially by people belonging to a different race, gender,     national origin, or other identities than most of their coworkers.</p> </li> <li> <p>Feel insecure: People may distrust their colleagues, because they     just arrived at the organization or may have negative past experiences either with     them or with similar people. They may not have a solid stance on an issue,     be shy or have problems of self esteem.</p> </li> <li> <p>Distrust the open-door internal policies: Past experiences in other     companies may lead the employee not to trust open-doors policies until they     have seen them in practice.</p> </li> <li> <p>Not knowing the internal processes of the organization: As a Slack study     shows,     55 percent of business owners described their organization as very transparent,     but only 18 percent of their employees would agree.</p> </li> </ul> <p>For all these reasons, some employees may remain silent when asked for direct feedback, to speak up against an internal issue or in need to report a colleague or manager. These factors are further amplified if:</p> <ul> <li>The person belongs to a minority group inside the organization.</li> <li>The greater the difference in position between the talking parties. It's more     difficult to talk to the CEO than to the immediate manager.</li> </ul> <p>Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust.</p>"}, {"location": "anonymous_feedback/#pros-and-cons", "title": "Pros and cons", "text": "<p>Pros of Anonymous Feedback:</p> <ul> <li> <p>Employees can express themselves freely and provide valuable insights: On     topics that are considered sensitive, you\u2019ll often find employees who are     afraid to share their opinions. But when employees have the option to use     anonymous feedback, you will be offering a safe space for them to share     their honest, constructive feedback about sensitive workplace issues,     without fear of being judged, victimized, radicalized or labelled in any     way.</p> <p>A formal, non-anonymous feedback form will only reveal some of the superficial, non-threatening issues that affect the workplace, without mentioning the most important, underlying problems. The real problems that no one talks about because they know they are so important that they could stir things up.</p> <p>In fact, these controversial, important issues are the ones that need to be brought to the table as soon as possible. They should be addressed by the entire team before they become a source of unhappiness, conflict and lack of productivity.</p> <p>An anonymous feedback instrument gives you real power over those issues because it doesn't matter who brought it up, but that it\u2019s resolved. For a manager, that insight is invaluable.</p> </li> <li> <p>It builds trust: Anonymous feedback allows the employee see how management     reacts to feedback, understand how people perceive their constructive (and     sometimes critical) opinions, how are the open-door policies being applied     and build up self esteem.</p> </li> <li> <p>It offers a sense of security: Anonymity soothes the employee anxiety and     creates a greater willingness to share our ideas and opinions.</p> </li> <li> <p>It allows every voice to be heard and respected: In workplaces, where they     practice direct or attributed feedback, leaders may give preference to some     voices over others. Due to our unconscious biases, people of higher     authority, backgrounds, or eloquence tend to command respect and attention.     In such situations, the issues they raise are likely to get immediate     attention than those raised by the rest of the group. However, when feedback     is collected anonymously, it eliminates biases and allows leaders to focus     entirely on the feedback.</p> </li> <li> <p>It encourages new employees to share their opinions:     Research     has shown that new employees, who happen to be less senior or influential,     see anonymous feedback as more appropriate for formal and informal     evaluations than their older colleagues. Typically, the last thing a new     employee wants is to start on the wrong foot, so they maintain a neutral     stance. Using anonymous feedback can make new employees feel     more comfortable sharing their real opinions on workplace issues.</p> </li> </ul> <p>Cons of Anonymous Feedback:</p> <ul> <li> <p>It can breed hostility: According to this Harvard Business Review article,     anonymity often sets off a \u201cwitch hunt\u201d, where leaders seek to know the     source of a negative comment. On the one hand, employees can hide behind     anonymity to say personal and hurtful things about their colleagues or     leaders. On the other hand, leaders may take constructive feedback as     a personal attack and become suspicious and hostile to all their     employees.</p> </li> <li> <p>It can be less impactful than attributed feedback: When using attributed     feedback where responses carry the employees\u2019 names, information can be     analyzed for relevance and impact. However, with anonymous feedback, it can     be difficult to analyze information accurately. It is not uncommon for     companies who choose to practice anonymous feedback, to find less specific     responses since details may reveal respondents\u2019 identities. Vague feedback     from employees would have less power to influence behaviors or drive change     in the organization.</p> </li> <li> <p>It can be difficult to act on: Since anonymous feedback is often difficult     to trace, it can be challenging for the organization to get context or     follow up on important issues, especially when a problem is peculiar to an     individual.</p> </li> </ul>"}, {"location": "anonymous_feedback/#how-to-request-anonymous-feedback", "title": "How to request anonymous feedback", "text": "<p>When requesting for anonymous feedback on an organizational level, it is necessary to:</p> <ul> <li>Set expectations for employees: Let your colleagues know how important their     feedback is to the organization. Also, assure them that their responses will     be non-identifiable (no identifiable names, titles, or other demographic     details). According to a Harvard Business Review     article,     \u201crespondents are much more likely to participate if they are confident that     personal anonymity is guaranteed.\u201d Set those expectations to increase the     chances of response from them.</li> <li>Deploy a feedback platform: Use a trusted feedback platform to send feedback     requests to the rest of the employees.</li> </ul>"}, {"location": "anonymous_feedback/#how-to-act-on-anonymous-feedback", "title": "How to Act on Anonymous Feedback", "text": "<p>Once you have sent the anonymous feedback, be sure to:</p> <ul> <li>Gather and share the findings: A significant issue with employee feedback is     that the data often ends up unused. After collecting the results, share the     data\u2014the positives and negatives\u2014with everyone. Doing this shows     transparency and makes your colleagues develop a positive attitude toward     future requests for feedback.</li> <li>Get everyone involved: Engage employees, managers, and leaders in     discussing and analyzing the feedback findings. Doing this helps to build     trust and develop actionable ideas to move the organization forward.</li> <li>Identify the key issues: From the discussions and analysis, identify the key     issues and understand how they would impact the organization, once     addressed.</li> <li>Define and act on the next steps: The purpose of collecting feedback would     be pointless if the next steps aren't defined. Real improvement comes     from knowing and working on the next steps.</li> </ul>"}, {"location": "anonymous_feedback/#references", "title": "References", "text": "<ul> <li>Osasumwen Arigbe article on Diversity, Inclusion, and Anonymous Feedback</li> <li>Paula Clapon article Why anonymous employee feedback is the better alternative</li> <li>Julian Cook article.     I haven't used it's text, but it's written for managers in their language,     it may help someone there.</li> </ul>"}, {"location": "ansible_snippets/", "title": "Ansible Snippets", "text": ""}, {"location": "ansible_snippets/#get-the-hosts-of-a-dynamic-ansible-inventory", "title": "Get the hosts of a dynamic ansible inventory", "text": "<pre><code>ansible-inventory -i environments/production --graph\n</code></pre> <p>You can also use the <code>--list</code> flag to get more info of the hosts.</p>"}, {"location": "ansible_snippets/#speed-up-the-stat-module", "title": "Speed up the stat module", "text": "<p>The <code>stat</code> module calculates the checksum and the md5 of the file in order to get the required data. If you just want to check if the file exists use:</p> <pre><code>- name: Verify swapfile status\n  stat:\n    path: \"{{ common_swapfile_location }}\"\n    get_checksum: no\n    get_md5: no\n    get_mime: no\n    get_attributes: no\n  register: swap_status\n  changed_when: not swap_status.stat.exists\n</code></pre>"}, {"location": "ansible_snippets/#stop-running-docker-containers", "title": "Stop running docker containers", "text": "<pre><code>- name: Get running containers\n  docker_host_info:\n    containers: yes\n  register: docker_info\n\n- name: Stop running containers\n  docker_container:\n    name: \"{{ item }}\"\n    state: stopped\n  loop: \"{{ docker_info.containers | map(attribute='Id') | list }}\"\n</code></pre>"}, {"location": "ansible_snippets/#moving-a-file-remotely", "title": "Moving a file remotely", "text": "<p>Funnily enough, you can't without a <code>command</code>. You could use the <code>copy</code> module with:</p> <pre><code>- name: Copy files from foo to bar\n  copy:\n    remote_src: True\n    src: /path/to/foo\n    dest: /path/to/bar\n\n- name: Remove old files foo\n  file: path=/path/to/foo state=absent\n</code></pre> <p>But that doesn't move, it copies and removes, which is not the same.</p> <p>To make the <code>command</code> idempotent you can use a <code>stat</code> task before.</p> <pre><code>- name: stat foo\n  stat: path=/path/to/foo\n  register: foo_stat\n\n- name: Move foo to bar\n  command: mv /path/to/foo /path/to/bar\n  when: foo_stat.stat.exists\n</code></pre>"}, {"location": "antifascism/", "title": "Antifascism", "text": "<p>Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the far right menace (Mark p. 11). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary. Usually sharing space and even blending with other politic stances that share the same principle, such as intersectional feminism.</p> <p>Read the references</p> <p>The articles under this section are the brushstrokes I use to learn how to become an efficient antifascist.</p> <p>It assumes that you identify yourself as an antifascist, so I'll go straight to the point, skipping much of the argumentation that is needed to sustain these ideas. I'll add links to Mark's and Pol's awesome books, which I strongly recommend you to buy, as they both are jewels that everyone should read.</p> <p>Despite the criminalization and stigmatization by the mainstream press and part of the society, antifascism is a rock solid organized movement with a lot of history, that has invested blood, tears and lives to prevent us from living in a yet more horrible world.</p> <p>The common stereotype is a small group of leftist young people that confront the nazis on the streets, preventing them from using the public space, and from further organizing through direct action and violence if needed. If you don't identify yourself with this stereotype, don't worry, they are only a small (but essential) part of antifascism, there are so many and diverse ways to be part of the antifascist movement that in fact, everyone can (and should) be an antifascist.</p>"}, {"location": "antifascism/#what-is-fascism", "title": "What is fascism", "text": "<p>Fascism in Paxton's words is:</p> <p>... a form of political behavior marked by obsessive preoccupation with community decline, humiliation, or victimhood and by compensatory cults of unity, energy, and purity, in which a mass-based party of commited nationalist militians, working in uneasy but effective collaboration with traditional elites, abandons democratic liberties and pursues with redemptive violence and without ethical or legal restrains goals of internal cleansing and external expansion.</p> <p>They are nourished by the people's weariness with the corruption and inoperability of the traditional political parties, and the growing fear and anguish of an uncertain economic situation.</p> <p>They continuously adapt, redefine and reappropriate concepts under an irreverent, politically incorrect and critical spirit, to spread the old discourse of the privileged against the oppressed.</p> <p>They dress themselves as antisystems, pursuing the liberty behind the authority, and accepting the democratic system introducing totalitarianism nuances (Pol p.20).</p>"}, {"location": "antifascism/#how-to-identify-fascism", "title": "How to identify fascism", "text": "<p>We need to make sure that we use the term well, otherwise we run into the risk of the word loosing meaning. But equally important is not to fall in a wording discussion that paralyzes us.</p> <p>One way to make it measurable is to use Kimberl\u00e9 Williams Crenshaw intersectionality theory , which states that individuals experience oppression or privilege based on a belonging to a plurality of social categories, to measure how close an action or discourse follows fascism principles (Pol p.26).</p> <p> Source</p> <p>Fascism has always been carried out by people with many privileges (the upper part of the diagram) against collectives under many oppressions (the lower part of the diagram). We can then state that the more the oppressions a discourse defends and perpetuates, the more probable it is to be fascist. If it also translates into physical or verbal aggressions, escalates into the will to transform that discourse into laws that backs up those aggressions, or tries to build a government under those ideas, then we clearly have a political roadmap towards fascism.</p> <p>The fact that they don't propose to abolish the democracy or try to send people to concentration camps doesn't mean they are not fascist. First, we don't need them to commit the exact same crimes that the fascists of last century made to put at risk some social collectives, and secondly, history tells us that classic fascism movements didn't show their true intentions in their early phases.</p> <p>Fascism shifts their form and particular characteristics based on place and time. Waiting to see it clear is risking being late to fight it. Therefore whenever we see a discourse that comes from a privileged person against a oppressed one, we should fight it immediately, once fought, you can analyze if it was fascist or not (Pol p.28)</p>"}, {"location": "antifascism/#how-to-fight-fascism", "title": "How to fight fascism", "text": "<p>There are many ways to fight it, the book Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo of Pol Andi\u00f1ach gathers some of them.</p> <p>One way we've seen pisses them off quite much is when they are ridiculed and they evocate the image of incompetence. It's a fine line to go, because if it falls into a pity image then it may strengthen their victim role.</p>"}, {"location": "antifascism/#references", "title": "References", "text": "<ul> <li>Antifa: The anti-fascist handbook by Mark Bray</li> <li>Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo de Pol     Andi\u00f1ach</li> </ul>"}, {"location": "antifascism/#magazines", "title": "Magazines", "text": "<ul> <li>Hope not Hate</li> <li>Searchlight</li> </ul>"}, {"location": "antifascism/#podcasts", "title": "Podcasts", "text": "<ul> <li>Hope not Hate</li> </ul>"}, {"location": "antifascist_actions/", "title": "Antifa Actions", "text": "<p>Collection of amazing and inspiring antifa actions.</p>"}, {"location": "antifascist_actions/#2022", "title": "2022", "text": ""}, {"location": "antifascist_actions/#an-open-data-initiative-to-map-spanish-hate-crimes", "title": "An open data initiative to map spanish hate crimes", "text": "<p>The project Crimenes de Odio have created an open database of the hate crimes registered in the spanish state.</p>"}, {"location": "antifascist_actions/#an-open-data-initiative-to-map-spanish-fascist-icons", "title": "An open data initiative to map spanish fascist icons", "text": "<p>The project Deber\u00edaDesaparecer have created an open database of the remains of the spanish fascist regime icons. The visualization they've created is astonishing, and they've provided a form so that anyone can contribute to the dataset.</p>"}, {"location": "antifascist_actions/#2021", "title": "2021", "text": ""}, {"location": "antifascist_actions/#a-fake-company-and-five-million-recycled-flyers", "title": "A fake company and five million recycled flyers", "text": "<p>A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda.</p> <p>They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\". At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\".</p> <p>They've done a crowdfunding to fund the legal process that may result.</p>"}, {"location": "antitransphobia/", "title": "Anti-transphobia", "text": "<p>Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia.</p> <p>It's yet another clear case of privileged people oppressing even further already oppressed collectives. We can clearly see it if we use the ever useful Kimberl\u00e9 Williams Crenshaw intersectionality theory diagram.</p> <p> Source</p>"}, {"location": "antitransphobia/#terf", "title": "TERF", "text": "<p>TERF is an acronym for trans-exclusionary radical feminist. The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.</p>"}, {"location": "antitransphobia/#arguments-against-theories-that-deny-the-reality-of-trans-people", "title": "Arguments against theories that deny the reality of trans people", "text": "<p>This section is a direct translation from Alana Portero's text called Definitions.</p>"}, {"location": "antitransphobia/#sex-is-a-medical-category-and-gender-a-social-category", "title": "Sex is a medical category and gender a social category", "text": "<p>Sex is a medical category, it's not a biological one. According to body features like the chromosome structure and genitalia appearance, medicine assigns the sex (hence gender) to the bodies. In the case of intersexual people, they are usually mutilated and hormonated so that their bodies fit into one of the two options contemplated by the medicine.</p> <p>Gender is the term we use to refer to how a person feels about themselves as a boy/man, a girl/woman or non-binary.</p> <p>Since birth, we're told what's appropriate (and what isn't) for each gender. These are the gender roles. It's not the same gender than gender role: the gender determines how you interact with the other roles. For example, a woman can take traditionally understood male roles gender roles, that doesn't mean that she is or isn't a woman.</p> <p>The problem arises when these two oppressions are mixed up: cissexism (the believe that bodies have an immutable gender defined by the sex assigned at birth) and misogyny (the base of feminine oppression). When you mixing them up you get the idea that the trans movement erases the feminine structural oppression, when in reality, it broadens the scope and makes it more precise, as they suffer the same misogyny than the cis women.</p> <p>Women are killed for being women. They are socially assigned the responsibility for care, they are prevented from having individual will and they are deterred from accessing resources. This structural violence is suffered by all women regardless of the sex assigned at birth.</p> <p>Questioning the adjudication of gender to the bodies and questioning the roles assigned to the genders are complementary paths for the feminism liberation.</p>"}, {"location": "antitransphobia/#avoid-the-interested-manipulation-of-the-sexual-or-gender-identity", "title": "Avoid the interested manipulation of the sexual or gender identity", "text": "<p>The sexual or gender identity determines whether there is correspondence with the gender assigned at birth. When there isn't, it concerns a trans person.</p> <p>The sex and gender terms represent the same reality, being sex the medical term, and gender the academic one. Equally transexual and transgender represent the same reality, although these last have a pathologizing undertone, the term trans is preferred.</p>"}, {"location": "antitransphobia/#avoid-the-fears-of-letting-trans-people-be", "title": "Avoid the fears of letting trans people be", "text": "<p>Some are afraid that the trans women negatively affect the statistics of unemployment, laboral inequality, feminization of the poverty and machist violence, and they contradict the problems of the cis women.</p> <p>Trans people usually have a greater unemployment rate (85% in Spain), so the glass ceiling is not yet even a concern, and they are also greatly affected by machist violence.</p> <p>The queer theory doesn't erase or blur women as a political subject. Thinking that it risks the rights and achievements earned through the feminist movement shows a complete misunderstanding of the theory.</p>"}, {"location": "antitransphobia/#women-are-not-an-entity", "title": "Women are not an entity", "text": "<p>Women are not an entity, they are a group of people that are placed below men in the social scale, each with her own unique experience. The woman identity belongs to any person that identifies herself with it.</p> <p>The fight against discrimination and towards inclusion politics should be mandatory for all society, and shouldn't be used against the trans people.</p>"}, {"location": "antitransphobia/#references", "title": "References", "text": "<ul> <li>Wikipedia page on Transphobia</li> </ul>"}, {"location": "asyncio/", "title": "Asyncio", "text": "<p>asyncio is a library to write concurrent code using the async/await syntax.</p> <p>asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.</p> <p>asyncio is often a perfect fit for IO-bound and high-level structured network code.</p> <p>Note</p> <pre><code>\"[Asyncer](https://asyncer.tiangolo.com/tutorial/) looks very useful\"\n</code></pre>"}, {"location": "asyncio/#tips", "title": "Tips", "text": ""}, {"location": "asyncio/#limit-concurrency", "title": "Limit concurrency", "text": "<p>Use <code>asyncio.Semaphore</code>.</p> <pre><code>sem = asyncio.Semaphore(10)\nasync with sem:\n    # work with shared resource\n</code></pre> <p>Note that this method is not thread-safe.</p>"}, {"location": "asyncio/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Awesome Asyncio</li> <li>Roguelynn tutorial</li> </ul>"}, {"location": "asyncio/#libraries-to-explore", "title": "Libraries to explore", "text": "<ul> <li>Asyncer</li> </ul>"}, {"location": "aws_savings_plan/", "title": "AWS Savings plan", "text": "<p>Saving plans offer a flexible pricing model that provides savings on AWS usage. You can save up to 72 percent on your AWS compute workloads.</p> <p>!!! note \"Please don't make Jeff Bezos even richer, try to pay as less money to AWS as you can.\"</p> <p>Savings Plans provide savings beyond On-Demand rates in exchange for a commitment of using a specified amount of compute power (measured per hour) for a one or three year period.</p> <p>When you sign up for Savings Plans, the prices you'll pay for usage stays the same through the plan term. You can pay for your commitment using All Upfront, Partial upfront, or No upfront payment options.</p> <p>Plan types:</p> <ul> <li> <p>Compute Savings Plans provide the most flexibility and prices that are up     to 66 percent off of On-Demand rates. These plans automatically apply to     your EC2 instance usage, regardless of instance family (for example, m5, c5,     etc.), instance sizes (for example, c5.large, c5.xlarge, etc.), Region (for     example, us-east-1, us-east-2, etc.), operating system (for example,     Windows, Linux, etc.), or tenancy (for example, Dedicated, default,     Dedicated Host). With Compute Savings Plans, you can move a workload from C5     to M5, shift your usage from EU (Ireland) to EU (London). You can continue     to benefit from the low prices provided by Compute Savings Plans as you make     these changes.</p> </li> <li> <p>EC2 Instance Savings Plans provide savings up to 72 percent off On-Demand,     in exchange for a commitment to a specific instance family in a chosen AWS     Region (for example, M5 in Virginia). These plans automatically apply to     usage regardless of size (for example, m5.xlarge, m5.2xlarge, etc.), OS (for     example, Windows, Linux, etc.), and tenancy (Host, Dedicated, Default)     within the specified family in a Region.</p> <p>With an EC2 Instance Savings Plan, you can change your instance size within the instance family (for example, from c5.xlarge to c5.2xlarge) or the operating system (for example, from Windows to Linux), or move from Dedicated tenancy to Default and continue to receive the discounted rate provided by your EC2 Instance Savings Plan.</p> </li> <li> <p>Standard Reserved Instances: The old reservation system, you reserve an     instance type and you can get up to 72 percent of discount. The lack of     flexibility makes them inferior to the new EC2 instance plans.</p> </li> <li> <p>Convertible Reserved Instances: Same as the Standard Reserved Instances but     with more flexibility. Discounts range up to 66%, similar to the new Compute     Savings Plan, which again gives more less the same discounts with more     flexibility, so I wouldn't use this plan either.</p> </li> </ul>"}, {"location": "aws_savings_plan/#understanding-how-savings-plans-apply-to-your-aws-usage", "title": "Understanding how Savings Plans apply to your AWS usage", "text": "<p>If you have active Savings Plans, they apply automatically to your eligible AWS usage to reduce your bill.</p> <p>Savings Plans apply to your usage after the Amazon EC2 Reserved Instances (RI) are applied. Then EC2 Instance Savings Plans are applied before Compute Savings Plans because Compute Savings Plans have broader applicability.</p> <p>They calculate your potential savings percentages of each combination of eligible usage. This percentage compares the Savings Plans rates with your current On-Demand rates. Your Savings Plans are applied to your highest savings percentage first. If there are multiple usages with equal savings percentages, Savings Plans are applied to the first usage with the lowest Savings Plans rate. Savings Plans continue to apply until there are no more remaining usages, or your commitment is exhausted. Any remaining usage is charged at the On-Demand rates.</p>"}, {"location": "aws_savings_plan/#savings-plan-example", "title": "Savings plan example", "text": "<p>In this example, you have the following usage in a single hour:</p> <ul> <li>4x r5.4xlarge Linux, shared tenancy instances in us-east-1, running for the     duration of a full hour.</li> <li>1x m5.24xlarge Linux, dedicated tenancy instance in us-east-1, running for the     duration of a full hour.</li> </ul> <p>Pricing example:</p> Type On-Demand rate Compute Savings Plans rate CSP Savings percentage EC2 Instance Savings Plans rate EC2IS percentage r5.4xlarge $1.00 $0.70 30% $0.60 40% m5.24xlarge $10.00 $8.20 18% $7.80 22% <p>They've included other products in the example but I've removed them for the sake of simplicity</p>"}, {"location": "aws_savings_plan/#scenario-1-savings-plan-apply-to-all-usage", "title": "Scenario 1: Savings Plan apply to all usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with a $50.00/hour commitment.</p> <p>Your Savings Plan covers all of your usage because multiplying each of your usages by the equivalent Compute Savings Plans is $47.13. This is still less than the $50.00/hour commitment.</p> <p>Without Savings Plans, you would be charged at On-Demand rates in the amount of $59.10.</p>"}, {"location": "aws_savings_plan/#scenario-2-savings-plans-apply-to-some-usage", "title": "Scenario 2: Savings Plans apply to some usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with a $2.00/hour commitment.</p> <p>In any hour, your Savings Plans apply to your usage starting with the highest discount percentage (30 percent).</p> <p>Your $2.00/hour commitment is used to cover approximately 2.9 units of this usage. The remaining 1.1 units are charged at On-Demand rates, resulting in $1.14 of On-Demand charges for r5.</p> <p>The rest of your usage are also charged at On-Demand rates, resulting in $55.10 of On-Demand charges. The total On-Demand charges for this usage are $56.24.</p>"}, {"location": "aws_savings_plan/#scenario-3-savings-plans-and-ec2-reserved-instances-apply-to-the-usage", "title": "Scenario 3: Savings Plans and EC2 reserved instances apply to the usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with an $18.20/hour commitment. You have two EC2 Reserved Instances (RI) for r5.4xlarge Linux shared tenancy in us-east-1.</p> <p>First, the Reserve Instances covers two of the r5.4xlarge instances. Then, the Savings Plans rate is applied to the remaining r5.4xlarge and the rest of the usage, which exhausts the hourly commitment of $18.20.</p>"}, {"location": "aws_savings_plan/#scenario-4-multiple-savings-plans-apply-to-the-usage", "title": "Scenario 4: Multiple Savings Plans apply to the usage", "text": "<p>You purchase a one-year, partial upfront EC2 Instance Family Savings Plan for the r5 family in us-east-1 with a $3.00/hour commitment. You also have a one-year, partial upfront Compute Savings Plan with a $16.80/hour commitment.</p> <p>Your EC2 Instance Family Savings Plan (r5, us-east-1) covers all of the r5.4xlarge usage because multiplying the usage by the EC2 Instance Family Savings Plan rate is $2.40. This is less than the $3.00/hour commitment.</p> <p>Next, the Compute Savings Plan is applied to rest of the resource usage, if it doesn't cover the whole expense, then On demand rates will apply.</p>"}, {"location": "aws_savings_plan/#monitoring-the-savings-plan", "title": "Monitoring the savings plan", "text": "<p>Monitoring is an important part of your Savings Plans usage. Understanding the Savings Plan that you own, how they are applying to your usage, and what usage is being covered are important parts of optimizing your costs with Savings Plans. You can monitor your usage in multiple forms.</p> <ul> <li> <p>Using the     inventory:     The Savings Plans Inventory page shows a detailed overview of the Savings     Plans that you own, or have queued for future purchase.</p> <p>To view your Inventory page:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>In the navigation pane, under Savings Plans, choose Inventory.</li> </ul> </li> <li> <p>Using the utilization     report:     Savings Plans utilization shows you the percentage of your Savings Plans     commitment that you're using across your On-Demand usage. You can use your     Savings Plans utilization report to visually understand how your Savings     Plans apply to your usage over the configured time period. Along with     a visualized graph, the report shows high-level metrics based on your     selected Savings Plan, filters, and lookback periods. Utilization is     calculated based on how your Savings Plans applied to your usage over the     lookback period.</p> <p>For example, if you have a $10/ hour commitment, and your usage billed with Savings Plans rates totals to $9.80 for the hour, your utilization for that hour is 98 percent.</p> <p>You can find high-level metrics in the Utilization report section:</p> <ul> <li>On-Demand Spend Equivalent: The amount you would have spent on the same     usage if you didn\u2019t commit to Savings Plans. This amount is the     equivalent On-Demand cost based on current On-Demand rates.</li> <li>Savings Plans spend: Your Savings Plans commitment spend over the     lookback period.</li> <li>Total Net Savings: The amount you saved using Savings Plans commitments     over the selected time period, compared to the On-Demand cost estimate.</li> </ul> <p>To access your utilization report:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>On the navigation pane, choose Savings Plans.</li> <li>In the left pane, choose Utilization report.</li> </ul> </li> <li> <p>Using the coverage     report:     The Savings Plans coverage report shows how much of your eligible spend was     covered by your Savings Plans, based on the selected time period.</p> <p>You can find the following high-level metrics in the Coverage report section:</p> <ul> <li>Average Coverage: The aggregated Savings Plans coverage percentage based     on the selected filters and look-back period.</li> <li>Additional potential savings: Your potential savings amount based on     your Savings Plans recommendations. This is shown as a monthly amount.</li> <li>On-Demand spend not covered: The amount of eligible savings spend that     was not covered by Savings Plans or Reserved Instances over the lookback     period.</li> </ul> <p>To access your utilization report:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>On the navigation pane, choose Savings Plans.</li> <li>In the left pane, choose Coverage report.</li> </ul> </li> </ul>"}, {"location": "aws_savings_plan/#doing-your-savings-plan", "title": "Doing your savings plan", "text": "<p>Go to the AWS savings plan simulator and check the different instances you were evaluating.</p>"}, {"location": "aws_snippets/", "title": "AWS Snippets", "text": ""}, {"location": "aws_snippets/#find-if-external-ip-belongs-to-you", "title": "Find if external IP belongs to you", "text": "<p>You can list the network interfaces that match the IP you're searching for</p> <pre><code>aws ec2 describe-network-interfaces --filters Name=association.public-ip,Values=\"{{ your_ip_address}}\"\n</code></pre>"}, {"location": "aws_waf/", "title": "AWS WAF", "text": "<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. You can also customize rules that filter out specific traffic patterns.</p>"}, {"location": "aws_waf/#extracting-information", "title": "Extracting information", "text": "<p>You can configure the WAF to write it's logs into S3, Kinesis or a Cloudwatch log group. S3 saves the data in small compressed files which are difficult to analyze, Kinesis makes sense if you post-process the data on a log system such as graylog, the last one allows you to use the WAF's builtin cloudwatch log insights which has the next interesting reports:             . * Top 100 Ip addresses * Top 100 countries * Top 100 hosts * Top 100 terminating rules             . Nevertheless, it still lacks some needed reports to analyze the traffic. But it's quite easy to build them yourself in Cloudwatch Log Insights. If you have time I'd always suggest to avoid using proprietary AWS tools, but sadly it's the quickest way to get results.</p>"}, {"location": "aws_waf/#creating-log-insights-queries", "title": "Creating Log Insights queries", "text": "<p>Inside the Cloudwatch site, on the left menu you'll see the <code>Logs</code> tab, and under it <code>Log Insights</code>. There you can write the query you want to run. Once it returns the expected result, you can save it. Saved queries can be seen on the right menu, under <code>Queries</code>.</p> <p>If you later change the query, you'll see a blue dot beside the query you last run. The query will remain changed until you click on <code>Actions</code> and then <code>Reset</code>.</p>"}, {"location": "aws_waf/#useful-queries", "title": "Useful Queries", "text": ""}, {"location": "aws_waf/#top-ips", "title": "Top IPs", "text": "<p>Is a directory to save the queries to analyze a count of requests aggregated by ips.</p>"}, {"location": "aws_waf/#top-ips-query", "title": "Top IPs query", "text": "<pre><code>fields httpRequest.clientIp\n| stats count(*) as requestCount by httpRequest.clientIp\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-ips-by-uri", "title": "Top IPs by uri", "text": "<pre><code>fields httpRequest.clientIp\n| filter httpRequest.uri like \"/\"\n| stats count(*) as requestCount by httpRequest.clientIp\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris", "title": "Top URIs", "text": "<p>Is a directory to save the queries to analyze a count of requests aggregated by uris.</p>"}, {"location": "aws_waf/#top-uris-query", "title": "Top URIs query", "text": "<p>This report shows all the uris that are allowed to pass the WAF.</p> <pre><code>fields httpRequest.uri\n| filter action like \"ALLOW\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-a-termination-rule", "title": "Top URIs of a termination rule", "text": "<pre><code>fields httpRequest.uri\n| filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-an-ip", "title": "Top URIs of an IP", "text": "<pre><code>fields httpRequest.uri\n| filter @message like \"6.132.241.132\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-a-cloudfront-id", "title": "Top URIs of a Cloudfront ID", "text": "<pre><code>fields httpRequest.uri\n| filter httpSourceId like \"CLOUDFRONT_ID\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-terminating-rules", "title": "WAF Top terminating rules", "text": "<p>Report that shows the top rules that are blocking the content.</p> <pre><code>fields terminatingRuleId\n| filter terminatingRuleId not like \"Default_Action\"\n| stats count(*) as requestCount by terminatingRuleId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-blocks-by-cloudfront-id", "title": "Top blocks by Cloudfront ID", "text": "<pre><code>fields httpSourceId\n| filter terminatingRuleId not like \"Default_Action\"\n| stats count(*) as requestCount by httpSourceId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-allows-by-cloudfront-id", "title": "Top allows by Cloudfront ID", "text": "<pre><code>fields httpSourceId\n| filter terminatingRuleId like \"Default_Action\"\n| stats count(*) as requestCount by httpSourceId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-countries", "title": "WAF Top countries", "text": "<pre><code>fields httpRequest.country\n| stats count(*) as requestCount by httpRequest.country\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by", "title": "Requests by", "text": "<p>Is a directory to save the queries to show the requests filtered by a criteria.</p>"}, {"location": "aws_waf/#requests-by-ip", "title": "Requests by IP", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter @message like \"6.132.241.132\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by-termination-rule", "title": "Requests by termination rule", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by-uri", "title": "Requests by URI", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter httpRequest.uri like \"wp-json\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-args-of-an-uri", "title": "WAF Top Args of an URI", "text": "<pre><code>fields httpRequest.args\n| filter httpRequest.uri like \"/\"\n| stats count(*) as requestCount by httpRequest.args\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#analysis-workflow", "title": "Analysis workflow", "text": "<p>To analyze the WAF insights you can:</p> <ul> <li>Analyze the traffic of the top IPs</li> <li>Analyze the top URIs</li> <li>Analyze the terminating rules</li> </ul>"}, {"location": "aws_waf/#analyze-the-traffic-of-the-top-ips", "title": "Analyze the traffic of the top IPS", "text": "<p>For IP in the WAF Top IPs report, do:</p> <ul> <li> <p>Analyze the top uris of that IP to see if they are     legit requests or if it contains malicious requests. If you want to get the     details of a particular request, you can use the requests by     uri report.</p> <p>For request in malicious requests:</p> <ul> <li>If it's not being filtered by the WAF update your WAF rules.</li> </ul> </li> <li> <p>If the IP is malicious mark it as problematic.</p> </li> </ul>"}, {"location": "aws_waf/#analyze-the-top-uris", "title": "Analyze the top uris", "text": "<p>For uri in the WAF Top URIs report, do:</p> <ul> <li> <p>For argument in the top arguments of that uri     report, see if they are legit requests or if it's malicious. If you want to     get the details of a particular request, you can use the requests by     uri report.</p> <p>For request in malicious requests:</p> <ul> <li>If it's not being filtered by the WAF update your WAF rules.</li> <li>For IP in top uris by IP report:<ul> <li>Mark IP as problematic.</li> </ul> </li> </ul> </li> </ul>"}, {"location": "aws_waf/#analyze-the-terminating-rules", "title": "Analyze the terminating rules", "text": "<p>For terminating rule in the WAF Top terminating rules report, do:</p> <ul> <li>For IP in the top ips by termination rule mark it as problematic.</li> </ul> <p>After some time you can see which rules are not being triggered and remove them. With the requests by termination rule you can see which requests are being blocked and try to block it in another rule set and merge both.</p>"}, {"location": "aws_waf/#mark-ip-as-problematic", "title": "Mark IP as problematic", "text": "<p>To process an problematic IP:</p> <ul> <li>Add it to the captcha list.</li> <li>If it is already in the captcha list and is still triggering problematic     requests, add it to the block list.</li> <li>Add a task to remove the IP from the block or captcha list X minutes or     hours in the future.</li> </ul>"}, {"location": "aws_waf/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "bash_snippets/", "title": "Bash Snippets", "text": ""}, {"location": "bash_snippets/#do-the-remainder-or-modulus-of-a-number", "title": "Do the remainder or modulus of a number", "text": "<pre><code>expr 5 % 3\n</code></pre>"}, {"location": "bash_snippets/#update-a-json-file-with-jq", "title": "Update a json file with jq", "text": "<p>Save the next snippet to a file, for example <code>jqr</code> and add it to your <code>$PATH</code>.</p> <pre><code>#!/bin/zsh\n\nquery=\"$1\"\nfile=$2\n\ntemp_file=\"$(mktemp)\"\n\n# Update the content\njq \"$query\" $file &gt; \"$temp_file\"\n\n# Check if the file has changed\ncmp -s \"$file\" \"$temp_file\"\nif [[ $? -eq 0 ]] ; then\n  /bin/rm \"$temp_file\"\nelse\n  /bin/mv \"$temp_file\" \"$file\"\nfi\n</code></pre> <p>Imagine you have the next json file:</p> <pre><code>{\n  \"property\": true,\n  \"other_property\": \"value\"\n}\n</code></pre> <p>Then you can run:</p> <pre><code>jqr '.property = false' status.json\n</code></pre> <p>And then you'll have:</p> <pre><code>{\n  \"property\": false,\n  \"other_property\": \"value\"\n}\n</code></pre>"}, {"location": "beancount/", "title": "Beancount", "text": "<p>Beancount is a Python double entry accounting command line tool similar to <code>ledger</code>.</p>"}, {"location": "beancount/#installation", "title": "Installation", "text": "<pre><code>pip3 install beancount\n</code></pre>"}, {"location": "beancount/#tools", "title": "Tools", "text": "<p><code>beancount</code> is the core component, it's a declarative language. It parses a text file, and produces reports from the resulting data structures.</p>"}, {"location": "beancount/#bean-check", "title": "bean-check", "text": "<p><code>bean-check</code> is the program you use to verify that your input syntax and transactions work correctly. All it does is load your input file and run the various plugins you configured in it, plus some extra validation checks.</p> <pre><code>bean-check /path/to/file.beancount\n</code></pre> <p>If there are no errors, there should be no output, it should exit quietly.</p>"}, {"location": "beancount/#bean-report", "title": "bean-report", "text": "<p>This is the main tool used to extract specialized reports to the console in text or one of the various other formats.</p> <p>For a graphic exploration of your data, use the fava web application instead.</p> <pre><code>bean-report /path/to/file.beancount {{ report_name }}\n</code></pre> <p>There are many reports available, to get a full list run <code>bean-report --help-reports</code></p> <p>Report names sometimes may accept arguments, if they do  so use <code>:</code></p> <pre><code>bean-report /path/to/file.beancount balances:Vanguard\n</code></pre>"}, {"location": "beancount/#to-get-the-balances", "title": "To get the balances", "text": "<pre><code>bean-report {{ path/to/file.beancount }} balances | treeify\n</code></pre>"}, {"location": "beancount/#to-get-the-journal", "title": "To get the journal", "text": "<pre><code>bean-report {{ path/to/file.beancount }} journal\n</code></pre>"}, {"location": "beancount/#to-get-the-holdings", "title": "To get the holdings", "text": "<p>To get the aggregations for the total list of holdings</p> <pre><code>bean-report {{ path/to/file.beancount }} holdings\n</code></pre>"}, {"location": "beancount/#to-get-the-accounts", "title": "To get the accounts", "text": "<pre><code>bean-report {{ path/to/file.beancount }} accounts\n</code></pre>"}, {"location": "beancount/#bean-query", "title": "bean-query", "text": "<p><code>bean-query</code> is a command-line tool that acts like a client to that in-memory database in which you can type queries in a variant of SQL. It has it's own document</p> <pre><code>bean-query /path/to/file.beancount\n</code></pre>"}, {"location": "beancount/#bean-web", "title": "bean-web", "text": "<p>Deprecated use fava instead</p> <p><code>bean-web</code> serves all the reports on a web server that runs on your computer</p> <pre><code>bean-web /path/to/file.beancount\n</code></pre> <p>It will serve on <code>localhost:8080</code></p>"}, {"location": "beancount/#bean-doctor", "title": "bean-doctor", "text": "<p>This is a debugging tool used to perform various diagnostics and run debugging commands, and to help provide information for reporting bugs.</p>"}, {"location": "beancount/#bean-format", "title": "bean-format", "text": "<p>Pure text processing tool will reformat Beancount input to right-align all the numbers at the same, minimal column.</p>"}, {"location": "beancount/#bean-example", "title": "bean-example", "text": "<p>Generates an example Beancount input file.</p>"}, {"location": "beancount/#bean-identify", "title": "bean-identify", "text": "<p>Given a messy list of downloaded files automatically identify which of your configured importers is able to handle them and print them out. This is to be used for debugging and figuring out if your configuration is properly associating a suitable importer for each of the files you downloaded.</p>"}, {"location": "beancount/#bean-extract", "title": "bean-extract", "text": "<p>Extracts transactions and statement date from each file, if at all possible. This produces some Beancount input text to be moved to your input file.</p> <pre><code>bean-extract {{ path/to/config.config }} {{ path/to/source/files }}\n</code></pre> <p>The tool calls methods on importer objects. You must provide a list of such importers; this list is the configuration for the importing process.</p> <p>For each file found, each of the importers is called to assert whether it can or cannot handle that file. If it deems that it can, methods can be called to produce a list of transactions extract a date, or produce a cleaned up filename for the downloaded file.</p> <p>The configuration should be a python3 module in which you instantiate the importers and assign the list to the module-level \"CONFIG\" variable</p> <pre><code>#!/usr/bin/env python3\nfrom myimporters.bank import acmebank\nfrom myimporters.bank import chase\n\u2026\nCONFIG = [\nacmebank.Importer(),\nchase.Importer(),\n\u2026\n]\n</code></pre>"}, {"location": "beancount/#writing-an-importer", "title": "Writing an importer", "text": "<p>Each of the importers must comply with a particular protocol and implement at least some of its methods. The full detail of the protocol is in the source of importer.py</p> <pre><code>\"\"\"Importer protocol.\n\nAll importers must comply with this interface and implement at least some of its\nmethods. A configuration consists in a simple list of such importer instances.\nThe importer processes run through the importers, calling some of its methods in\norder to identify, extract and file the downloaded files.\n\nEach of the methods accept a cache.FileMemo object which has a 'name' attribute\nwith the filename to process, but which also provides a place to cache\nconversions. Use its convert() method whenever possible to avoid carrying out\nthe same conversion multiple times. See beancount.ingest.cache for more details.\n\nSynopsis:\n\n name(): Return a unique identifier for the importer instance.\n identify(): Return true if the identifier is able to process the file.\n extract(): Extract directives from a file's contents and return of list of entries.\n file_account(): Return an account name associated with the given file for this importer.\n file_date(): Return a date associated with the downloaded file (e.g., the statement date).\n file_name(): Return a cleaned up filename for storage (optional).\n\nJust to be clear: Although this importer will not raise NotImplementedError\nexceptions (it returns default values for each method), you NEED to derive from\nit in order to do anything meaningful. Simply instantiating this importer will\nnot match not provide any useful information. It just defines the protocol for\nall importers.\n\"\"\"\n__copyright__ = \"Copyright (C) 2016  Martin Blais\"\n__license__ = \"GNU GPLv2\"\n\nfrom beancount.core import flags\n\n\nclass ImporterProtocol:\n    \"Interface that all source importers need to comply with.\"\n\n    # A flag to use on new transaction. Override this flag in derived classes if\n    # you prefer to create your imported transactions with a different flag.\n    FLAG = flags.FLAG_OKAY\n\n    def name(self):\n        \"\"\"Return a unique id/name for this importer.\n\n        Returns:\n          A string which uniquely identifies this importer.\n        \"\"\"\n        cls = self.__class__\n        return '{}.{}'.format(cls.__module__, cls.__name__)\n\n    __str__ = name\n\n    def identify(self, file):\n        \"\"\"Return true if this importer matches the given file.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          A boolean, true if this importer can handle this file.\n        \"\"\"\n\n    def extract(self, file):\n        \"\"\"Extract transactions from a file.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          A list of new, imported directives (usually mostly Transactions)\n          extracted from the file.\n        \"\"\"\n\n    def file_account(self, file):\n        \"\"\"Return an account associated with the given file.\n\n        Note: If you don't implement this method you won't be able to move the\n        files into its preservation hierarchy; the bean-file command won't work.\n\n        Also, normally the returned account is not a function of the input\n        file--just of the importer--but it is provided anyhow.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          The name of the account that corresponds to this importer.\n        \"\"\"\n\n    def file_name(self, file):\n        \"\"\"A filter that optionally renames a file before filing.\n\n        This is used to make tidy filenames for filed/stored document files. The\n        default implementation just returns the same filename. Note that a\n        simple RELATIVE filename must be returned, not an absolute filename.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          The tidied up, new filename to store it as.\n        \"\"\"\n\n    def file_date(self, file):\n        \"\"\"Attempt to obtain a date that corresponds to the given file.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          A date object, if successful, or None if a date could not be extracted.\n          (If no date is returned, the file creation time is used. This is the\n          default.)\n</code></pre> <p>A summary of the methods you need to, or may want to implement:</p> <ul> <li> <p>name(): Provides a unique id for each importer instance. It's convenient to   be able to refer to your importers with a unique name; it gets printed out by   the identification process.</p> </li> <li> <p>identify(): This method just returns true if this importer can handle the   given file. You must implement this method, and all the tools invoke it ot   figure out the list of (file, importer) pairs.</p> </li> <li> <p>extract(): This is called to attempt to extract some Beancount directives   from the file contents. It must create the directives by instatiating the   objects define in beancout.core.data and return them.</p> </li> </ul> <pre><code>from beancount.ingest import importer\n\nclass Importer(importer.ImporterProtocol):\n\n  def identify(self, file):\n  \u2026\n  # Override other methods\u2026\n</code></pre> <p>Some importer examples:</p> <ul> <li>mterwill gist</li> <li>wzyboy importers</li> </ul>"}, {"location": "beancount/#bean-file", "title": "bean-file", "text": "<ul> <li><code>bean-file</code> filing documents. It si able to identify which document belongs to   which account, it can move the downloaded file to the documents archive   automatically.</li> </ul>"}, {"location": "beancount/#basic-concepts", "title": "Basic concepts", "text": ""}, {"location": "beancount/#beancount-transaction", "title": "Beancount transaction", "text": "<pre><code>2014-05-23 * \"CAFE MOGADOR NEW YO\" \"Dinner with Caroline\"\n  Liabilities:US:BofA:CreditCard -98.32 USD\n  Expenses:Restaurant\n</code></pre> <ul> <li>Currencies must be entirely in capital letters.</li> <li>Account names do not admit spaces.</li> <li>Description strings must be quoted.</li> <li>Dates are only parsed in YYYY-MM-DD format.</li> <li>Tags must begin with <code>#</code> and links with <code>^</code>.</li> </ul>"}, {"location": "beancount/#beancount-operators", "title": "Beancount Operators", "text": ""}, {"location": "beancount/#open", "title": "Open", "text": "<p>All accounts need to be declared open in order to accept amounts posted to them.</p> <pre><code>YYYY-MM-DD open {{ account_name }} [{{ ConstrainCurrency }}]\n</code></pre>"}, {"location": "beancount/#close", "title": "Close", "text": "<pre><code>YYYY-MM-DD close {{ account_name }}\n</code></pre> <p>It's useful to insert a balance of 0 units just before closing an account, just to make sure its contents are empty as you close it.</p>"}, {"location": "beancount/#commodity", "title": "Commodity", "text": "<p>It can be used to declare currencies, financial instruments, commodities... It's optional</p> <pre><code>YYYY-MM-DD commodity {{ currency_name }}\n</code></pre>"}, {"location": "beancount/#transactions", "title": "Transactions", "text": "<pre><code>YYYY-MM-DD txn \"[{{ payee }}]\"  \"{{ Comment }}\"\n  {{ Account1 }} {{ value}}\n  [{{ Accountn-1 }} {{ value }}]\n  {{ Accountn }}\n</code></pre> <p>Payee is a string that represents an external entity that is involved in the transaction. Payees are sometimes useful on transactions that post amounts to Expense accounts, whereby the account accumulates a category of expenses from multiple business</p> <p>As transactions is the most common, you can substitute <code>txn</code> for a flag, by default : * <code>*</code>: Completed transaction, known amounts, \"this looks correct\" * <code>!</code>: Incomplete transaction, needs confirmation or revision, \"this looks   incorrect\"</p> <p>You can also attach flags to the postings themselves, if you want to flag one of the transaction's legs in particular:</p> <pre><code>2014-05-05 * \"Transfer from Savings account\"\n  Assets:MyBank:Checking     -400.00 USD\n  ! Assets:MyBank:Savings\n</code></pre> <p>This is useful in the intermediate stage of de-duping transactions</p>"}, {"location": "beancount/#tags-vs-payee", "title": "Tags vs Payee", "text": "<p>You can tag your transactions with <code>#{{tag_name}}</code>, so you can later filter or generate reports based on that tag. Therefore the Payee could be used as whom or who pays and the tag for the context. For example, for a trip I could use the tag #34C3</p> <p>To mark a series of transactions with tags use the following syntax <pre><code>pushtag #berlin-trip-2014\n\n2014-04-23 * \"Flight to Berlin\"\n  Expenses:Flights -1230.27 USD\n  Liabilities:CreditCard\n\n...\n\npoptag #berlin-trip-2014\n</code></pre></p>"}, {"location": "beancount/#links", "title": "Links", "text": "<p>Transactions can also be linked together. You may think of the link as a special kind of tag that can be used to group together a set of financially related transactions over time.</p> <pre><code>2014-02-05 * \"Invoice for January\" ^invoice-acme-studios-jan14\n  Income:Clients:ACMEStudios   -8450.00 USD\n  Assets:AccountsReceivable\n\n...\n\n2014-02-20 * \"Check deposit - payment from ACME\" ^invoice-acme-studios-jan14\n  Assets:BofA:Checking         8450.00 USD\n  Assets:AccountsReceivable\n</code></pre>"}, {"location": "beancount/#balance", "title": "Balance", "text": "<p>A balance assertion is a way for you to input your statement balance into the flow of transactions. It tells Beancount to verify that the number of units of a particular commodity in some account should equal some expected value at some point in time.</p> <p>If no error is reported, you should have some confidence that the list of transactions that precedes it in this account is highly likely to be correct. This is useful in practice because in many cases some transactions can get imported separately from the accounts of each of their postings.</p> <p>As all other non-transaction directives, it applies at the beginning of it's date. Just imagine that the balance checks occurs right after midnight on that day.</p> <pre><code>YYYY-MM-DD balance {{ account_name }} {{ amount }}\n</code></pre>"}, {"location": "beancount/#pad", "title": "Pad", "text": "<p>A padding directive automatically inserts a transaction that will make the subsequent balance assertion succeed, if it is needed. It inserts the difference needed to fulfill that balance assertion.</p> <p>Being subsequent in date order, not in the order of the declarations in the file.</p> <pre><code>YYYY-MM-DD pad {{ account_name }} {{ account_name_to_pad }}\n</code></pre> <p>The first account is the account to credit the automatically calculated amount to. This is the account that should have a balance assertion following it. The second leg is the source where the funds will come from, and this is almost always some Equity account.</p> <pre><code>1990-05-17 open Assets:Cash EUR\n1990-05-17 pad Assets:Cash Equity:Opening-Balances\n2017-12-26 balance Assets:Cash 250 EUR\n</code></pre> <p>You could also insert pad entries between balance assertions so as to fix un registered transactions</p>"}, {"location": "beancount/#notes", "title": "Notes", "text": "<p>A note directive is simply used to attach a dated comment to the journal of a particular account.</p> <p>this can be useful to record facts and claims associated with a financial event.</p> <pre><code>YYYY-MM-DD note {{ account_name }} {{ comment }}\n</code></pre>"}, {"location": "beancount/#document", "title": "Document", "text": "<p>A Document directive can be used to attach an external file to the journal of an account.</p> <p>The filename gets rendered as a browser link in the journals of the web interface for the corresponding account and you should be able to click on it to view the contents of the file itself.</p> <pre><code>YYYY-MM-DD {{ account_name }} {{ path/to/document }}\n</code></pre>"}, {"location": "beancount/#includes", "title": "Includes", "text": "<p>This allows you to split up large input files into multiple files.</p> <pre><code>include {{ path/to/file.beancount }}\n</code></pre> <p>The path could be relative or absolute.</p>"}, {"location": "beancount/#library-usage", "title": "Library usage", "text": "<p>Beancount can also be used as a Python library.</p> <p>There are some articles in the documentation where you can start seeing how to use it: scripting plugins , external contributions and the api reference. Although I found it more pleasant to read the source code itself as it's really well documented (both by docstrings and type hints).</p>"}, {"location": "beancount/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Git</li> <li>Docs</li> <li>Awesome beancount</li> <li>Docs in google</li> <li>Vim plugin</li> </ul>"}, {"location": "beautifulsoup/", "title": "BeautifulSoup", "text": "<p>BeautifulSoup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree.</p> <pre><code>import requests\nfrom bs4 import BeautifulSoup\n\nrequest = requests.get('{{ url }}')\nsoup = BeautifulSoup(request.text, \"html.parser\")\n</code></pre> <p>Here are some simple ways to navigate that data structure:</p> <pre><code>soup.title\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\n\nsoup.title.name\n# u'title'\n\nsoup.title.string\n# u'The Dormouse's story'\n\nsoup.title.parent.name\n# u'head'\n\nsoup.p\n# &lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;\n\nsoup.p['class']\n# u'title'\n\nsoup.a\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\nsoup.find_all('a')\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n\nsoup.find(id=\"link3\")\n# &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;\n</code></pre>"}, {"location": "beautifulsoup/#installation", "title": "Installation", "text": "<pre><code>pip install beautifulsoup4\n</code></pre> <p>The default parser <code>html.parser</code> doesn't work with HTML5, so you'll probably need to use the <code>html5lib</code> parser, it's not included by default, so you might need to install it as well</p> <pre><code>pip install html5lib\n</code></pre>"}, {"location": "beautifulsoup/#usage", "title": "Usage", "text": ""}, {"location": "beautifulsoup/#kinds-of-objects", "title": "Kinds of objects", "text": "<p>Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you\u2019ll only ever have to deal with about four kinds of objects: <code>Tag</code>, <code>NavigableString</code>, <code>BeautifulSoup</code>, and <code>Comment</code>.</p>"}, {"location": "beautifulsoup/#tag", "title": "Tag", "text": "<p>A <code>Tag</code> object corresponds to an XML or HTML tag in the original document:</p> <pre><code>soup = BeautifulSoup('&lt;b class=\"boldest\"&gt;Extremely bold&lt;/b&gt;')\ntag = soup.b\ntype(tag)\n# &lt;class 'bs4.element.Tag'&gt;\n</code></pre> <p>The most important features of a tag are its <code>name</code> and <code>attributes</code>.</p>"}, {"location": "beautifulsoup/#name", "title": "Name", "text": "<p>Every tag has a <code>name</code>, accessible as <code>.name</code>:</p> <pre><code>tag.name\n# u'b'\n</code></pre> <p>If you change a tag\u2019s name, the change will be reflected in any HTML markup generated by Beautiful Soup:.</p> <pre><code>tag.name = \"blockquote\"\ntag\n# &lt;blockquote class=\"boldest\"&gt;Extremely bold&lt;/blockquote&gt;\n</code></pre>"}, {"location": "beautifulsoup/#attributes", "title": "Attributes", "text": "<p>A tag may have any number of attributes. The tag <code>&lt;b id=\"boldest\"&gt;</code> has an attribute <code>id</code> whose value is <code>boldest</code>. You can access a tag\u2019s attributes by treating the tag like a dictionary:</p> <pre><code>tag['id']\n# u'boldest'\n</code></pre> <p>You can access that dictionary directly as <code>.attrs</code>:</p> <pre><code>tag.attrs\n# {u'id': 'boldest'}\n</code></pre> <p>You can add, remove, and modify a tag\u2019s attributes. Again, this is done by treating the tag as a dictionary:</p> <pre><code>tag['id'] = 'verybold'\ntag['another-attribute'] = 1\ntag\n# &lt;b another-attribute=\"1\" id=\"verybold\"&gt;&lt;/b&gt;\n\ndel tag['id']\ndel tag['another-attribute']\ntag\n# &lt;b&gt;&lt;/b&gt;\n\ntag['id']\n# KeyError: 'id'\nprint(tag.get('id'))\n# None\n</code></pre>"}, {"location": "beautifulsoup/#multi-valued-attributes", "title": "Multi-valued attributes", "text": "<p>HTML 4 defines a few attributes that can have multiple values. HTML 5 removes a couple of them, but defines a few more. The most common multi-valued attribute is <code>class</code> (that is, a tag can have more than one CSS class). Others include <code>rel</code>, <code>rev</code>, <code>accept-charset</code>, <code>headers</code>, and <code>accesskey</code>. Beautiful Soup presents the value(s) of a multi-valued attribute as a list:</p> <pre><code>css_soup = BeautifulSoup('&lt;p class=\"body\"&gt;&lt;/p&gt;')\ncss_soup.p['class']\n# [\"body\"]\n\ncss_soup = BeautifulSoup('&lt;p class=\"body strikeout\"&gt;&lt;/p&gt;')\ncss_soup.p['class']\n# [\"body\", \"strikeout\"]\n</code></pre> <p>If an attribute looks like it has more than one value, but it\u2019s not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup will leave the attribute alone:</p> <pre><code>id_soup = BeautifulSoup('&lt;p id=\"my id\"&gt;&lt;/p&gt;')\nid_soup.p['id']\n# 'my id'\n</code></pre> <p>When you turn a tag back into a string, multiple attribute values are consolidated:</p> <pre><code>rel_soup = BeautifulSoup('&lt;p&gt;Back to the &lt;a rel=\"index\"&gt;homepage&lt;/a&gt;&lt;/p&gt;')\nrel_soup.a['rel']\n# ['index']\nrel_soup.a['rel'] = ['index', 'contents']\nprint(rel_soup.p)\n# &lt;p&gt;Back to the &lt;a rel=\"index contents\"&gt;homepage&lt;/a&gt;&lt;/p&gt;\n</code></pre> <p>If you parse a document as XML, there are no multi-valued attributes:</p>"}, {"location": "beautifulsoup/#navigablestring", "title": "NavigableString", "text": "<p>A string corresponds to a bit of text within a tag. Beautiful Soup uses the <code>NavigableString</code> class to contain these bits of text:</p> <pre><code>tag.string\n# u'Extremely bold'\ntype(tag.string)\n# &lt;class 'bs4.element.NavigableString'&gt;\n</code></pre> <p>A <code>NavigableString</code> is just like a Python Unicode string, except that it also supports some of the features described in Navigating the tree and Searching the tree. You can convert a <code>NavigableString</code> to a Unicode string with <code>unicode()</code>:</p> <pre><code>unicode_string = unicode(tag.string)\nunicode_string\n# u'Extremely bold'\ntype(unicode_string)\n# &lt;type 'unicode'&gt;\n</code></pre> <p>You can\u2019t edit a string in place, but you can replace one string with another, using <code>replace_with()</code>:</p> <pre><code>tag.string.replace_with(\"No longer bold\")\ntag\n# &lt;blockquote&gt;No longer bold&lt;/blockquote&gt;\n</code></pre>"}, {"location": "beautifulsoup/#beautifulsoup", "title": "BeautifulSoup", "text": "<p>The <code>BeautifulSoup</code> object represents the parsed document as a whole. For most purposes, you can treat it as a <code>Tag</code> object. This means it supports most of the methods described in Navigating the tree and Searching the tree.</p>"}, {"location": "beautifulsoup/#navigating-the-tree", "title": "Navigating the tree", "text": ""}, {"location": "beautifulsoup/#going-down", "title": "Going down", "text": "<p>Tags may contain strings and other tags. These elements are the tag\u2019s children. Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag\u2019s children.</p> <p>Note that Beautiful Soup strings don\u2019t support any of these attributes, because a string can\u2019t have children.</p>"}, {"location": "beautifulsoup/#navigating-using-tag-names", "title": "Navigating using tag names", "text": "<p>The simplest way to navigate the parse tree is to say the name of the tag you want. If you want the <code>&lt;head&gt;</code> tag, just say <code>soup.head</code>:</p> <pre><code>soup.head\n# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n\nsoup.title\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\n</code></pre> <p>You can do use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <code>&lt;b&gt;</code> tag beneath the <code>&lt;body&gt;</code> tag:</p> <pre><code>soup.body.b\n# &lt;b&gt;The Dormouse's story&lt;/b&gt;\n</code></pre> <p>Using a tag name as an attribute will give you only the first tag by that name:</p> <pre><code>soup.a\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n</code></pre> <p>If you need to get all the <code>&lt;a&gt;</code> tags, or anything more complicated than the first tag with a certain name, you\u2019ll need to use one of the methods described in Searching the tree, such as <code>find_all()</code>:</p> <pre><code>soup.find_all('a')\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#contents-and-children", "title": "<code>.contents</code> and <code>.children</code>", "text": "<p>A tag\u2019s children are available in a list called <code>.contents</code>:</p> <pre><code>head_tag = soup.head\nhead_tag\n# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n\nhead_tag.contents\n[&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n\ntitle_tag = head_tag.contents[0]\ntitle_tag\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\ntitle_tag.contents\n# [u'The Dormouse's story']\n</code></pre> <p>Instead of getting them as a list, you can iterate over a tag\u2019s children using the <code>.children</code> generator:</p> <pre><code>for child in title_tag.children:\n    print(child)\n# The Dormouse's story\n</code></pre>"}, {"location": "beautifulsoup/#descendants", "title": "<code>.descendants</code>", "text": "<p>The <code>.contents</code> and <code>.children</code> attributes only consider a tag\u2019s direct children. For instance, the <code>&lt;head&gt;</code> tag has a single direct child\u2013the <code>&lt;title&gt;</code> tag:</p> <pre><code>head_tag.contents\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n</code></pre> <p>But the <code>&lt;title&gt;</code> tag itself has a child: the string <code>The Dormouse\u2019s story</code>. There\u2019s a sense in which that string is also a child of the <code>&lt;head&gt;</code> tag. The <code>.descendants</code> attribute lets you iterate over all of a tag\u2019s children, recursively: its direct children, the children of its direct children, and so on:.</p> <pre><code>for child in head_tag.descendants:\n    print(child)\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\n# The Dormouse's story\n</code></pre>"}, {"location": "beautifulsoup/#string", "title": "<code>.string</code>", "text": "<p>If a tag has only one child, and that child is a <code>NavigableString</code>, the child is made available as <code>.string</code>:</p> <pre><code>title_tag.string\n# u'The Dormouse's story'\n</code></pre> <p>If a tag\u2019s only child is another tag, and that tag has a <code>.string</code>, then the parent tag is considered to have the same <code>.string</code> as its child:</p> <pre><code>head_tag.contents\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n\nhead_tag.string\n# u'The Dormouse's story'\n</code></pre> <p>If a tag contains more than one thing, then it\u2019s not clear what <code>.string</code> should refer to, so <code>.string</code> is defined to be <code>None</code>:</p> <pre><code>print(soup.html.string)\n# None\n</code></pre>"}, {"location": "beautifulsoup/#strings-and-stripped_strings", "title": "<code>.strings</code> and <code>.stripped_strings</code>", "text": "<p>If there\u2019s more than one thing inside a tag, you can still look at just the strings. Use the <code>.strings</code> generator:</p> <pre><code>for string in soup.strings:\n    print(repr(string))\n# u\"The Dormouse's story\"\n# u'\\n\\n'\n# u\"The Dormouse's story\"\n# u'\\n\\n'\n</code></pre> <p>These strings tend to have a lot of extra whitespace, which you can remove by using the <code>.stripped_strings</code> generator instead:</p> <pre><code>for string in soup.stripped_strings:\n    print(repr(string))\n# u\"The Dormouse's story\"\n# u\"The Dormouse's story\"\n# u'Once upon a time there were three little sisters; and their names were'\n# u'Elsie'\n</code></pre>"}, {"location": "beautifulsoup/#going-up", "title": "Going up", "text": "<p>Continuing the \u201cfamily tree\u201d analogy, every tag and every string has a parent: the tag that contains it.</p>"}, {"location": "beautifulsoup/#parent", "title": "<code>.parent</code>", "text": "<p>You can access an element\u2019s parent with the <code>.parent</code> attribute.</p> <pre><code>title_tag = soup.title\ntitle_tag\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\ntitle_tag.parent\n# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n</code></pre>"}, {"location": "beautifulsoup/#parents", "title": "<code>.parents</code>", "text": "<p>You can iterate over all of an element\u2019s parents with <code>.parents</code>.</p>"}, {"location": "beautifulsoup/#going-sideways", "title": "Going sideways", "text": "<p>When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write.</p>"}, {"location": "beautifulsoup/#next_sibling-and-previous_sibling", "title": "<code>.next_sibling</code> and <code>.previous_sibling</code>", "text": "<p>You can use <code>.next_sibling</code> and <code>.previous_sibling</code> to navigate between page elements that are on the same level of the parse tree:.</p> <pre><code>sibling_soup.b.next_sibling\n# &lt;c&gt;text2&lt;/c&gt;\n\nsibling_soup.c.previous_sibling\n# &lt;b&gt;text1&lt;/b&gt;\n</code></pre> <p>The <code>&lt;b&gt;</code> tag has a <code>.next_sibling</code>, but no <code>.previous_sibling</code>, because there\u2019s nothing before the <code>&lt;b&gt;</code> tag on the same level of the tree. For the same reason, the <code>&lt;c&gt;</code> tag has a <code>.previous_sibling</code> but no <code>.next_sibling</code>:</p> <pre><code>print(sibling_soup.b.previous_sibling)\n# None\nprint(sibling_soup.c.next_sibling)\n# None\n</code></pre> <p>In real documents, the <code>.next_sibling</code> or <code>.previous_sibling</code> of a tag will usually be a string containing whitespace.</p> <pre><code>&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;\n</code></pre> <p>You might think that the .next_sibling of the first <code>&lt;a&gt;</code> tag would be the second <code>&lt;a&gt;</code> tag. But actually, it\u2019s a string: the comma and newline that separate the first <code>&lt;a&gt;</code> tag from the second:</p> <pre><code>link = soup.a\nlink\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\nlink.next_sibling\n# u',\\n'\n</code></pre> <p>The second <code>&lt;a&gt;</code> tag is actually the <code>.next_sibling</code> of the comma:</p> <pre><code>link.next_sibling.next_sibling\n# &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n</code></pre>"}, {"location": "beautifulsoup/#next_siblings-and-previous_siblings", "title": "<code>.next_siblings</code> and <code>.previous_siblings</code>", "text": "<p>You can iterate over a tag\u2019s siblings with <code>.next_siblings</code> or <code>.previous_siblings</code>:</p> <pre><code>for sibling in soup.a.next_siblings:\n    print(repr(sibling))\n# u',\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n# u' and\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;\n# u'; and they lived at the bottom of a well.'\n# None\n\nfor sibling in soup.find(id=\"link3\").previous_siblings:\n    print(repr(sibling))\n# ' and\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n# u',\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n# u'Once upon a time there were three little sisters; and their names were\\n'\n# None\n</code></pre>"}, {"location": "beautifulsoup/#searching-the-tree", "title": "Searching the tree", "text": "<p>By passing in a filter to an argument like <code>find_all()</code>, you can zoom in on the parts of the document you\u2019re interested in.</p>"}, {"location": "beautifulsoup/#kinds-of-filters", "title": "Kinds of filters", "text": ""}, {"location": "beautifulsoup/#a-string", "title": "A string", "text": "<p>The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the <code>&lt;b&gt;</code> tags in the document:</p> <pre><code>soup.find_all('b')\n# [&lt;b&gt;The Dormouse's story&lt;/b&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#a-regular-expression", "title": "A regular expression", "text": "<p>If you pass in a regular expression object, Beautiful Soup will filter against that regular expression using its <code>search()</code> method. This code finds all the tags whose names start with the letter <code>b</code>; in this case, the <code>&lt;body&gt;</code> tag and the <code>&lt;b&gt;</code> tag:</p> <pre><code>import re\nfor tag in soup.find_all(re.compile(\"^b\")):\n    print(tag.name)\n# body\n# b\n</code></pre>"}, {"location": "beautifulsoup/#a-list", "title": "A list", "text": "<p>If you pass in a list, Beautiful Soup will allow a string match against any item in that list. This code finds all the <code>&lt;a&gt;</code> tags and all the <code>&lt;b&gt;</code> tags:</p> <pre><code>soup.find_all([\"a\", \"b\"])\n# [&lt;b&gt;The Dormouse's story&lt;/b&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#a-function", "title": "A function", "text": "<p>If none of the other matches work for you, define a function that takes an element as its only argument. The function should return <code>True</code> if the argument matches, and <code>False</code> otherwise.</p> <p>Here\u2019s a function that returns <code>True</code> if a tag defines the <code>class</code> attribute but doesn\u2019t define the <code>id</code> attribute:</p> <pre><code>def has_class_but_no_id(tag):\n    return tag.has_attr('class') and not tag.has_attr('id')\n</code></pre> <p>Pass this function into <code>find_all()</code> and you\u2019ll pick up all the <code>&lt;p&gt;</code> tags:</p> <pre><code>soup.find_all(has_class_but_no_id)\n# [&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;,\n#  &lt;p class=\"story\"&gt;Once upon a time there were...&lt;/p&gt;,\n#  &lt;p class=\"story\"&gt;...&lt;/p&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#find_all", "title": "find_all()", "text": "<p>The <code>find_all()</code> method looks through a tag\u2019s descendants and retrieves all descendants that match your filters.</p> <pre><code>soup.find_all(\"title\")\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n\nsoup.find_all(\"p\", \"title\")\n# [&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]\n\nsoup.find_all(\"a\")\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n\nsoup.find_all(id=\"link2\")\n# [&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;]\n\nimport re\nsoup.find(string=re.compile(\"sisters\"))\n# u'Once upon a time there were three little sisters; and their names were\\n'\n</code></pre>"}, {"location": "beautifulsoup/#the-name-argument", "title": "The <code>name</code> argument", "text": "<p>Pass in a value for <code>name</code> and you\u2019ll tell Beautiful Soup to only consider tags with certain names. Text strings will be ignored, as will tags whose names that don\u2019t match.</p> <p>This is the simplest usage:</p> <pre><code>soup.find_all(\"title\")\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#the-keyword-arguments", "title": "The <code>keyword</code> arguments", "text": "<p>Any argument that\u2019s not recognized will be turned into a filter on one of a tag\u2019s attributes. If you pass in a value for an argument called <code>id</code>, Beautiful Soup will filter against each tag\u2019s <code>id</code> attribute:</p> <pre><code>soup.find_all(id='link2')\n# [&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;]\n</code></pre> <p>You can filter an attribute based on a string, a regular expression, a list, a function, or the value True.</p> <p>You can filter multiple attributes at once by passing in more than one keyword argument:</p> <pre><code>soup.find_all(href=re.compile(\"elsie\"), id='link1')\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;three&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#searching-by-css-class", "title": "Searching by CSS class", "text": "<p>It\u2019s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, <code>class</code>, is a reserved word in Python. Using class as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument <code>class_</code>:</p> <pre><code>soup.find_all(\"a\", class_=\"sister\")\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#the-string-argument", "title": "The string argument", "text": "<p>With <code>string</code> you can search for strings instead of tags.</p> <pre><code>soup.find_all(string=\"Elsie\")\n# [u'Elsie']\n\nsoup.find_all(string=[\"Tillie\", \"Elsie\", \"Lacie\"])\n# [u'Elsie', u'Lacie', u'Tillie']\n\nsoup.find_all(string=re.compile(\"Dormouse\"))\n[u\"The Dormouse's story\", u\"The Dormouse's story\"]\n\ndef is_the_only_string_within_a_tag(s):\n    \"\"\"Return True if this string is the only child of its parent tag.\"\"\"\n    return (s == s.parent.string)\n\nsoup.find_all(string=is_the_only_string_within_a_tag)\n# [u\"The Dormouse's story\", u\"The Dormouse's story\", u'Elsie', u'Lacie', u'Tillie', u'...']\n</code></pre> <p>Although string is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose <code>.string</code> matches your value for string.</p> <pre><code>soup.find_all(\"a\", string=\"Elsie\")\n# [&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#the-limit-argument", "title": "The limit argument", "text": "<p><code>find_all()</code> returns all the tags and strings that match your filters. This can take a while if the document is large. If you don\u2019t need all the results, you can pass in a number for <code>limit</code>.</p>"}, {"location": "beautifulsoup/#the-recursive-argument", "title": "The recursive argument", "text": "<p>If you call <code>mytag.find_all()</code>, Beautiful Soup will examine all the descendants of <code>mytag</code>. If you only want Beautiful Soup to consider direct children, you can pass in <code>recursive=False</code>.</p>"}, {"location": "beautifulsoup/#calling-a-tag-is-like-calling-find_all", "title": "Calling a tag is like calling find_all()", "text": "<p>Because <code>find_all()</code> is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object or a Tag object as though it were a function, then it\u2019s the same as calling <code>find_all()</code> on that object. These two lines of code are equivalent:</p> <pre><code>soup.find_all(\"a\")\nsoup(\"a\")\n</code></pre>"}, {"location": "beautifulsoup/#find", "title": "<code>find()</code>", "text": "<p><code>find()</code> is like <code>find_all()</code> but returning just one result.</p>"}, {"location": "beautifulsoup/#find_parent-and-find_parents", "title": "<code>find_parent()</code> and <code>find_parents()</code>", "text": "<p>These methods work their way up the tree, looking at a tag\u2019s (or a string\u2019s) parents.</p>"}, {"location": "beautifulsoup/#find_next_siblings-and-find_next_sibling", "title": "<code>find_next_siblings()</code> and <code>find_next_sibling()</code>", "text": "<p>These methods use <code>.next_siblings</code> to iterate over the rest of an element\u2019s siblings in the tree. The <code>find_next_siblings()</code> method returns all the siblings that match, and <code>find_next_sibling()</code> only returns the first one:</p> <pre><code>first_link = soup.a\nfirst_link\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\nfirst_link.find_next_siblings(\"a\")\n# [&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre> <p>To go in the other direction you can use <code>find_previous_siblings()</code> and <code>find_previous_sibling()</code></p>"}, {"location": "beautifulsoup/#modifying-the-tree", "title": "Modifying the tree", "text": ""}, {"location": "beautifulsoup/#replace_with", "title": "<code>replace_with</code>", "text": "<p><code>PageElement.replace_with()</code> removes a tag or string from the tree, and replaces it with the tag or string of your choice:</p> <pre><code>markup = '&lt;a href=\"http://example.com/\"&gt;I linked to &lt;i&gt;example.com&lt;/i&gt;&lt;/a&gt;'\nsoup = BeautifulSoup(markup)\na_tag = soup.a\n\nnew_tag = soup.new_tag(\"b\")\nnew_tag.string = \"example.net\"\na_tag.i.replace_with(new_tag)\n\na_tag\n# &lt;a href=\"http://example.com/\"&gt;I linked to &lt;b&gt;example.net&lt;/b&gt;&lt;/a&gt;\n</code></pre> <p>Sometimes it doesn't work. If it doesn't use:</p> <pre><code>a_tag.clear()\na_tag.append(new_tag)\n</code></pre>"}, {"location": "beautifulsoup/#tips", "title": "Tips", "text": ""}, {"location": "beautifulsoup/#show-content-beautified-prettified", "title": "Show content beautified / prettified", "text": "<p>Use <code>print(soup.prettify())</code>.</p>"}, {"location": "beautifulsoup/#cleaning-escaped-html-code", "title": "Cleaning escaped HTML code", "text": "<pre><code>soup = BeautifulSoup(s.replace(r\"\\\"\", '\"').replace(r\"\\/\", \"/\"), \"html.parser\")\n</code></pre>"}, {"location": "beautifulsoup/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "beets/", "title": "Beets", "text": "<p>Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music.</p> <p>Through plugins it supports:</p> <ul> <li>Fetch or calculate all the metadata you could possibly need: album art,     lyrics, genres, tempos,     ReplayGain     levels, or acoustic fingerprints.</li> <li>Get metadata from MusicBrainz, Discogs, or Beatport. Or guess metadata using     songs\u2019 filenames or their acoustic fingerprints.</li> <li>Transcode audio to any format you like.</li> <li>Check your library for duplicate tracks and albums or for albums that are     missing tracks.</li> <li>Browse your music library graphically through a Web browser and play it in any     browser that supports HTML5 Audio.</li> </ul> <p>Still, if beets doesn't do what you want yet, writing your own plugin is easy if you know a little Python. Or you can use it as a library.</p>"}, {"location": "beets/#installation", "title": "Installation", "text": "<pre><code>pip install beets\n</code></pre>"}, {"location": "beets/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Homepage</li> </ul>"}, {"location": "book_binding/", "title": "Book binding", "text": "<p>Book binding is the process of physically assembling a book of codex format from an ordered stack of paper sheets that are folded together into sections called signatures or sometimes left as a stack of individual sheets. Several signatures are then bound together along one edge with a thick needle and sturdy thread.</p>"}, {"location": "book_binding/#references", "title": "References", "text": "<ul> <li>http://tuxgraphics.org/npa/book-binding/</li> <li>https://www.instructables.com/id/How-to-bind-your-own-Hardback-Book/</li> <li>https://www.instructables.com/id/Binding-a-Book-With-Common-Materials/</li> <li>https://www.instructables.com/id/Perfect-Bound-Paperback-Notebook/</li> </ul>"}, {"location": "book_binding/#videos", "title": "Videos", "text": "<ul> <li>https://www.youtube.com/watch?v=S2FRKbQI2kY</li> <li>https://www.youtube.com/watch?v=Av_rU-yOPd4</li> <li>https://www.youtube.com/watch?v=9O4kFTOEh6k</li> <li>https://www.youtube.com/watch?v=XGQ5P8QVHSg</li> </ul>"}, {"location": "book_management/", "title": "Book Management", "text": "<p>Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions:</p> <ul> <li>Automatically index and download metadata of new books.</li> <li>Notify the user when a new book is added.</li> <li>Monitor the books of an author, and get them once they are released.</li> <li>Send books to the e-reader.</li> <li>A nice interface to browse the existent library, with the possibility of     filtering by author, genre, years, tags or series.</li> <li>An interface to preview or read the items.</li> <li>An interface to rate and review library items.</li> <li>An interface to discover new content based on the ratings and item metadata.</li> </ul> <p>I haven't yet found a single piece of software that fulfills all these needs, so we need to split it into subsystems.</p> <ul> <li>Downloader and indexer.</li> <li>Gallery browser.</li> <li>Review system.</li> <li>Content discovery.</li> </ul>"}, {"location": "book_management/#downloader-and-indexer", "title": "Downloader and indexer", "text": "<p>System that monitors the availability of books in a list of indexers, when they are available, they download it to a directory of the server. The best one that I've found is Readarr, it makes it easy to search for authors and books, supports a huge variety of indexers (such as Archive.org), and download clients (such as torrent clients).</p> <p>It can be used as a limited gallery browser, you can easily see the books of an author or series, but it doesn't yet support the automatic fetch of genres or tags.</p> <p>I haven't found an easy way of marking elements as read, prioritize the list of books to read, or add a user rating. Until these features are added (if they ever are), we need to use it in parallel with a better gallery browser.</p>"}, {"location": "book_management/#gallery-browser", "title": "Gallery browser", "text": "<p>System that shows the books in the library in a nice format, allowing the user to filter out the contents, prioritize them, mark them as read, rate them and optionally sync books with the ereader.</p> <p>Calibre-web is a beautiful solution, without trying it, it looks like it supports all of the required features, but it doesn't work well with Readarr. Readarr has support to interact with Calibre content server by defining a root folder to be managed by Calibre, but the books you want to have Readarr recognize on initial library import must already be in Calibre. Books within the folder and not in Calibre will be ignored. So you'll need to do the first import in Calibre, instead of Readarr (which is quite pleasant). Note also that you cannot add Calibre integration to a root folder after it's created.</p> <p>Calibre-web interacts directly with the Sqlite database of Calibre, so it doesn't expose the Calibre Content Server, therefore is not compatible with Readarr.</p> <p>To make it work, you'd need to have both the <code>calibre</code> server and the <code>calibre-web</code> running at the same time, which has led to database locks (1, and 2) that the <code>calibre-web</code> developer has tried to avoid by controlling the database writes, and said that:</p> <p>If you start Calibre first and afterwards Calibre-Web, Calibre indeed locks the database and doesn't allow Calibre-Web to access the database (metadata.db) file. Starting Calibre-Web and afterwards Calibre should work.</p> <p>The problem comes when Readarr writes in the database through calibre to add books, and calibre-web tries to write too to add user ratings or other metadata.</p> <p>Another option would be to only run calibre-web and automatically import the books once they are downloaded by Readarr. calibre-web is not going to support a watch directory feature, the author recommends to use a cron script to do it. I haven't tried this path yet.</p> <p>Another option would be to assume that calibre-web is not going to do any insert in the database, so it would become a read-only web interface, therefore we wouldn't be able to edit the books or rate them, one of the features we'd like to have in the gallery browser. To make sure that we don't get locks, instead of using the same file, a cron job could do an <code>rsync</code> between the database managed by <code>calibre</code> and the one used by <code>calibre-web</code>.</p> <p>Calibre implements genres with tags, behind the scenes it uses the <code>fetch-ebook-metadata</code> command line tool, that returns all the metadata in human readable form</p> <pre><code>$: fetch-ebook-metadata -i 9780061796883 -c cover.jpg\n\nTitle               : The Dispossessed\nAuthor(s)           : Ursula K. le Guin\nPublisher           : Harper Collins\nTags                : Fiction, Science Fiction, Space Exploration, Literary, Visionary &amp; Metaphysical\nLanguages           : eng\nPublished           : 2009-10-13T20:34:30.878865+00:00\nIdentifiers         : google:tlhFtmTixvwC, isbn:9780061796883\nComments            : \u201cOne of the greats\u2026.Not just a science fiction writer; a literary icon.\u201d \u2013 Stephen KingFrom the brilliant and award-winning author Ursula K. Le Guin comes a classic tale of two planets torn\napart by conflict and mistrust \u2014 and the man who risks everything to reunite them.A bleak moon settled by utopian anarchists, Anarres has long been isolated from other worlds, including its mother planet, Urras\u2014a\n civilization of warring nations, great poverty, and immense wealth. Now Shevek, a brilliant physicist, is determined to reunite the two planets, which have been divided by centuries of distrust. He will seek ans\nwers, question the unquestionable, and attempt to tear down the walls of hatred that have kept them apart.To visit Urras\u2014to learn, to teach, to share\u2014will require great sacrifice and risks, which Shevek willingly\n accepts. But the ambitious scientist's gift is soon seen as a threat, and in the profound conflict that ensues, he must reexamine his beliefs even as he ignites the fires of change.\nCover               : cover.jpg\n</code></pre> <p>Or in xml if you use the <code>-o</code> flag.</p> <p>I've checked if these tags could be automatically applied to Readarr, but their tags are meant only to be attached to Authors to apply metadata profiles. I've opened an issue to see if they plan to implement tags for books.</p> <p>It's a pity we are not going to use <code>calibre-web</code> as it also had support to sync the reading stats from Kobo.</p> <p>In the past I used gcstar and then polar bookshelf, but decided not to use them anymore for different reasons.</p> <p>In conclusion, the tools reviewed don't work as I need them to, some ugly patches could be applied and maybe it would work, but it clearly shows that they are not ready yet unless you want to invest time in it, and even if you did, it will be unstable. Until a better system shows up, I'm going to use Readarr to browse the books that I want to read, and add them to an ordered markdown file with sections as genres, not ideal, but robust as hell xD.</p>"}, {"location": "book_management/#review-system", "title": "Review system", "text": "<p>System to write reviews and rate books, if the gallery browser doesn't include it, we'll use an independent component.</p> <p>Until I find something better, I'm saving the title, author, genre, score, and review in a json file, so it's easy to import in the chosen component.</p>"}, {"location": "book_management/#content-discovery", "title": "Content discovery", "text": "<p>Recommendation system to analyze the user taste and suggest books that might like. Right now I'm monitoring the authors with Readarr to get notifications when they release a new book. I also manually go through goodreads and similar websites looking for similar books to the ones I liked.</p>"}, {"location": "book_management/#deprecated-components", "title": "Deprecated components", "text": ""}, {"location": "book_management/#polar-bookself", "title": "Polar bookself", "text": "<p>It was a very promising piece of software that went wrong :(. It had a nice interface built for incremental reading and studying with anki, and a nice tag system. It was a desktop application you installed in your computer, but since Polar 2.0 they moved into a cloud hosted service, with no possibility of self-hosting it, so you give them your books and all your data, a nasty turn of events.</p>"}, {"location": "book_management/#gcstar", "title": "GCStar", "text": "<p>The first free open source application for managing collections I used, it has an old looking desktop interface and is no longer maintained.</p>"}, {"location": "boto3/", "title": "Boto3", "text": "<p>Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services.</p>"}, {"location": "boto3/#installation", "title": "Installation", "text": "<pre><code>pip install boto3\n</code></pre>"}, {"location": "boto3/#usage", "title": "Usage", "text": ""}, {"location": "boto3/#s3", "title": "S3", "text": ""}, {"location": "boto3/#list-the-files-of-a-bucket", "title": "List the files of a bucket", "text": "<pre><code>def list_s3_by_prefix(\n    bucket: str, key_prefix: str, max_results: int = 100\n) -&gt; List[str]:\n    next_token = \"\"\n    all_keys = []\n    while True:\n        if next_token:\n            res = s3.list_objects_v2(\n                Bucket=bucket, ContinuationToken=next_token, Prefix=key_prefix\n            )\n        else:\n            res = s3.list_objects_v2(Bucket=bucket, Prefix=key_prefix)\n\n        if \"Contents\" not in res:\n            break\n\n        if res[\"IsTruncated\"]:\n            next_token = res[\"NextContinuationToken\"]\n        else:\n            next_token = \"\"\n\n        keys = [item[\"Key\"] for item in res[\"Contents\"]]\n\n        all_keys.extend(keys)\n\n        if not next_token:\n            break\n    return all_keys[-1 * max_results :]\n</code></pre> <p>The <code>boto3</code> doesn't have any way to sort the outputs of the bucket, you need to do them once you've loaded all the objects :S.</p>"}, {"location": "boto3/#ec2", "title": "EC2", "text": ""}, {"location": "boto3/#run-ec2-instance", "title": "Run EC2 instance", "text": "<p>Use the <code>run_instances</code> method of the <code>ec2</code> client. Check their docs for the different configuration options. The required ones are <code>MinCount</code> and <code>MaxCount</code>.</p> <pre><code>import boto3\n\nec2 = boto3.client('ec2')\ninstance = ec2.run_instances(MinCount=1, MaxCount=1)\n</code></pre>"}, {"location": "boto3/#get-instance-types", "title": "Get instance types", "text": "<pre><code>from pydantic import BaseModel\nimport boto3\n\nclass InstanceType(BaseModel):\n    \"\"\"Define model of the instance type.\n\n    Args:\n        id_: instance type name\n        cpu_vcores: Number of virtual cpus (cores * threads)\n        cpu_speed: Sustained clock speed in Ghz\n        ram: RAM memory in MiB\n        network_performance:\n        price: Hourly cost\n    \"\"\"\n\n    id_: str\n    cpu_vcores: int\n    cpu_speed: Optional[int] = None\n    ram: int\n    network_performance: str\n    price: Optional[float] = None\n\n    @property\n    def cpu(self) -&gt; int:\n        \"\"\"Calculate the total Ghz available.\"\"\"\n        if self.cpu_speed is None:\n            return self.cpu_vcores\n        return self.cpu_vcores * self.cpu_speed\n\n\n\ndef get_instance_types() -&gt; InstanceTypes:\n    \"\"\"Get the available instance types.\"\"\"\n    log.info(\"Retrieving instance types\")\n    instance_types: InstanceTypes = {}\n    for type_ in _ec2_instance_types(cpu_arch=\"x86_64\"):\n        instance = InstanceType(\n            id_=instance_type,\n            cpu_vcores=type_[\"VCpuInfo\"][\"DefaultVCpus\"],\n            ram=type_[\"MemoryInfo\"][\"SizeInMiB\"],\n            network_performance=type_[\"NetworkInfo\"][\"NetworkPerformance\"],\n            price=_ec2_price(instance_type),\n        )\n\n        with suppress(KeyError):\n            instance.cpu_speed = type_[\"ProcessorInfo\"][\"SustainedClockSpeedInGhz\"]\n\n        instance_types[type_[\"InstanceType\"]] = instance\n\n    return instance_types\n</code></pre>"}, {"location": "boto3/#get-instance-prices", "title": "Get instance prices", "text": "<pre><code>import json\nimport boto3\nfrom pkg_resources import resource_filename\n\ndef _ec2_price(\n    instance_type: str,\n    region_code: str = \"us-east-1\",\n    operating_system: str = \"Linux\",\n    preinstalled_software: str = \"NA\",\n    tenancy: str = \"Shared\",\n    is_byol: bool = False,\n) -&gt; Optional[float]:\n    \"\"\"Get the price of an EC2 instance type.\"\"\"\n    log.debug(f\"Retrieving price of {instance_type}\")\n    region_name = _get_region_name(region_code)\n\n    if is_byol:\n        license_model = \"Bring your own license\"\n    else:\n        license_model = \"No License required\"\n\n    if tenancy == \"Host\":\n        capacity_status = \"AllocatedHost\"\n    else:\n        capacity_status = \"Used\"\n\n    filters = [\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"termType\", \"Value\": \"OnDemand\"},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"capacitystatus\", \"Value\": capacity_status},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"location\", \"Value\": region_name},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"instanceType\", \"Value\": instance_type},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"tenancy\", \"Value\": tenancy},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"operatingSystem\", \"Value\": operating_system},\n        {\n            \"Type\": \"TERM_MATCH\",\n            \"Field\": \"preInstalledSw\",\n            \"Value\": preinstalled_software,\n        },\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"licenseModel\", \"Value\": license_model},\n    ]\n\n    pricing_client = boto3.client(\"pricing\", region_name=\"us-east-1\")\n    response = pricing_client.get_products(ServiceCode=\"AmazonEC2\", Filters=filters)\n\n    for price in response[\"PriceList\"]:\n        price = json.loads(price)\n\n        for on_demand in price[\"terms\"][\"OnDemand\"].values():\n            for price_dimensions in on_demand[\"priceDimensions\"].values():\n                price_value = price_dimensions[\"pricePerUnit\"][\"USD\"]\n\n        return float(price_value)\n    return None\n\n\ndef _get_region_name(region_code: str) -&gt; str:\n    \"\"\"Extract the region name from it's code.\"\"\"\n    endpoint_file = resource_filename(\"botocore\", \"data/endpoints.json\")\n\n    with open(endpoint_file, \"r\", encoding=\"UTF8\") as f:\n        endpoint_data = json.load(f)\n\n    region_name = endpoint_data[\"partitions\"][0][\"regions\"][region_code][\"description\"]\n    return region_name.replace(\"Europe\", \"EU\")\n</code></pre>"}, {"location": "boto3/#type-hints", "title": "Type hints", "text": "<p>AWS library doesn't have working type hints <code>-.-</code>, so you either use <code>Any</code> or dive into the myriad of packages that implement them. I've so far tried boto3_type_annotations, boto3-stubs, and mypy_boto3_builder without success. <code>Any</code> it is for now...</p>"}, {"location": "boto3/#testing", "title": "Testing", "text": "<p>Programs that interact with AWS through <code>boto3</code> create, change or get information on real AWS resources.</p> <p>When developing these programs, you don't want the testing framework to actually do those changes, as it might break things and cost you money. You need to find a way to intercept the calls to AWS and substitute them with the data their API would return. I've found three ways to achieve this:</p> <ul> <li>Manually mocking the <code>boto3</code> methods used by the program with <code>unittest.mock</code>.</li> <li>Using moto.</li> <li>Using Botocore's     Stubber.</li> </ul> <p>TL;DR</p> <p>Try to use moto, using the stubber as fallback option.</p> <p>Using <code>unittest.mock</code> forces you to know what the API is going to return and hardcode it in your tests. If the response changes, you need to update your tests, which is not good.</p> <p>moto is a library that allows you to easily mock out tests based on AWS infrastructure. It works well because it mocks out all calls to AWS automatically without requiring any dependency injection. The downside is that it goes behind <code>boto3</code> so some of the methods you need to test won't be still implemented, that leads us to the third option.</p> <p>Botocore's Stubber is a class that allows you to stub out requests so you don't have to hit an endpoint to write tests. Responses are returned first in, first out. If operations are called out of order, or are called with no remaining queued responses, an error will be raised. It's like the first option but cleaner. If you go down this path, check adamj's post on testing S3.</p>"}, {"location": "boto3/#moto", "title": "moto", "text": "<p>moto's library lets you fictitiously create and change AWS resources as you normally do with the <code>boto3</code> library. They mimic what the real methods do on fake objects.</p> <p>The Docs are awful though.</p>"}, {"location": "boto3/#install", "title": "Install", "text": "<pre><code>pip install moto\n</code></pre>"}, {"location": "boto3/#simple-usage", "title": "Simple usage", "text": "<p>To understand better how it works, I'm going to show you an understandable example, it's not the best way to use it though, go to the usage section for production ready usage.</p> <p>Imagine you have a function that you use to launch new ec2 instances:</p> <pre><code>import boto3\n\n\ndef add_servers(ami_id, count):\n    client = boto3.client('ec2', region_name='us-west-1')\n    client.run_instances(ImageId=ami_id, MinCount=count, MaxCount=count)\n</code></pre> <p>To test it we'd use:</p> <pre><code>from . import add_servers\nfrom moto import mock_ec2\n\n@mock_ec2\ndef test_add_servers():\n    add_servers('ami-1234abcd', 2)\n\n    client = boto3.client('ec2', region_name='us-west-1')\n    instances = client.describe_instances()['Reservations'][0]['Instances']\n    assert len(instances) == 2\n    instance1 = instances[0]\n    assert instance1['ImageId'] == 'ami-1234abcd'\n</code></pre> <p>The decorator <code>@mock_ec2</code> tells <code>moto</code> to capture all <code>boto3</code> calls to AWS. When we run the <code>add_servers</code> function to test, it will create the fake objects on the memory (without contacting AWS servers), and the <code>client.describe_instances</code> <code>boto3</code> method returns the data of that fake data. Isn't it awesome?</p>"}, {"location": "boto3/#usage_1", "title": "Usage", "text": "<p>You can use it with decorators, context managers, directly or with pytest fixtures.</p> <p>Being a pytest fan, the last option looks the cleaner to me.</p> <p>To make sure that you don't change the real infrastructure, ensure that your tests have dummy environmental variables.</p> <p>File: <code>tests/conftest.py</code></p> <pre><code>@pytest.fixture()\ndef _aws_credentials() -&gt; None:\n    \"\"\"Mock the AWS Credentials for moto.\"\"\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n    os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n\n\n@pytest.fixture()\ndef ec2(_aws_credentials: None) -&gt; Any:\n    \"\"\"Configure the boto3 EC2 client.\"\"\"\n    with mock_ec2():\n        yield boto3.client(\"ec2\", region_name=\"us-east-1\")\n</code></pre> <p>The <code>ec2</code> fixture can then be used in the tests to setup the environment or assert results.</p>"}, {"location": "boto3/#testing-ec2", "title": "Testing EC2", "text": "<p>If you want to add security groups to the tests, you need to create the resource first.</p> <pre><code>def test_ec2_with_security_groups(ec2: Any) -&gt; None:\n    security_group_id = ec2.create_security_group(\n        GroupName=\"TestSecurityGroup\", Description=\"SG description\"\n    )[\"GroupId\"]\n    instance = ec2.run_instances(\n        ImageId=\"ami-xxxx\",\n        MinCount=1,\n        MaxCount=1,\n        SecurityGroupIds=[security_group_id],\n    )[\"Instances\"][0]\n\n    # Test your code here\n</code></pre> <p>To add tags, use:</p> <pre><code>def test_ec2_with_security_groups(ec2: Any) -&gt; None:\n    instance = ec2.run_instances(\n        ImageId=\"ami-xxxx\",\n        MinCount=1,\n        MaxCount=1,\n        TagSpecifications=[\n            {\n                \"ResourceType\": \"instance\",\n                \"Tags\": [\n                    {\n                        \"Key\": \"Name\",\n                        \"Value\": \"instance name\",\n                    },\n                ],\n            }\n        ],\n    )[\"Instances\"][0]\n\n    # Test your code here\n</code></pre>"}, {"location": "boto3/#testing-rds", "title": "Testing RDS", "text": "<p>Use the <code>rds</code> fixture:</p> <pre><code>from moto import mock_rds2\n\n@pytest.fixture()\ndef rds(_aws_credentials: None) -&gt; Any:\n    \"\"\"Configure the boto3 RDS client.\"\"\"\n    with mock_rds2():\n        yield boto3.client(\"rds\", region_name=\"us-east-1\")\n</code></pre> <p>To create an instance use:</p> <pre><code>instance = rds.create_db_instance(\n    DBInstanceIdentifier=\"db-xxxx\",\n    DBInstanceClass=\"db.m3.2xlarge\",\n    Engine=\"postgres\",\n)[\"DBInstance\"]\n</code></pre> <p>It won't have VPC information, if you need it, create the subnet group first (you'll need the <code>ec2</code> fixture too):</p> <pre><code>subnets = [subnet['SubnetId'] for subnet in ec2.describe_subnets()[\"Subnets\"]]\nrds.create_db_subnet_group(DBSubnetGroupName=\"dbsg\", SubnetIds=subnets, DBSubnetGroupDescription=\"Text\")\ninstance = rds.create_db_instance(\n    DBInstanceIdentifier=\"db-xxxx\",\n    DBInstanceClass=\"db.m3.2xlarge\",\n    Engine=\"postgres\",\n    DBSubnetGroupName=\"dbsg\",\n)[\"DBInstance\"]\n</code></pre>"}, {"location": "boto3/#testing-s3", "title": "Testing S3", "text": "<p>Use the <code>s3_mock</code> fixture:</p> <pre><code>from moto import mock_s3\n\n@pytest.fixture()\ndef s3_mock(_aws_credentials: None) -&gt; Any:\n    \"\"\"Configure the boto3 S3 client.\"\"\"\n    with mock_s3():\n        yield boto3.client(\"s3\")\n</code></pre> <p>To create an instance use:</p> <pre><code>s3_mock.create_bucket(Bucket=\"mybucket\")\ninstance = s3_mock.list_buckets()[\"Buckets\"][0]\n</code></pre> <p>Check the official docs to check the <code>create_bucket</code> arguments.</p>"}, {"location": "boto3/#testing-route53", "title": "Testing Route53", "text": "<p>Use the <code>route53</code> fixture:</p> <pre><code>from moto import mock_route53\n\n@pytest.fixture(name='route53')\ndef route53_(_aws_credentials: None) -&gt; Any:\n    \"\"\"Configure the boto3 Route53 client.\"\"\"\n    with mock_route53():\n        yield boto3.client(\"route53\")\n</code></pre> <p>To create an instance use:</p> <p><pre><code>hosted_zone = route53.create_hosted_zone(\n    Name=\"example.com\", CallerReference=\"Test\"\n)[\"HostedZone\"]\nhosted_zone_id = re.sub(\".hostedzone.\", \"\", hosted_zone[\"Id\"])\nroute53.change_resource_record_sets(\n    ChangeBatch={\n        \"Changes\": [\n            {\n                \"Action\": \"CREATE\",\n                \"ResourceRecordSet\": {\n                    \"Name\": \"example.com\",\n                    \"ResourceRecords\": [\n                        {\n                            \"Value\": \"192.0.2.44\",\n                        },\n                    ],\n                    \"TTL\": 60,\n                    \"Type\": \"A\",\n                },\n            },\n        ],\n        \"Comment\": \"Web server for example.com\",\n    },\n    HostedZoneId=hosted_zone_id,\n)\n</code></pre> You need to first create a hosted zone. The <code>change_resource_record_sets</code> order to create the instance doesn't return any data, so if you need to work on it, use the <code>list_resource_record_sets</code> method of the route53 client (you'll need to set the <code>HostedZoneId</code> argument). If you have more than 300 records, the endpoint gives you a paginated response, so if the <code>IsTruncated</code> attribute is <code>True</code>, you need to call the method again setting the <code>StartRecordName</code> and <code>StartRecordType</code> to the <code>NextRecordName</code> and <code>NextRecordType</code> response arguments. Not nice at all.</p> <p>Pagination is not yet supported by moto, so you won't be able to test that part of your code.</p> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_hosted_zone</code>.</li> <li><code>change_resource_record_sets</code>.</li> </ul>"}, {"location": "boto3/#test-vpc", "title": "Test VPC", "text": "<p>Use the <code>ec2</code> fixture defined in the usage section.</p> <p>To create an instance use:</p> <pre><code>instance = ec2.create_vpc(\n    CidrBlock=\"172.16.0.0/16\",\n)[\"Vpc\"]\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_vpc</code>.</li> <li><code>create_subnet</code>.</li> </ul>"}, {"location": "boto3/#testing-autoscaling-groups", "title": "Testing autoscaling groups", "text": "<p>Use the <code>autoscaling</code> fixture:</p> <pre><code>from moto import mock_autoscaling\n\n@pytest.fixture(name='autoscaling')\ndef autoscaling_(_aws_credentials: None) -&gt; Any:\n    \"\"\"Configure the boto3 Autoscaling Group client.\"\"\"\n    with mock_autoscaling():\n        yield boto3.client(\"autoscaling\")\n</code></pre> <p>They don't yet support LaunchTemplates, so you'll have to use LaunchConfigurations. To create an instance use:</p> <pre><code>autoscaling.create_launch_configuration(LaunchConfigurationName='LaunchConfiguration', ImageId='ami-xxxx', InstanceType='t2.medium')\nautoscaling.create_auto_scaling_group(AutoScalingGroupName='ASG name', MinSize=1, MaxSize=3, LaunchConfigurationName='LaunchConfiguration', AvailabilityZones=['us-east-1a'])\ninstance = autoscaling.describe_auto_scaling_groups()[\"AutoScalingGroups\"][0]\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_auto_scaling_group</code>.</li> <li><code>create_launch_configuration</code>.</li> <li><code>describe_auto_scaling_groups</code>.</li> </ul>"}, {"location": "boto3/#test-security-groups", "title": "Test Security Groups", "text": "<p>Use the <code>ec2</code> fixture defined in the usage section.</p> <p>To create an instance use:</p> <pre><code>instance_id = ec2.create_security_group(\n    GroupName=\"TestSecurityGroup\", Description=\"SG description\"\n)[\"GroupId\"]\ninstance = ec2.describe_security_groups(GroupIds=[instance_id])\n</code></pre> <p>To add permissions to the security group you need to use the <code>authorize_security_group_ingress</code> and <code>authorize_security_group_egress</code> methods.</p> <pre><code>ec2.authorize_security_group_ingress(\n    GroupId=instance_id,\n    IpPermissions=[\n        {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": 80,\n            \"ToPort\": 80,\n            \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}],\n        },\n    ],\n)\n</code></pre> <p>By default, the created security group comes with an egress rule to allow all traffic. To remove rules use the <code>revoke_security_group_egress</code> and <code>revoke_security_group_ingress</code> methods.</p> <pre><code>ec2.revoke_security_group_egress(\n    GroupId=instance_id,\n    IpPermissions=[\n        {\"IpProtocol\": \"-1\", \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}]},\n    ],\n)\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_security_group</code>.</li> <li><code>describe_security_group</code>.</li> <li><code>authorize_security_group_ingress</code>.</li> <li><code>authorize_security_group_egress</code>.</li> <li><code>revoke_security_group_ingress</code>.</li> <li><code>revoke_security_group_egress</code>.</li> </ul>"}, {"location": "boto3/#test-iam-users", "title": "Test IAM users", "text": "<p>Use the <code>iam</code> fixture:</p> <pre><code>from moto import mock_iam\n\n@pytest.fixture(name='iam')\ndef iam_(_aws_credentials: None) -&gt; Any:\n    \"\"\"Configure the boto3 IAM client.\"\"\"\n    with mock_iam():\n        yield boto3.client(\"iam\")\n</code></pre> <p>To create an instance use:</p> <pre><code>instance = iam.create_user(UserName=\"User\")[\"User\"]\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_user</code></li> <li><code>list_users</code></li> </ul>"}, {"location": "boto3/#test-iam-groups", "title": "Test IAM Groups", "text": "<p>Use the <code>iam</code> fixture defined in the test IAM users section:</p> <p>To create an instance use:</p> <pre><code>user = iam.create_user(UserName=\"User\")[\"User\"]\ninstance = iam.create_group(GroupName=\"UserGroup\")[\"Group\"]\niam.add_user_to_group(GroupName=instance[\"GroupName\"], UserName=user[\"UserName\"])\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_group</code></li> <li><code>add_user_to_group</code></li> </ul>"}, {"location": "boto3/#issues", "title": "Issues", "text": "<ul> <li>Support LaunchTemplates: Once     they are, test clinv autoscaling group     adapter support for launch templates.</li> <li>Support Route53 pagination: test     clinv route53 update and update the test route53 section.</li> <li><code>cn-north-1</code> rds and autoscaling     errors: increase the timeout of     clinv, and test if the coverage has changed.</li> </ul>"}, {"location": "boto3/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "calendar_management/", "title": "Calendar Management", "text": "<p>Since the break of my taskwarrior instance I've used a physical calendar to manage the tasks that have a specific date. Can't wait for the first version of <code>pydo</code> to be finished.</p> <p>The next factors made me search for a temporal solution:</p> <ul> <li>It's taking longer than expected.</li> <li>I've started using a nextcloud calendar with some friends.</li> <li>I frequently use Google calendar at work.</li> <li>I'm sick of having to log in Nexcloud and Google to get the day's     appointments.</li> </ul> <p>To fulfill my needs the solution needs to:</p> <ul> <li>Import calendar events from different sources, basically through     the CalDAV protocol.</li> <li>Have a usable terminal user interface</li> <li>Optionally have a command line interface or python library so it's easy to make scripts.</li> <li>Optionally it can be based in python so it's easy to contribute</li> <li>Support having a personal calendar mixed with the shared ones.</li> <li>Show all calendars in the same interface</li> </ul>"}, {"location": "calendar_management/#khal", "title": "Khal", "text": "<p>Looking at the available programs I found <code>khal</code>, which looks like it may be up to the task.</p> <p>Go through the installation steps and configure the instance to have a local calendar.</p> <p>If you want to sync your calendar events through CalDAV, you need to set vdirsyncer.</p>"}, {"location": "calendar_versioning/", "title": "Calendar Versioning", "text": "<p>Calendar Versioning is a versioning convention based on your project's release calendar, instead of arbitrary numbers.</p> <p>CalVer suggests version number to be in format of: <code>YEAR.MONTH.sequence</code>. For example, <code>20.1</code> indicates a release in 2020 January, while <code>20.5.2</code> indicates a release that occurred in 2020 May, while the <code>2</code> indicates this is the third release of the month.</p> <p>You can see it looks similar to semantic versioning and has the benefit that a later release qualifies as bigger than an earlier one within the semantic versioning world (which mandates that a version number must grow monotonically). This makes it easy to use in all places where semantic versioning can be used.</p> <p>The idea here is that if the only maintained version is the latest, then we might as well use the version number to indicate the release date to signify just how old of a version you\u2019re using. You also have the added benefit that you can make calendar-based promises. For example, Ubuntu offers five years of support, therefore given version <code>20.04</code> you can quickly determine that it will be supported up to April 2025.</p>"}, {"location": "calendar_versioning/#when-to-use-calver", "title": "When to use CalVer", "text": "<p>Check the Deciding what version system to use for your programs article section.</p>"}, {"location": "calendar_versioning/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>A changelog is a file which contains a curated, chronologically ordered list of notable changes for each version of a project.</p> <p>It's purpose is to make it easier for users and contributors to see precisely what notable changes have been made between each release (or version) of the project.</p>"}, {"location": "changelog/#types-of-changes", "title": "Types of changes", "text": "<ul> <li>Added for new features.</li> <li>Changed for changes in existing functionality.</li> <li>Deprecated for soon-to-be removed features.</li> <li>Removed for now removed features.</li> <li>Fixed for any bug fixes.</li> <li>Security in case of vulnerabilities.</li> </ul>"}, {"location": "changelog/#changelog-guidelines", "title": "Changelog Guidelines", "text": "<p>Good changelogs follow the next principles:</p> <ul> <li>Changelogs are for humans, not machines.</li> <li>There should be an entry for every single version.</li> <li>The same types of changes should be grouped.</li> <li>Versions and sections should be linkable.</li> <li>The latest version comes first.</li> <li>The release date of each version is displayed.</li> <li>Mention your versioning strategy.</li> <li>Call it     <code>CHANGELOG.md</code>.</li> </ul> <p>Some examples of bad changelogs are:</p> <ul> <li> <p>Commit log diffs: The purpose of a changelog entry is to document the     noteworthy difference, often across multiple commits, to communicate them     clearly to end users. If someone wants to see the commit log diffs they can     access it through the <code>git</code> command.</p> </li> <li> <p>Ignoring Deprecations: When people upgrade from one version to another, it     should be painfully clear when something will break. It should be possible     to upgrade to a version that lists deprecations, remove what's deprecated,     then upgrade to the version where the deprecations become removals.</p> </li> <li> <p>Confusing Dates: Regional date formats vary throughout the world and it's     often difficult to find a human-friendly date format that feels intuitive to     everyone. The advantage of dates formatted like 2017-07-17 is that they     follow the order of largest to smallest units: year, month, and day. This     format also doesn't overlap in ambiguous ways with other date formats,     unlike some regional formats that switch the position of month and day     numbers. These reasons, and the fact this date format is an ISO standard,     are why it is the recommended date format for changelog entries.</p> </li> </ul>"}, {"location": "changelog/#how-to-reduce-the-effort-required-to-maintain-a-changelog", "title": "How to reduce the effort required to maintain a changelog", "text": "<p>There are two ways to ease the burden of maintaining a changelog:</p>"}, {"location": "changelog/#build-it-automatically", "title": "Build it automatically", "text": "<p>If you use Semantic Versioning you can use the commitizen tool to automatically generate the changelog each time you cut a new release by running <code>cz bump --changelog --no-verify</code>.</p> <p>The <code>--no-verify</code> part is required if you use pre-commit hooks.</p>"}, {"location": "changelog/#use-the-unreleased-section", "title": "Use the <code>Unreleased</code> section", "text": "<p>Keep an Unreleased section at the top to track upcoming changes.</p> <p>This serves two purposes:</p> <ul> <li>People can see what changes they might expect in upcoming releases.</li> <li>At release time, you can move the Unreleased section changes into a new     release version section.</li> </ul>"}, {"location": "changelog/#references", "title": "References", "text": "<ul> <li>Keep a Changelog</li> </ul>"}, {"location": "code_learning/", "title": "Learning to code", "text": "<p>Learning to code is a never ending, rewarding, frustrating, enlightening task. In this article you can see what is the generic roadmap (in my personal opinion) of a developer. As each of us is different, probably a generic roadmap won't suit your needs perfectly, if you are new to coding, I suggest you find a mentor so you can both tweak it to your case.</p>"}, {"location": "code_learning/#learning-methods", "title": "Learning methods", "text": "<p>Not all of us like to learn in the same way, first you need to choose how do you want to learn, for example through:</p> <ul> <li>Attendance-based courses.</li> <li>Online courses.</li> <li>Video courses.</li> <li>Reading books.</li> </ul> <p>Whichever you choose make sure you have regular feedback from other humans such as:</p> <ul> <li>Mentors.</li> <li>Learning communities.</li> <li>Friends.</li> </ul>"}, {"location": "code_learning/#roadmap", "title": "Roadmap", "text": "<p>The roadmap is divided in these main phases:</p> <ul> <li> <p>Beginner: You start from scratch and need to get the basic     knowledge, skills and tool set to set the learning ball rolling. At the end     of this phase you will be able to develop simple pieces of code and     collaborate with other projects with the help of a mentor.</p> </li> <li> <p>Junior: In this phase you'll learn how to:</p> <ul> <li>Improve the quality of your code through the use of testing,     documentation, linters, fixers and other techniques.</li> <li>Be more proficient with your development environment.</li> </ul> <p>At the end of the phase you'll become a senior developer that's able to code autonomously by:</p> <ul> <li>Creating a whole project from start to finish.</li> <li>Contribute to other open source projects by yourself.</li> </ul> </li> <li> <p>Senior: In this never ending phase you     keep on improving your development skills, knowledge and tools.</p> </li> </ul> <p>Don't try to rush, this is a lifetime roadmap, depending on how much time you put into learning the first steps may take from months to one or two years, the refinement phase from 2 to 8 years, and the enhancement phase never ends.</p>"}, {"location": "code_learning/#beginner", "title": "Beginner", "text": "<p>First steps are hard, you're entering a whole new world that mostly looks like magic to you. Probably you'll feel overwhelmed by the path ahead but don't fret, as every path, it's doable one step at a time.</p> <p>First steps are also exciting so try to channel all that energy into the learning process to overcome the obstacles you find in your way.</p> <p>In this section you'll learn how to start walking in the development world by:</p> <ul> <li>Setting up the development environment.</li> <li>Learning the basics</li> </ul>"}, {"location": "code_learning/#setup-your-development-environment", "title": "Setup your development environment", "text": ""}, {"location": "code_learning/#editor", "title": "Editor", "text": "<p>TBD</p>"}, {"location": "code_learning/#git", "title": "Git", "text": "<p>Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems).</p> <p>Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible.</p> <p>I've listed you some resources here on how to start. From that article I think it's also interesting that you read about:</p> <ul> <li>Pull Request process</li> <li>Git workflow</li> </ul>"}, {"location": "code_learning/#language-specific-environment", "title": "Language specific environment", "text": ""}, {"location": "code_learning/#learn-the-basics", "title": "Learn the basics", "text": "<p>Now it's the time to study, choose your desired learning method and follow them until you get the basics of the type of developer you want to become, for example:</p> <ul> <li>Frontend developer.</li> </ul> <p>In parallel it's crucial to learn Git as soon as you can, it's the main tool to collaborate with other developers and your safety net in the development workflow.</p>"}, {"location": "code_learning/#searching-for-information", "title": "Searching for information", "text": "<ul> <li>Search engines</li> <li>Github</li> </ul>"}, {"location": "code_learning/#junior", "title": "Junior", "text": "<p>TBD</p>"}, {"location": "code_learning/#senior", "title": "Senior", "text": "<p>TBD</p>"}, {"location": "cone/", "title": "Cone", "text": "<p>Cone is a mobile ledger application compatible with beancount. I use it as part of my accounting automation workflow.</p>"}, {"location": "cone/#installation", "title": "Installation", "text": "<ul> <li>Download the application from F-droid.</li> <li>It assumes that you have a txt file to store the     information. As it doesn't yet     support the edition or deletion of     transactions, I suggest you     create the <code>ledger.txt</code> file with your favorite mobile editor such as     Markor.</li> <li>Open the application and load the <code>ledger.txt</code> file.</li> </ul>"}, {"location": "cone/#usage", "title": "Usage", "text": "<p>To be compliant with my beancount ledger:</p> <ul> <li>I've initialized the <code>ledger.txt</code> file with the <code>open</code> statements of the     beancount accounts, so the transaction UI autocompletes them.</li> <li>Cone doesn't still support the beancount     format by default, so in the     description of the transaction I also introduce the payee. For example:     <code>* \"payee1\" \"Bought X</code> instead of just <code>Bought X</code>.</li> </ul> <p>If I need to edit or delete a transaction, I change it with the Markor editor.</p> <p>To send the ledger file to the computer, I use either Share via HTTP or Termux through ssh.</p>"}, {"location": "cone/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "contact/", "title": "Contact", "text": "<p>I'm available through:</p> <ul> <li>Email: <code>hello</code> at <code>labastelaria.com</code></li> </ul>"}, {"location": "cooking/", "title": "Cooking", "text": "<p>Cooking as defined in Wikipedia, is the art, science, and craft of using heat to prepare food for consumption. It sounds like an enlightening experience that brings you joy. Reality then slaps you in the face yet once again. It's very different to cook because you want to, than cooking because you need to. I love to eat, but have always hated to cook, mainly because I've always seen cooking with that second view.</p> <p>Being something disgusting that I had to do, pushed me to batch cook once a week as quickly as possible, and to buy prepared food from the local squat center's tavern. Now I aim to shift my point of view to enjoy the time invested preparing food. Two are the main reasons: I'm going to spend a great amount of my life in front of the stove, so I'd better enjoy it, and probably the end result would be better for my well being.</p> <p>One way that can help me with the switch, is to understand the science behind it and be precise with the process. Thus this section was born, I'll start with the very basics and build from there on.</p>"}, {"location": "cooking_basics/", "title": "Cooking Basics", "text": "<p>All great recipes are based on the same basic principles and processes, these are the cooking snippets that I've needed.</p>"}, {"location": "cooking_basics/#boiling-an-egg", "title": "Boiling an egg", "text": "<p>Cooking an egg well is a matter of time.</p> <ul> <li>Put enough water in the pot so that the eggs are completely covered.</li> <li>Add a pinch of salt and a dash of vinegar.</li> <li>Let the water boil. Use a kettle to heat it if you have one.</li> <li>Add the eggs.</li> <li>Depending on the type of egg you want, you need to wait more or less time:<ul> <li>5-6 minutes: You'll get soft boiled eggs, whose yolk is liquid and the     white is semi-liquid.</li> <li>7 minutes: mollet egs, with semi-liquid yolk and curdled white.</li> <li>10-12 minutes: boiled eggs, compact white and curdled yolk.</li> </ul> </li> <li>Pour off the hot water, shake it gently to crack the eggs, and add cold water,     with even a few ice cubes.</li> <li>Wait 5 minutes if you want to serve them warm, or 15 otherwise, and then peel     them under the same water.</li> </ul> <p>Here are some tips to improve your chances to get the perfect egg:</p> <ul> <li>Use fresh eggs, when they've been in the fridge for a while, they get     dehydrated and the air that's inside gets expanded. That's why, when you put     an egg into a glass of water, if it doesn't stay at the bottom you'd better     not use it.</li> <li>Take them out of the fridge an hour before cooking them. (Yeah, like you're     going to remember to do it :P).</li> </ul>"}, {"location": "cooking_basics/#cooking-legumes", "title": "Cooking legumes", "text": "<p>Legumes are wonderful, to squeeze their value remember to:</p> <ul> <li>Don't use old ones: If you're legumes are older than a year, they can be     old. They loose water with the time up to a point that they can be     impossible to cook.</li> <li> <p>Soak them: Some legumes like the lentils don't need to be soaked, but for most     of them it's better to be hydrated before putting them in the pot.</p> <p>For chickpeas and beans, the best is to soak them for 10 to 12 hours. They'll drink the water, so add it until you double the volume of the legumes. Once done, discard that water, rinse them and use new one for the pot. That way you'll prevent the acids and oligosaccharides that promotes a heavy digestion. * Don't scare them: Don't cut the cooking with cold water, they won't help you avoid farting and for some great chefs it's one of the worst errors you can do, at least with the chickpeas. Remember always to have enough water from the start to avoid this situation. * Know when to add them in the pot: chickpeas need to be added when the water is already boiling</p> </li> </ul> <p>When you're using boiled legumes in your recipes, be careful, after the hydration, they weight the double!</p>"}, {"location": "cooking_basics/#boil-chickpeas-when-youve-forgotten-to-soak-them", "title": "Boil chickpeas when you've forgotten to soak them", "text": "<p>Soaked chickpeas take one or two hours to cook in a normal pot, 20 to 30 in a fast pot, which saves a ton of energy.</p> <p>If you forgot to soak them, add a level teaspoon of baking soda to the pot and cook them as usual. When not using a fast pot, you'll need to periodically remove the foam that will be created. The problem with this method is that you don't discard the first water round, and they can be more indigestible. Another option is to cook them for an hour, change the water and then cook them again.</p>"}, {"location": "cooking_software/", "title": "Cooking software", "text": "<p>While using grocy to manage my recipes I've found what I want from a recipe manager:</p> <ul> <li>Write recipes in plaintext files.</li> <li>Import recipes from urls.</li> <li>Import other recipes in a recipe. To avoid code repetition. For example if     I want to do raviolis carbonara, I want to have a basic recipe to create raviolis     and another for carbonara, then a normal recipe that imports both.</li> <li>Define processes that can be imported as steps in recipes, for example boil     water</li> <li>Change the number the servings</li> <li>Annotate lessons learned</li> <li>Translate common cooking units (spoonful, a piece) into metric system units of     volume and weight.</li> <li>Helper to follow the recipe while cooking</li> <li>Keep track of the evolution of the recipe each time you cook it with     deviations from plan, evaluation of the result dish, lessons learned and     changes made to the recipe.</li> <li>Attach image to the recipe</li> <li>Specify the storage size (a ration of lentils uses 2cm of a tupper).</li> <li>Give rating to recipes</li> <li>Track the number of times you do a recipe</li> <li>Grade the maturity level of a recipe by the number of times done and the     number of changes.</li> <li>Browse all available recipes with the possibility of:<ul> <li>Search by name</li> <li>Search by ingredient</li> <li>Filter by season</li> <li>Filter by meal type (lunch, dinner, dessert...)</li> <li>Sort by rating</li> <li>Sort by number of times cooked</li> <li>Sort by last time cooked</li> <li>Show never cooked recipes</li> </ul> </li> <li>Do a meal plan</li> <li>Create reusable meal plans</li> <li>Be able to talk to the inventory management tool to:<ul> <li>See if there are enough ingredients.</li> <li>Fill up the shopping list.</li> </ul> </li> <li>Select which recipes I want to cook this month, the program should show me the     available ones taking into account the season.</li> <li>Keep track of the season of the ingredients, so you can mark an ingredient as     in season or out of season, and that will allow you to select which recipes     to add, or tell you which recipes are no longer going to be available.</li> <li>Define variations of a recipe, imagine that instead of red pepper you have     green</li> <li>Be able to tell the brand of an ingredient</li> <li>Define the cooking tools to use, which will be shown when preparing a recipe</li> <li>Define the preparing and clean steps of a cooking tool.</li> <li>Show the number of inactivity interruptions, their time and the total amount     of inactive time.</li> <li>Calculate the recipe time from the times of the processes involved and     subsequent recipes</li> <li>Be able to define where each process is carried out, what is the distance     between the places</li> <li>Suggest optimizations in the cooking process:<ul> <li>Reorder of steps to reduce waiting time and movement</li> </ul> </li> <li>Be able to start from the basic features and incrementally add on it</li> </ul>"}, {"location": "cooking_software/#software-review", "title": "Software review", "text": ""}, {"location": "cooking_software/#grocy", "title": "Grocy", "text": "<p>Grocy is an awesome software to manage your inventory, it's also a good one to manage your recipes, with an awesome integration with the inventory management. In fact, I've been using it for some years.</p> <p>Nevertheless, you can't:</p> <ul> <li>Write them in plaintext.</li> <li>Reuse recipes with other recipes in a pleasant way.</li> <li>Do variations of a recipe.</li> <li>Track the variations of a recipe.</li> </ul> <p>So I think whatever I choose to manage recipes needs to be able to speak to a Grocy instance at least to get an idea of the stock available and to complete the shopping list.</p>"}, {"location": "cooking_software/#cooklang", "title": "Cooklang", "text": "<p><code>Cooklang</code> looks real good, you write the recipes in plaintext but the syntax is not as smooth as I'd like:</p> <pre><code>Then add @salt and @ground black pepper{} to taste.\n</code></pre> <p>I'd like the parser to be able to detect the ingredient <code>salt</code> without the need of the <code>@</code>, and I don't either like how to specify the measurements:</p> <pre><code>Place @bacon strips{1%kg} on a baking sheet and glaze with @syrup{1/2%tbsp}.\n</code></pre> <p>On the other side there is more less popular:</p> <ul> <li>Their spec has their spec 400 stars.</li> <li>It has a cli, and an Android and ios apps</li> <li>There is a vim plugin for the     syntax.</li> <li>You can import recipes from urls.</li> <li>There are some recipe     books, some of     them     look nice with a <code>mkdocs</code> frontend.</li> </ul> <p>It could be used as part of the system, but it falls short in many of my desired features.</p>"}, {"location": "cooking_software/#kookbook", "title": "KookBook", "text": "<p><code>KookBook</code> is KDE solution for plaintext recipe management. Their documentation is sparse and not popular at all. I don't feel like using it.</p>"}, {"location": "cooking_software/#recipesage", "title": "RecipeSage", "text": "<p>RecipeSage is free personal recipe keeper, meal planner, and shopping list manager for Web, IOS, and Android.</p> <p>Quickly capture and save recipes from any website simply by entering the website URL. Sync your recipes, meal plans, and shopping lists between all of your devices. Share your recipes, shopping lists, and meal plans with family and friends.</p> <p>It looks good, but I'd use <code>grocy</code> instead.</p>"}, {"location": "cooking_software/#mealie", "title": "Mealie", "text": "<p>Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor.</p> <p>It does have an API, but it looks too complex. Maybe to be used as a backend to retrieve recipes from the internet, but no plaintext recipes.</p>"}, {"location": "cooking_software/#chowdown", "title": "Chowdown", "text": "<p><code>Chowdown</code> is a simple, plaintext recipe database for hackers. It has nice features:</p> <ul> <li>You write your recipes in Markdown.</li> <li>You can easily import recipes in other recipes.</li> </ul> <p>An example would be:</p> <pre><code>---\n\nlayout: recipe\ntitle:  \"Broccoli Beer Cheese Soup\"\nimage: broccoli-beer-cheese-soup.jpg\ntags: sides, soups\n\ningredients:\n- 4 tablespoons butter\n- 1 cup diced onion\n- 1/2 cup shredded carrot\n- 1/2 cup diced celery\n- 1 tablespoon garlic\n- 1/4 cup flour\n- 1 quart chicken broth\n- 1 cup heavy cream\n- 10 ounces muenster cheese\n- 1 cup white white wine\n- 1 cup pale beer\n- 1 teaspoon Worcestershire sauce\n- 1/2 teaspoon hot sauce\n\ndirections:\n- Start with butter, onions, carrots, celery, garlic until cooked down\n- Add flour, stir well, cook for 4-5 mins\n- Add chicken broth, bring to a boil\n- Add wine and reduce to a simmer\n- Add cream, cheese, Worcestershire, and hot sauce\n- Serve with croutons\n\n---\n\nThis recipe is inspired by one of my favorites, Gourmand's Beer Cheese Soup, which uses Shiner Bock. Feel free to use whatever you want, then go to [Gourmand's](http://lovethysandwich.com) to have the real thing.\n</code></pre> <p>Or using other recipes:</p> <pre><code>---\n\nlayout: recipe\ntitle:  \"Red Berry Tart\"\nimage: red-berry-tart.jpg\ntags: desserts\n\ndirections:\n- Bake the crust and let it cool\n- Make the custard, pour into crust\n- Make the red berry topping, spread over the top\n\ncomponents:\n- Graham Cracker Crust\n- Vanilla Custard Filling\n- Red Berry Dessert Topping\n\n---\n\nA favorite when I go to BBQs (parties, hackathons, your folks' place), this red berry tart is fairly easy to make and packs a huge wow factor.\n</code></pre> <p>Where <code>Graham Cracker Crust</code> is another recipe. The outcome is nice too.</p> <p>Downsides are:</p> <ul> <li>It's written in HTML and javascript.</li> <li>They don't answer issues so it     looks unmaintained.</li> </ul> <p>It redirects to an interesting schema of a recipe.</p> <p>https://github.com/clarklab/chowdown https://raw.githubusercontent.com/clarklab/chowdown/gh-pages/_recipes/broccoli-cheese-soup.md https://www.paprikaapp.com/</p>"}, {"location": "cooking_software/#recipes", "title": "Recipes", "text": "<p>https://docs.tandoor.dev/features/authentication/</p>"}, {"location": "cooking_software/#chef", "title": "Chef", "text": ""}, {"location": "cpu/", "title": "CPU", "text": "<p>A central processing unit or CPU, also known as the brain of the server, is the electronic circuitry that executes instructions comprising a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program.</p> <p>The main processor factors you should consider the most while buying a server are:</p> <ul> <li>Clock speed: Is measured in gigahertz (GHz). The higher the clock speed,     the faster the processor will calculate. The     clock speed along with the bit width conveys us that in a single second how     much data can flow. If a processor has a speed of 2.92 GHz and the bit width     of 32 bits, then it means that it can process almost 3 billion units of 32     bits of data per second.</li> <li>Cores: Every processor core handles individual processing tasks. A multi-core     processor can process multiple computing instructions in parallel..</li> <li>Threads: Threads refer to the number of processors a chip can handle     simultaneously. The difference between threads and cores is that cores are     physical processing units in charge of computing processes. In contrast,     threads are virtual components that run tasks as software would.</li> <li> <p>Cache: Used by the processor to speed up the access to instructions and data     between the processor and the RAM. There are three types of cache you will     come across while looking at the specification sheet:</p> <ul> <li>L1: Is the fastest, but cramped.</li> <li>L2: Is roomier, but slower.</li> <li>L3: Is spacious, but comparatively slower than above two.</li> </ul> </li> </ul>"}, {"location": "cpu/#speed", "title": "Speed", "text": "<p>To achieve the same speed you can play with the speed of a core and the number of cores.</p> <p>Having more cores and lesser clock speed has the next advantages:</p> <ul> <li>Processors with higher cores deliver higher performance and are cost-effective.</li> <li>Applications supporting multi-threading would benefit from a higher number of cores.</li> <li>Multi-threading support for applications will continue to enhance over time.</li> <li>Easily run more applications without experiencing the performance drop.</li> <li>Great for virtualization and running multiple virtual machines.</li> </ul> <p>With the disadvantages that it offers lower single threaded performance.</p> <p>On the other hand, using fewer cores and higher clock speed gives the next advantages:</p> <ul> <li>Offers better single threaded performance.</li> <li>Lower cost.</li> </ul> <p>And the next disadvantages</p> <ul> <li>Due to the lower number of cores, it becomes difficult to split between     applications.</li> <li>Not so strong as multi-threading performance.</li> </ul>"}, {"location": "cpu/#providers", "title": "Providers", "text": ""}, {"location": "cpu/#amd", "title": "AMD", "text": "<p>The Ryzen family is broken down into four distinct branches:</p> <ul> <li>Ryzen 3: entry-level.</li> <li>Ryzen 5: mainstream.</li> <li>Ryzen 7: performance</li> <li>Ryzen 9:  high-end</li> </ul> <p>They're all great chips in their own ways, but some certainly offer more value than others, and for many, the most powerful chips will be complete overkill.</p>"}, {"location": "cpu/#market-analysis", "title": "Market analysis", "text": "Property Ryzen 7 5800x Ryzen 5 5600x Ryzen 7 5700x Ryzen 5 5600G Cores 8 6 8 6 Threads 16 12 16 12 Clock 3.8 3.7 3.4 3.9 Socket AM4 AM4 AM4 AM4 PCI 4.0 4.0 4.0 3.0 Thermal Not included Wraith Stealth Not included Wraith Stealth Default TDP 105W 65W 65W 65W System Mem spec &gt;= 3200 MHz &gt;= 3200 MHz &gt;= 3200 MHz &gt;= 3200 MHz Mem type DDR4 DDR4 DDR4 DDR4 Price 315 232 279 179 <p>The data was extracted from AMD's official page.</p> <p>They all support the chosen RAM and the motherboard.</p> <p>I'm ruling out Ryzen 7 5800x because it's too expensive both on monetary and power consumption terms. Also ruling out Ryzen 5 5600G because it has comparatively bad properties.</p> <p>Between Ryzen 5 5600x and Ryzen 7 5700x, after checking these comparisons (1, 2) it looks like:</p> <ul> <li>Single core performance is similar.</li> <li>7 wins when all cores are involved.</li> <li>7 is more power efficient.</li> <li>7 is better rated.</li> <li>7 is newer (1.5 years).</li> <li>7 has around 3.52 GB/s (7%) higher theoretical RAM memory bandwidth</li> <li>They have the same cache</li> <li>7 has 5 degrees less of max temperature</li> <li>They both support ECC</li> <li>5 has a greater market share</li> <li>5 is 47$ cheaper</li> </ul> <p>I think that for 47$ it's work the increase on cores and theoretical RAM memory bandwidth. Therefore I'd go with the Ryzen 7 5700x.</p>"}, {"location": "cpu/#cpu-coolers", "title": "CPU coolers", "text": "<p>One of the most important decisions when building your PC, especially if you plan on overclocking, is choosing the best CPU cooler. The cooler is often a limiting factor to your overclocking potential, especially under sustained loads. Your cooler choice can also make a substantial difference in noise output. So buying a cooler that can handle your best CPU\u2019s thermal output/heat, (be it at stock settings or when overclocked) is critical to avoiding throttling and achieving your system\u2019s full potential, while keeping the whole system quiet.</p> <p>CPU Coolers come in dozens of shapes and sizes, but most fall into these categories:</p> <ul> <li>Air: made of some combination of metal heatsinks and fans, come in all shapes     and sizes and varying thermal dissipation capacities (sometimes listed as     TDP). High-end air coolers these days rival many all-in-one (AIO) liquid     coolers that have become popular in the market over the past several years.</li> <li>Closed-loop</li> <li>All-in one (AIO)custom</li> </ul> <p>AIO or closed-loop coolers can be (but aren\u2019t always) quieter than air coolers, without requiring the complications of cutting and fitting custom tubes and maintaining coolant levels after setup. AIOs have also become increasingly resistant to leaks over the years, and are easier to install. But they require room for a radiator, so may require a larger case than some air coolers.</p> <p>Here\u2019s a quick comparison of some of the pros and cons of air and liquid cooling.</p> <p>Liquid Cooling Pros:</p> <ul> <li>Highest cooling potential .</li> <li>Fewer clearance issues around the socket.</li> </ul> <p>Liquid Cooling Cons:</p> <ul> <li>Price is generally higher (and price to performance ratio is typically lower     as well).</li> <li>(Slim) possibility of component-damaging leaks.</li> </ul> <p>Air Cooling Pros:</p> <ul> <li>Price is generally lower (better price to performance ratio).</li> <li>No maintenance required.</li> <li>Zero chance for leaks.</li> </ul> <p>Air Cooling Cons:</p> <ul> <li>Limited cooling potential.</li> <li>Increased fitment issues around the socket with memory, fans, etc).</li> <li>Can be heavy/difficult to mount.</li> </ul>"}, {"location": "cpu/#quick-shopping-tips", "title": "Quick shopping tips", "text": "<ul> <li> <p>Own a recent Ryzen CPU?: You may not need to buy a cooler, even for     overclocking. All Ryzen 300- and 2000-series processors and some older Ryzen     models ship with coolers, and many of them can handle moderate overclocks.     If you want the best CPU clock speed possible, you\u2019ll still want to buy an     aftermarket cooler, but for many Ryzen owners, that won\u2019t be necessary.</p> </li> <li> <p>Check clearances before buying: Big air coolers and low-profile models can     bump up against tall RAM and even VRM heat sinks sometimes. And tall coolers     can butt up against your case door or window. Be sure to check the     dimensions and advertised clearances of any cooler and your case before     buying.</p> </li> <li> <p>More fans=better cooling, but more noise: The coolers that do the absolute     best job of moving warm air away from your CPU and out of your case are also     often the loudest. If fan noise is a problem for you, you\u2019ll want a cooler     that does a good job of balancing noise and cooling.</p> </li> <li> <p>Make sure you can turn off RGB: Many coolers these days include RGB fans and     / or lighting. This can be a fun way to customize the look of your PC. But     be sure there\u2019s a way, either via a built-in controller or when plugging the     cooler into a compatible RGB motherboard header, to turn the lights off     without turning off the PC.</p> </li> <li> <p>Check that the CPU has GPU if you don't want to use an external graphic card.     Otherwise the BIOS won't start.</p> </li> </ul>"}, {"location": "cpu/#market-analysis_1", "title": "Market analysis", "text": "<p>After a quick review I'm deciding between the Dark Rock 4 and the Enermax ETS-T50 Axe. The analysis is done after reading Tomshardware reviews (1, 2).</p> <p>They are equal in:</p> <ul> <li>CPU core and motherboard temperatures.</li> <li>Have installation videos.</li> </ul> <p>The Enermax has the advantages:</p> <ul> <li>Is much more silent (between 2 and 4 dB).</li> <li>It has better acoustic efficiency (Relative temperature against Relative     Noise) between 15% and 29%.</li> <li>More TDP</li> <li>Much cheaper (25 EUR)</li> <li>If you have a Phillips screwdriver (normal cross screwdriver) you don't get     another one.</li> </ul> <p>All in all the Enermax ETS-T50 Axe is a better one, but after checking the sizes, my case limit on the height of the CPU cooler is 160mm and the Enermax is 163mm... The Cooler Master Masterair ma610p has 166mm, so it's out of the question too. The Dark Rock 4 max height is 159mm. I don't know if I should bargain.</p> <p>To be in the safe side I'll go with the Dark Rock 4</p>"}, {"location": "cpu/#ryzen-recommended-coolers", "title": "Ryzen recommended coolers", "text": ""}, {"location": "cpu/#cpu-thermal-paste", "title": "CPU Thermal paste", "text": "<p>Thermal paste is designed to minimize microscopic air gaps and irregularities between the surface of the cooler and the CPU's IHS (integrated heat spreader), the piece of metal which is built into the top of the processor.</p> <p>Good thermal paste can have a profound impact on your performance, because it will allow your processor to transfer more of its waste heat to your cooler, keeping your processor running cool.</p> <p>Most pastes are comprised of ceramic or metallic materials suspended within a proprietary binder which allows for easy application and spread as well as simple cleanup.</p> <p>These thermal pastes can be electrically conductive or non-conductive, depending on their specific formula. Electrically conductive thermal pastes can carry current between two points, meaning that if the paste squeezes out onto other components, it can cause damage to motherboards and CPUs when you switch on the power. A single drop out of place can lead to a dead PC, so extra care is imperative.</p> <p>Liquid metal compounds are almost always electrically conductive, so while these compounds provide better performance than their paste counterparts, they require more focus and attention during application. They are very hard to remove if you get some in the wrong place, which would fry your system.</p> <p>In contrast, traditional thermal paste compounds are relatively simple for every experience level. Most, but not all, traditional pastes are electrically non-conductive.</p> <p>Most cpu coolers come with their own thermal paste, so check yours before buying another one.</p>"}, {"location": "cpu/#market-analysis_2", "title": "Market analysis", "text": "Model ProlimaTech PK-3 Thermal Grizzly Kryonaut Cooler Master MasterGel Pro v2 Electrical conductive No No No Thermal Conductivity 11.2 W/mk 12.5 W/mk 9 W/mk Ease of Use 4.5 4.5 4.5 Relative Performance 4.0 4.0 3.5 Price per gram 6.22 9.48 2.57 <p>The best choice would be the ProlimaTech but the package sold are expensive because it has many grams.</p> <p>In my case, my cooler comes with the thermal paste so I'd start with that before spending 20$ more.</p>"}, {"location": "cpu/#installation", "title": "Installation", "text": "<p>When installing an AM4 CPU in the motherboard, rotate the CPU so that the small arrow on one of the corners of the chip matches the arrow on the corner of the motherboard socket.</p>"}, {"location": "cpu/#references", "title": "References", "text": "<ul> <li>Tom's hardware CPU guide</li> <li>Tom's hardware CPU cooling guide</li> <li>How to select the best processor for your server</li> <li>Cloudzy best server processor</li> </ul>"}, {"location": "css/", "title": "CSS", "text": "<p>CSS stands for Cascading Style Sheets and is used to format the layout of a webpage.</p> <p>With CSS, you can control the color, font, the size of text, the spacing between elements, how elements are positioned and laid out, what background images or background colors are to be used, different displays for different devices and screen sizes, and much more!</p>"}, {"location": "css/#using-css-in-html", "title": "Using CSS in HTML", "text": "<p>CSS can be added to HTML documents in 3 ways:</p> <ul> <li>Inline: by using the style attribute inside HTML elements.</li> <li>Internal: by using a"}, {"location": "cypress/", "title": "Cypress", "text": "<p>Cypress is a next generation front end testing tool built for the modern web.</p> <p>Cypress enables you to write all types of tests:</p> <ul> <li>End-to-end tests</li> <li>Integration tests</li> <li>Unit tests</li> </ul> <p>Cypress can test anything that runs in a browser.</p>"}, {"location": "cypress/#features", "title": "Features", "text": "<ul> <li>Time Travel: Cypress takes snapshots as your tests run. Hover over commands     in the Command Log to see exactly what happened at each step.</li> <li>Debuggability: Stop guessing why your tests are failing. Debug directly from     familiar tools like Developer Tools. Our readable errors and stack traces     make debugging lightning fast.</li> <li>Automatic Waiting: Never add waits or sleeps to your tests. Cypress     automatically waits for commands and assertions before moving on. No more     async hell.</li> <li>Spies, Stubs, and Clocks: Verify and control the behavior of functions,     server responses, or timers. The same functionality you love from unit     testing is right at your fingertips.</li> <li>Network Traffic Control: Easily control, stub, and test edge cases without     involving your server. You can stub network traffic however you like.</li> <li>Consistent Results: Our architecture doesn\u2019t use Selenium or WebDriver. Say     hello to fast, consistent and reliable tests that are flake-free.</li> <li>Screenshots and Videos: View screenshots taken automatically on failure, or     videos of your entire test suite when run from the CLI.</li> <li>Cross browser Testing: Run tests within Firefox and Chrome-family browsers     (including Edge and Electron) locally and optimally in a Continuous     Integration pipeline.</li> </ul> <p>Check the key differences page to see more benefits of using the tool.</p>"}, {"location": "cypress/#installation", "title": "Installation", "text": "<pre><code>npm install cypress --save-dev\n</code></pre>"}, {"location": "cypress/#usage", "title": "Usage", "text": "<p>You first need to open cypress with <code>npx cypress open</code>.</p> <p>To get an overview of cypress' workflow follow the Writing your first test tutorial</p> <p>Tests live in the <code>cypress</code> directory, if you create a new file in the <code>cypress/integration</code> directory it will automatically show up in the UI. Cypress monitors your spec files for any changes and automatically displays any changes.</p> <p>Writing tests is meant to be simple, for example:</p> <pre><code>describe('My First Test', () =&gt; {\n  it('Does not do much!', () =&gt; {\n    expect(true).to.equal(true)\n  })\n})\n</code></pre>"}, {"location": "cypress/#test-structure", "title": "Test structure", "text": "<p>The test interface, borrowed from Mocha, provides <code>describe()</code>, <code>context()</code>, <code>it()</code> and <code>specify()</code>. <code>context()</code> is identical to <code>describe()</code> and <code>specify()</code> is identical to <code>it()</code>.</p> <pre><code>describe('Unit test our math functions', () =&gt; {\n  context('math', () =&gt; {\n    it('can add numbers', () =&gt; {\n      expect(add(1, 2)).to.eq(3)\n    })\n\n    it('can subtract numbers', () =&gt; {\n      expect(subtract(5, 12)).to.eq(-7)\n    })\n\n    specify('can divide numbers', () =&gt; {\n      expect(divide(27, 9)).to.eq(3)\n    })\n\n    specify('can multiply numbers', () =&gt; {\n      expect(multiply(5, 4)).to.eq(20)\n    })\n  })\n})\n</code></pre>"}, {"location": "cypress/#hooks", "title": "Hooks", "text": "<p>Hooks are helpful to set conditions that you want to run before a set of tests or before each test. They're also helpful to clean up conditions after a set of tests or after each test.</p> <pre><code>before(() =&gt; {\n  // root-level hook\n  // runs once before all tests\n})\n\nbeforeEach(() =&gt; {\n  // root-level hook\n  // runs before every test block\n})\n\nafterEach(() =&gt; {\n  // runs after each test block\n})\n\nafter(() =&gt; {\n  // runs once all tests are done\n})\n\ndescribe('Hooks', () =&gt; {\n  before(() =&gt; {\n    // runs once before all tests in the block\n  })\n\n  beforeEach(() =&gt; {\n    // runs before each test in the block\n  })\n\n  afterEach(() =&gt; {\n    // runs after each test in the block\n  })\n\n  after(() =&gt; {\n    // runs once after all tests in the block\n  })\n})\n</code></pre> <p>!!! warning \"Before writing <code>after()</code> or <code>afterEach()</code> hooks, read the anti-pattern of cleaning up state with <code>after()</code> or <code>afterEach()</code>\"</p>"}, {"location": "cypress/#skipping-tests", "title": "Skipping tests", "text": "<p>You can skip tests in the next ways:</p> <pre><code>describe('TodoMVC', () =&gt; {\n  it('is not written yet')\n\n  it.skip('adds 2 todos', function () {\n    cy.visit('/')\n    cy.get('.new-todo').type('learn testing{enter}').type('be cool{enter}')\n    cy.get('.todo-list li').should('have.length', 100)\n  })\n\n  xit('another test', () =&gt; {\n    expect(false).to.true\n  })\n})\n</code></pre>"}, {"location": "cypress/#querying-elements", "title": "Querying elements", "text": "<p>Cypress automatically retries the query until either the element is found or a set timeout is reached. This makes Cypress robust and immune to dozens of common problems that occur in other testing tools.</p>"}, {"location": "cypress/#query-by-html-properties", "title": "Query by HTML properties", "text": "<p>You need to find the elements to act upon, usually you do it with the <code>cy.get()</code> function. For example:</p> <pre><code>cy.get('.my-selector')\n</code></pre> <p>Cypress leverages jQuery's powerful selector engine and exposes many of its DOM traversal methods to you so you can work with complex HTML structures. For example:</p> <pre><code>cy.get('#main-content').find('.article').children('img[src^=\"/static\"]').first()\n</code></pre> <p>If you follow the Write testable code guide, you'll select elements by the <code>data-cy</code> element.</p> <pre><code>cy.get('[data-cy=submit]')\n</code></pre> <p>You'll probably write that a lot, that's why it's useful to define the next commands in <code>/cypress/support/commands.ts</code>.</p> <pre><code>Cypress.Commands.add('getById', (selector, ...args) =&gt; {\n  return cy.get(`[data-cy=${selector}]`, ...args)\n})\n\nCypress.Commands.add('getByIdLike', (selector, ...args) =&gt; {\n  return cy.get(`[data-cy*=${selector}]`, ...args)\n})\n\nCypress.Commands.add('findById', {prevSubject: true}, (subject, selector, ...args) =&gt; {\n  return subject.find(`[data-cy=${selector}]`, ...args)\n})\n</code></pre> <p>So you can now do <pre><code>cy.getById('submit')\n</code></pre></p>"}, {"location": "cypress/#query-by-content", "title": "Query by content", "text": "<p>Another way to locate things -- a more human way -- is to look them up by their content, by what the user would see on the page. For this, there's the handy <code>cy.contains()</code> command, for example:</p> <pre><code>// Find an element in the document containing the text 'New Post'\ncy.contains('New Post')\n\n// Find an element within '.main' containing the text 'New Post'\ncy.get('.main').contains('New Post')\n</code></pre> <p>This is helpful when writing tests from the perspective of a user interacting with your app. They only know that they want to click the button labeled \"Submit\". They have no idea that it has a type attribute of submit, or a CSS class of <code>my-submit-button</code>.</p>"}, {"location": "cypress/#changing-the-timeout", "title": "Changing the timeout", "text": "<p>The querying methods accept the <code>timeout</code> argument to change the default timeout.</p> <pre><code>// Give this element 10 seconds to appear\ncy.get('.my-slow-selector', { timeout: 10000 })\n</code></pre>"}, {"location": "cypress/#select-by-position-in-list", "title": "Select by position in list", "text": "<p>Inside our list, we can select elements based on their position in the list, using <code>.first()</code>, <code>.last()</code> or <code>.eq()</code> selector.</p> <pre><code>cy\n  .get('li')\n  .first(); // select \"red\"\n\ncy\n  .get('li')\n  .last(); // select \"violet\"\n\ncy\n  .get('li')\n  .eq(2); // select \"yellow\"\n</code></pre> <p>You can also use <code>.next()</code> and <code>.prev()</code> to navigate through the elements.</p>"}, {"location": "cypress/#select-elements-by-filtering", "title": "Select elements by filtering", "text": "<p>Once you select multiple elements, you can filter within these based on another selector.</p> <pre><code>cy\n  .get('li')\n  .filter('.primary') // select all elements with the class .primary\n</code></pre> <p>To do the exact opposite, you can use <code>.not()</code> command.</p> <p>cy   .get('li')   .not('.primary') // select all elements without the class .primary</p>"}, {"location": "cypress/#finding-elements", "title": "Finding elements", "text": "<p>You can specify your selector by first selecting an element you want to search within, and then look down the DOM structure to find a specific element you are looking for.</p> <pre><code>cy\n  .get('.list')\n  .find('.violet') // finds an element with class .violet inside .list element\n</code></pre> <p>Instead of looking down the DOM structure and finding an element within another element, we can look up. In this example, we first select our list item, and then try to find an element with a <code>.list</code> class.</p> <pre><code>cy\n  .get('.violet')\n  .parent('.list') // finds an element with class .list that is above our .violet element\n</code></pre>"}, {"location": "cypress/#interacting-with-elements", "title": "Interacting with elements", "text": "<p>Cypress allows you to click on and type into elements on the page by using <code>.click()</code> and <code>.type()</code> commands with a <code>cy.get()</code> or <code>cy.contains()</code> command. This is a great example of chaining in action.</p> <pre><code>cy.get('textarea.post-body').type('This is an excellent post.')\n</code></pre> <p>We're chaining the <code>.type()</code> onto the <code>cy.get()</code>, telling it to type into the subject yielded from the <code>cy.get()</code> command, which will be a DOM element.</p> <p>Here are even more action commands Cypress provides to interact with your app:</p> <ul> <li><code>.blur()</code>: Make a focused DOM element blur.</li> <li><code>.focus()</code>: Focus on a DOM element.</li> <li><code>.clear()</code>: Clear the value of an input or <code>textarea</code>.</li> <li><code>.check()</code>: Check checkbox(es) or radio(s).</li> <li><code>.uncheck()</code>: Uncheck checkbox(es).</li> <li><code>.select()</code>: Select an <code>&lt;option&gt;</code> within a <code>&lt;select&gt;</code>.</li> <li><code>.dblclick()</code>: Double-click a DOM element.</li> <li><code>.rightclick()</code>: Right-click a DOM element.</li> </ul> <p>These commands ensure some guarantees about what the state of the elements should be prior to performing their actions.</p> <p>For example, when writing a <code>.click()</code> command, Cypress ensures that the element is able to be interacted with (like a real user would). It will automatically wait until the element reaches an \"actionable\" state by:</p> <ul> <li>Not being hidden</li> <li>Not being covered</li> <li>Not being disabled</li> <li>Not animating</li> </ul> <p>This also helps prevent flake when interacting with your application in tests.</p> <p>If you want to jump into the command flow and use a custom function use <code>.then()</code>. When the previous command resolves, it will call your callback function with the yielded subject as the first argument.</p> <p>If you wish to continue chaining commands after your <code>.then()</code>, you'll need to specify the subject you want to yield to those commands, which you can achieve with a return value other than <code>null</code> or <code>undefined</code>. Cypress will yield that to the next command for you.</p> <pre><code>cy\n  // Find the el with id 'some-link'\n  .get('#some-link')\n\n  .then(($myElement) =&gt; {\n    // ...massage the subject with some arbitrary code\n\n    // grab its href property\n    const href = $myElement.prop('href')\n\n    // strip out the 'hash' character and everything after it\n    return href.replace(/(#.*)/, '')\n  })\n  .then((href) =&gt; {\n    // href is now the new subject\n    // which we can work with now\n  })\n</code></pre>"}, {"location": "cypress/#setting-aliases", "title": "Setting aliases", "text": "<p>Cypress has some added functionality for quickly referring back to past subjects called Aliases.</p> <p>It looks something like this:</p> <pre><code>cy.get('.my-selector')\n  .as('myElement') // sets the alias\n  .click()\n\n/* many more actions */\n\ncy.get('@myElement') // re-queries the DOM as before (only if necessary)\n  .click()\n</code></pre> <p>This lets us reuse our DOM queries for faster tests when the element is still in the DOM, and it automatically handles re-querying the DOM for us when it is not immediately found in the DOM. This is particularly helpful when dealing with front end frameworks that do a lot of re-rendering.</p> <p>It can be used to share context between tests, for example with fixtures:</p> <pre><code>beforeEach(() =&gt; {\n  // alias the users fixtures\n  cy.fixture('users.json').as('users')\n})\n\nit('utilize users in some way', function () {\n  // access the users property\n  const user = this.users[0]\n\n  // make sure the header contains the first\n  // user's name\n  cy.get('header').should('contain', user.name)\n})\n</code></pre>"}, {"location": "cypress/#asserting-about-elements", "title": "Asserting about elements", "text": "<p>Assertions let you do things like ensuring an element is visible or has a particular attribute, CSS class, or state. Assertions are commands that enable you to describe the desired state of your application. Cypress will automatically wait until your elements reach this state, or fail the test if the assertions don't pass. For example:</p> <pre><code>cy.get(':checkbox').should('be.disabled')\n\ncy.get('form').should('have.class', 'form-horizontal')\n\ncy.get('input').should('not.have.value', 'US')\n</code></pre> <p>Cypress bundles Chai, Chai-jQuery, and Sinon-Chai to provide built-in assertions. You can see a comprehensive list of them in the list of assertions reference. You can also write your own assertions as Chai plugins and use them in Cypress.</p>"}, {"location": "cypress/#default-assertions", "title": "Default assertions", "text": "<p>Many commands have a default, built-in assertion, or rather have requirements that may cause it to fail without needing an explicit assertion you've added.</p> <ul> <li><code>cy.visit()</code>: Expects the page to send text/html content with a 200 status     code.</li> <li><code>cy.request()</code>: Expects the remote server to exist and provide a response.</li> <li><code>cy.contains()</code>: Expects the element with content to eventually exist in the     DOM.</li> <li><code>cy.get()</code>: Expects the element to eventually exist in the DOM.</li> <li><code>.find()</code>: Also expects the element to eventually exist in the DOM.</li> <li><code>.type()</code>: Expects the element to eventually be in a typeable state.</li> <li><code>.click()</code>: Expects the element to eventually be in an actionable state.</li> <li><code>.its()</code>: Expects to eventually find a property on the current subject.</li> </ul> <p>Certain commands may have a specific requirement that causes them to immediately fail without retrying: such as <code>cy.request()</code>. Others, such as DOM based commands will automatically retry and wait for their corresponding elements to exist before failing.</p>"}, {"location": "cypress/#writing-assertions", "title": "Writing assertions", "text": "<p>There are two ways to write assertions in Cypress:</p> <ul> <li>Implicit Subjects: Using <code>.should()</code> or <code>.and()</code>.</li> <li>Explicit Subjects: Using <code>expect</code>.</li> </ul> <p>The implicit form is much shorter, so only use the explicit form in the next cases:</p> <ul> <li>Assert multiple things about the same subject.</li> <li>Massage the subject in some way prior to making the assertion.</li> </ul>"}, {"location": "cypress/#implicit-subjects", "title": "Implicit Subjects", "text": "<p>Using <code>.should()</code> or <code>.and()</code> commands is the preferred way of making assertions in Cypress.</p> <pre><code>// the implicit subject here is the first &lt;tr&gt;\n// this asserts that the &lt;tr&gt; has an .active class\ncy.get('tbody tr:first').should('have.class', 'active')\n</code></pre> <p>You can chain multiple assertions together using <code>.and()</code>, which is another name for <code>.should()</code> that makes things more readable:</p> <pre><code>cy.get('#header a')\n  .should('have.class', 'active')\n  .and('have.attr', 'href', '/users')\n</code></pre> <p>Because <code>.should('have.class')</code> does not change the subject, <code>.and('have.attr')</code> is executed against the same element. This is handy when you need to assert multiple things against a single subject quickly.</p>"}, {"location": "cypress/#explicit-subjects", "title": "Explicit Subjects", "text": "<p>Using <code>expect</code> allows you to pass in a specific subject and make an assertion about it.</p> <pre><code>// the explicit subject here is the boolean: true\nexpect(true).to.be.true\n</code></pre>"}, {"location": "cypress/#common-assertions", "title": "Common Assertions", "text": "<ul> <li> <p>Length:</p> <pre><code>// retry until we find 3 matching &lt;li.selected&gt;\ncy.get('li.selected').should('have.length', 3)\n</code></pre> </li> <li> <p>Attribute: For example to test links     <pre><code>// check the content of an attribute\ncy\n  .get('a')\n  .should('have.attr', 'href', 'https://docs.cypress.io')\n  .and('have.attr', 'target', '_blank') // Test it's meant to be opened\n  // another tab\n</code></pre></p> </li> <li> <p>Class:</p> <pre><code>// retry until this input does not have class disabled\ncy.get('form').find('input').should('not.have.class', 'disabled')\n</code></pre> </li> <li> <p>Value:</p> <pre><code>// retry until this textarea has the correct value\ncy.get('textarea').should('have.value', 'foo bar baz')\n</code></pre> </li> <li> <p>Text Content:</p> <pre><code>// assert the element's text content is exactly the given text\ncy.get('#user-name').should('have.text', 'Joe Smith')\n// assert the element's text includes the given substring\ncy.get('#address').should('include.text', 'Atlanta')\n// retry until this span does not contain 'click me'\ncy.get('a').parent('span.help').should('not.contain', 'click me')\n// the element's text should start with \"Hello\"\ncy.get('#greeting')\n  .invoke('text')\n  .should('match', /^Hello/)\n// tip: use cy.contains to find element with its text\n// matching the given regular expression\ncy.contains('#a-greeting', /^Hello/)\n</code></pre> </li> <li> <p>Visibility:</p> <pre><code>// retry until the button with id \"form-submit\" is visible\ncy.get('button#form-submit').should('be.visible')\n// retry until the list item with text \"write tests\" is visible\ncy.contains('.todo li', 'write tests').should('be.visible')\n</code></pre> <p>Note: if there are multiple elements, the assertions <code>be.visible</code> and <code>not.be.visible</code> act differently:</p> <pre><code>// retry until SOME elements are visible\ncy.get('li').should('be.visible')\n// retry until EVERY element is invisible\ncy.get('li.hidden').should('not.be.visible')\n</code></pre> </li> <li> <p>Existence:</p> <pre><code>// retry until loading spinner no longer exists\ncy.get('#loading').should('not.exist')\n</code></pre> </li> <li> <p>State:</p> <pre><code>// retry until our radio is checked\ncy.get(':radio').should('be.checked')\n</code></pre> </li> <li> <p>CSS:</p> <pre><code>// retry until .completed has matching css\ncy.get('.completed').should('have.css', 'text-decoration', 'line-through')\n\n// retry while .accordion css has the \"display: none\" property\ncy.get('#accordion').should('not.have.css', 'display', 'none')\n</code></pre> </li> <li> <p>Disabled property:</p> <pre><code>&lt;input type=\"text\" id=\"example-input\" disabled /&gt;\n</code></pre> <pre><code>cy.get('#example-input')\n  .should('be.disabled')\n  // let's enable this element from the test\n  .invoke('prop', 'disabled', false)\n\ncy.get('#example-input')\n  // we can use \"enabled\" assertion\n  .should('be.enabled')\n  // or negate the \"disabled\" assertion\n  .and('not.be.disabled')\n</code></pre> </li> </ul>"}, {"location": "cypress/#negative-assertions", "title": "Negative assertions", "text": "<p>There are positive and negative assertions. Examples of positive assertions are:</p> <pre><code>cy.get('.todo-item').should('have.length', 2).and('have.class', 'completed')\n</code></pre> <p>The negative assertions have the <code>not</code> chainer prefixed to the assertion. For example:</p> <pre><code>cy.contains('first todo').should('not.have.class', 'completed')\ncy.get('#loading').should('not.be.visible')\n</code></pre> <p>We recommend using negative assertions to verify that a specific condition is no longer present after the application performs an action. For example, when a previously completed item is unchecked, we might verify that a CSS class is removed.</p> <pre><code>// at first the item is marked completed\ncy.contains('li.todo', 'Write tests')\n  .should('have.class', 'completed')\n  .find('.toggle')\n  .click()\n\n// the CSS class has been removed\ncy.contains('li.todo', 'Write tests').should('not.have.class', 'completed')\n</code></pre> <p>Read more on the topic in the blog post Be Careful With Negative Assertions.</p>"}, {"location": "cypress/#custom-assertions", "title": "Custom assertions", "text": "<p>You can write your own assertion function and pass it as a callback to the <code>.should()</code> command.</p> <pre><code>cy.get('div').should(($div) =&gt; {\n  expect($div).to.have.length(1)\n\n  const className = $div[0].className\n\n  // className will be a string like \"main-abc123 heading-xyz987\"\n  expect(className).to.match(/heading-/)\n})\n</code></pre>"}, {"location": "cypress/#setting-up-the-tests", "title": "Setting up the tests", "text": "<p>Depending on how your application is built - it's likely that your web application is going to be affected and controlled by the server.</p> <p>Traditionally when writing e2e tests using Selenium, before you automate the browser you do some kind of set up and tear down on the server.</p> <p>You generally have three ways to facilitate this with Cypress:</p> <ul> <li><code>cy.exec()</code>: To run system commands.</li> <li><code>cy.task()</code>: To run code in Node via the <code>pluginsFile</code>.</li> <li><code>cy.request()</code>: To make HTTP requests.</li> </ul> <p>If you're running node.js on your server, you might add a <code>before</code> or <code>beforeEach</code> hook that executes an npm task.</p> <pre><code>describe('The Home Page', () =&gt; {\n  beforeEach(() =&gt; {\n    // reset and seed the database prior to every test\n    cy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n  })\n\n  it('successfully loads', () =&gt; {\n    cy.visit('/')\n  })\n})\n</code></pre> <p>Instead of just executing a system command, you may want more flexibility and could expose a series of routes only when running in a test environment.</p> <p>For instance, you could compose several requests together to tell your server exactly the state you want to create.</p> <pre><code>describe('The Home Page', () =&gt; {\n  beforeEach(() =&gt; {\n    // reset and seed the database prior to every test\n    cy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n\n    // seed a post in the DB that we control from our tests\n    cy.request('POST', '/test/seed/post', {\n      title: 'First Post',\n      authorId: 1,\n      body: '...',\n    })\n\n    // seed a user in the DB that we can control from our tests\n    cy.request('POST', '/test/seed/user', { name: 'Jane' })\n      .its('body')\n      .as('currentUser')\n  })\n\n  it('successfully loads', () =&gt; {\n    // this.currentUser will now point to the response\n    // body of the cy.request() that we could use\n    // to log in or work with in some way\n\n    cy.visit('/')\n  })\n})\n</code></pre> <p>While there's nothing really wrong with this approach, it does add a lot of complexity. You will be battling synchronizing the state between your server and your browser - and you'll always need to set up / tear down this state before tests (which is slow).</p> <p>The good news is that we aren't Selenium, nor are we a traditional e2e testing tool. That means we're not bound to the same restrictions.</p> <p>With Cypress, there are several other approaches that can offer an arguably better and faster experience.</p>"}, {"location": "cypress/#stubbing-the-server", "title": "Stubbing the server", "text": "<p>Another valid approach opposed to seeding and talking to your server is to bypass it altogether.</p> <p>While you'll still receive all of the regular HTML / JS / CSS assets from your server and you'll continue to <code>cy.visit()</code> it in the same way - you can instead stub the JSON responses coming from it.</p> <p>This means that instead of resetting the database, or seeding it with the state we want, you can force the server to respond with whatever you want it to. In this way, we not only prevent needing to synchronize the state between the server and browser, but we also prevent mutating state from our tests. That means tests won't build up state that may affect other tests.</p> <p>Another upside is that this enables you to build out your application without needing the contract of the server to exist. You can build it the way you want the data to be structured, and even test all of the edge cases, without needing a server.</p> <p>However - there is likely still a balance here where both strategies are valid (and you should likely do them).</p> <p>While stubbing is great, it means that you don't have the guarantees that these response payloads actually match what the server will send. However, there are still many valid ways to get around this:</p> <ul> <li> <p>Generate the fixture stubs ahead of time: You could have the server generate     all of the fixture stubs for you ahead of time. This means their data will     reflect what the server will actually send.</p> </li> <li> <p>Write a single e2e test without stubs, and then stub the rest: Another more     balanced approach is to integrate both strategies. You likely want to have     a single test that takes a true e2e approach and stubs nothing. It'll use     the feature for real - including seeding the database and setting up state.</p> <p>Once you've established it's working you can then use stubs to test all of the edge cases and additional scenarios. There are no benefits to using real data in the vast majority of cases. We recommend that the vast majority of tests use stub data. They will be orders of magnitude faster, and much less complex.</p> </li> </ul> <p><code>cy.intercept()</code> is used to control the behavior of HTTP requests. You can statically define the body, HTTP status code, headers, and other response characteristics.</p> <pre><code>cy.intercept(\n  {\n    method: 'GET', // Route all GET requests\n    url: '/users/*', // that have a URL that matches '/users/*'\n  },\n  [] // and force the response to be: []\n).as('getUsers') // and assign an alias\n</code></pre>"}, {"location": "cypress/#fixtures", "title": "Fixtures", "text": "<p>A fixture is a fixed set of data located in a file that is used in your tests. The purpose of a test fixture is to ensure that there is a well known and fixed environment in which tests are run so that results are repeatable. Fixtures are accessed within tests by calling the <code>cy.fixture()</code> command.</p> <p>When stubbing a response, you typically need to manage potentially large and complex JSON objects. Cypress allows you to integrate fixture syntax directly into responses.</p> <pre><code>// we set the response to be the activites.json fixture\ncy.intercept('GET', '/activities/*', { fixture: 'activities.json' })\n</code></pre> <p>Fixtures live in <code>/cypress/fixtures/</code> and can be further organized within additional directories. For instance, you could create another folder called images and add images:</p> <pre><code>/cypress/fixtures/images/cats.png\n/cypress/fixtures/images/dogs.png\n/cypress/fixtures/images/birds.png\n</code></pre> <p>To access the fixtures nested within the images folder, include the folder in your <code>cy.fixture()</code> command.</p> <pre><code>cy.fixture('images/dogs.png') // yields dogs.png as Base64\n</code></pre>"}, {"location": "cypress/#use-the-content-of-a-fixture-set-in-a-hook-in-a-test", "title": "Use the content of a fixture set in a hook in a test", "text": "<p>If you store and access the fixture data using this test context object, make sure to use <code>function () { ... }</code> callbacks both for the hook and the test. Otherwise the test engine will NOT have this pointing at the test context.</p> <pre><code>describe('User page', () =&gt; {\n  beforeEach(function () {\n    // \"this\" points at the test context object\n    cy.fixture('user').then((user) =&gt; {\n      // \"this\" is still the test context object\n      this.user = user\n    })\n  })\n\n  // the test callback is in \"function () { ... }\" form\n  it('has user', function () {\n    // this.user exists\n    expect(this.user.firstName).to.equal('Jane')\n  })\n})\n</code></pre>"}, {"location": "cypress/#logging-in", "title": "Logging in", "text": "<p>One of the first (and arguably one of the hardest) hurdles you'll have to overcome in testing is logging into your application.</p> <p>It's a great idea to get your signup and login flow under test coverage since it is very important to all of your users and you never want it to break.</p> <p>Logging in is one of those features that are mission critical and should likely involve your server. We recommend you test signup and login using your UI as a real user would. For example:</p> <pre><code>describe('The Login Page', () =&gt; {\n  beforeEach(() =&gt; {\n    // reset and seed the database prior to every test\n    cy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n\n    // seed a user in the DB that we can control from our tests\n    // assuming it generates a random password for us\n    cy.request('POST', '/test/seed/user', { username: 'jane.lane' })\n      .its('body')\n      .as('currentUser')\n  })\n\n  it('sets auth cookie when logging in via form submission', function () {\n    // destructuring assignment of the this.currentUser object\n    const { username, password } = this.currentUser\n\n    cy.visit('/login')\n\n    cy.get('input[name=username]').type(username)\n\n    // {enter} causes the form to submit\n    cy.get('input[name=password]').type(`${password}{enter}`)\n\n    // we should be redirected to /dashboard\n    cy.url().should('include', '/dashboard')\n\n    // our auth cookie should be present\n    cy.getCookie('your-session-cookie').should('exist')\n\n    // UI should reflect this user being logged in\n    cy.get('h1').should('contain', 'jane.lane')\n  })\n})\n</code></pre> <p>You'll likely also want to test your login UI for:</p> <ul> <li>Invalid username / password.</li> <li>Username taken.</li> <li>Password complexity requirements.</li> <li>Edge cases like locked / deleted accounts.</li> </ul> <p>Each of these likely requires a full blown e2e test, and it makes sense to go through the login process. But when you're testing another area of the system that relies on a state from a previous feature: do not use your UI to set up this state. So for these cases you'd do:</p> <p><pre><code>describe('The Dashboard Page', () =&gt; {\n  beforeEach(() =&gt; {\n    // reset and seed the database prior to every test\n    cy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n\n    // seed a user in the DB that we can control from our tests\n    // assuming it generates a random password for us\n    cy.request('POST', '/test/seed/user', { username: 'jane.lane' })\n      .its('body')\n      .as('currentUser')\n  })\n\n  it('logs in programmatically without using the UI', function () {\n    // destructuring assignment of the this.currentUser object\n    const { username, password } = this.currentUser\n\n    // programmatically log us in without needing the UI\n    cy.request('POST', '/login', {\n      username,\n      password,\n    })\n\n    // now that we're logged in, we can visit\n    // any kind of restricted route!\n    cy.visit('/dashboard')\n\n    // our auth cookie should be present\n    cy.getCookie('your-session-cookie').should('exist')\n\n    // UI should reflect this user being logged in\n    cy.get('h1').should('contain', 'jane.lane')\n  })\n})\n</code></pre> This saves an enormous amount of time visiting the login page, filling out the username, password, and waiting for the server to redirect us before every test.</p> <p>Because we previously tested the login system end-to-end without using any shortcuts, we already have 100% confidence it's working correctly.</p> <p>Here are other login recipes.</p>"}, {"location": "cypress/#setting-up-backend-servers-for-e2e-tests", "title": "Setting up backend servers for E2E tests", "text": "<p>Cypress team does NOT recommend trying to start your back end web server from within Cypress.</p> <p>Any command run by <code>cy.exec()</code> or <code>cy.task()</code> has to exit eventually. Otherwise, Cypress will not continue running any other commands.</p> <p>Trying to start a web server from <code>cy.exec()</code> or <code>cy.task()</code> causes all kinds of problems because:</p> <ul> <li>You have to background the process.</li> <li>You lose access to it via terminal.</li> <li>You don't have access to its stdout or logs.</li> <li>Every time your tests run, you'd have to work out the complexity around     starting an already running web server.</li> <li>You would likely encounter constant port conflicts.</li> </ul> <p>Therefore you should start your web server before running Cypress and kill it after it completes. They have examples showing you how to start and stop your web server in a CI environment.</p>"}, {"location": "cypress/#waiting", "title": "Waiting", "text": "<p>Cypress enables you to declaratively <code>cy.wait()</code> for requests and their responses.</p> <pre><code>cy.intercept('/activities/*', { fixture: 'activities' }).as('getActivities')\ncy.intercept('/messages/*', { fixture: 'messages' }).as('getMessages')\n\n// visit the dashboard, which should make requests that match\n// the two routes above\ncy.visit('http://localhost:8888/dashboard')\n\n// pass an array of Route Aliases that forces Cypress to wait\n// until it sees a response for each request that matches\n// each of these aliases\ncy.wait(['@getActivities', '@getMessages'])\n\n// these commands will not run until the wait command resolves above\ncy.get('h1').should('contain', 'Dashboard')\n</code></pre> <p>If you would like to check the response data of each response of an aliased route, you can use several <code>cy.wait()</code> calls.</p> <pre><code>cy.intercept({\n  method: 'POST',\n  url: '/myApi',\n}).as('apiCheck')\n\ncy.visit('/')\ncy.wait('@apiCheck').then((interception) =&gt; {\n  assert.isNotNull(interception.response.body, '1st API call has data')\n})\n\ncy.wait('@apiCheck').then((interception) =&gt; {\n  assert.isNotNull(interception.response.body, '2nd API call has data')\n})\n\ncy.wait('@apiCheck').then((interception) =&gt; {\n  assert.isNotNull(interception.response.body, '3rd API call has data')\n})\n</code></pre> <p>Waiting on an aliased route has big advantages:</p> <ul> <li>Tests are more robust with much less flake.</li> <li>Failure messages are much more precise.</li> <li>You can assert about the underlying request object.</li> </ul>"}, {"location": "cypress/#avoiding-flake-tests", "title": "Avoiding Flake tests", "text": "<p>One advantage of declaratively waiting for responses is that it decreases test flake. You can think of <code>cy.wait()</code> as a guard that indicates to Cypress when you expect a request to be made that matches a specific routing alias. This prevents the next commands from running until responses come back and it guards against situations where your requests are initially delayed.</p> <pre><code>cy.intercept('/search*', [{ item: 'Book 1' }, { item: 'Book 2' }]).as(\n  'getSearch'\n)\n\n// our autocomplete field is throttled\n// meaning it only makes a request after\n// 500ms from the last keyPress\ncy.get('#autocomplete').type('Book')\n\n// wait for the request + response\n// thus insulating us from the\n// throttled request\ncy.wait('@getSearch')\n\ncy.get('#results').should('contain', 'Book 1').and('contain', 'Book 2')\n</code></pre>"}, {"location": "cypress/#assert-on-wait-content", "title": "Assert on wait content", "text": "<p>Another benefit of using <code>cy.wait()</code> on requests is that it allows you to access the actual request object. This is useful when you want to make assertions about this object.</p> <p>In our example above we can assert about the request object to verify that it sent data as a query string in the URL. Although we're mocking the response, we can still verify that our application sends the correct request.</p> <pre><code>// any request to \"/search/*\" endpoint will automatically receive\n// an array with two book objects\ncy.intercept('/search/*', [{ item: 'Book 1' }, { item: 'Book 2' }]).as(\n  'getSearch'\n)\n\ncy.get('#autocomplete').type('Book')\n\n// this yields us the interception cycle object which includes\n// fields for the request and response\ncy.wait('@getSearch').its('request.url').should('include', '/search?query=Book')\n\ncy.get('#results').should('contain', 'Book 1').and('contain', 'Book 2')\n</code></pre> <p>Of the intercepted object you can check:</p> <ul> <li>URL.</li> <li>Method.</li> <li>Status Code.</li> <li>Request Body.</li> <li>Request Headers.</li> <li>Response Body.</li> <li>Response Headers.</li> </ul> <pre><code>// spy on POST requests to /users endpoint\ncy.intercept('POST', '/users').as('new-user')\n// trigger network calls by manipulating web app's user interface, then\ncy.wait('@new-user').should('have.property', 'response.statusCode', 201)\n\n// we can grab the completed interception object again to run more assertions\n// using cy.get(&lt;alias&gt;)\ncy.get('@new-user') // yields the same interception object\n  .its('request.body')\n  .should(\n    'deep.equal',\n    JSON.stringify({\n      id: '101',\n      firstName: 'Joe',\n      lastName: 'Black',\n    })\n  )\n\n// and we can place multiple assertions in a single \"should\" callback\ncy.get('@new-user').should(({ request, response }) =&gt; {\n  expect(request.url).to.match(/\\/users$/)\n  expect(request.method).to.equal('POST')\n  // it is a good practice to add assertion messages\n  // as the 2nd argument to expect()\n  expect(response.headers, 'response headers').to.include({\n    'cache-control': 'no-cache',\n    expires: '-1',\n    'content-type': 'application/json; charset=utf-8',\n    location: '&lt;domain&gt;/users/101',\n  })\n})\n</code></pre> <p>You can inspect the full request cycle object by logging it to the console</p> <pre><code>cy.wait('@new-user').then(console.log)\n</code></pre>"}, {"location": "cypress/#dont-repeat-yourself", "title": "Don't repeat yourself", "text": ""}, {"location": "cypress/#share-code-before-each-test", "title": "Share code before each test", "text": "<pre><code>describe('my form', () =&gt; {\n  beforeEach(() =&gt; {\n    cy.visit('/users/new')\n    cy.get('#first').type('Johnny')\n    cy.get('#last').type('Appleseed')\n  })\n\n  it('displays form validation', () =&gt; {\n    cy.get('#first').clear() // clear out first name\n    cy.get('form').submit()\n    cy.get('#errors').should('contain', 'First name is required')\n  })\n\n  it('can submit a valid form', () =&gt; {\n    cy.get('form').submit()\n  })\n})\n</code></pre>"}, {"location": "cypress/#parametrization", "title": "Parametrization", "text": "<p>If you want to run similar tests with different data, you can use parametrization. For example to test the same pages for different screen sizes use:</p> <pre><code>const sizes = ['iphone-6', 'ipad-2', [1024, 768]]\n\ndescribe('Logo', () =&gt; {\n  sizes.forEach((size) =&gt; {\n    // make assertions on the logo using\n    // an array of different viewports\n    it(`Should display logo on ${size} screen`, () =&gt; {\n      if (Cypress._.isArray(size)) {\n        cy.viewport(size[0], size[1])\n      } else {\n        cy.viewport(size)\n      }\n\n      cy.visit('https://www.cypress.io')\n      cy.get('#logo').should('be.visible')\n    })\n  })\n})\n</code></pre>"}, {"location": "cypress/#use-functions", "title": "Use functions", "text": "<p>Sometimes, the piece of code is redundant and we don't we don't require it in all the test cases. We can create utility functions and move such code there.</p> <p>We can create a separate folder as utils in support folder and store our functions in a file in that folder.</p> <p>Consider the following example of utility function for login.</p> <pre><code>//cypress/support/utils/common.js\n\nexport const loginViaUI = (username, password) =&gt; {\n  cy.get(\"[data-cy='login-email-field']\").type(username);\n  cy.get(\"[data-cy='login-password-field']\").type(password);\n  cy.get(\"[data-cy='submit-button']\").submit()\n}\n</code></pre> <p>This is how we can use utility function in our test case:</p> <pre><code>import {\n  loginViaUI\n} from '../support/utils/common.js';\n\ndescribe(\"Login\", () =&gt; {\n  it('should allow user to log in', () =&gt; {\n    cy.visit('/login');\n    loginViaUI('username', 'password');\n  });\n});\n</code></pre> <p>Utility functions are similar to Cypress commands. If the code being used in almost every test suite, we can create a custom command for it. The benefit of this is that we don't have to import the js file to use the command, it is available directly on cy object i.e. <code>cy.loginViaUI()</code>.</p> <p>But, this doesn't mean that we should use commands for everything. If the code is used in only some of the test suite, we can create a utility function and import it whenever needed.</p>"}, {"location": "cypress/#setting-up-time-of-the-tests", "title": "Setting up time of the tests", "text": "<p>Specify a <code>now</code> timestamp</p> <pre><code>// your app code\n$('#date').text(new Date().toJSON())\n\nconst now = new Date(2017, 3, 14).getTime() // April 14, 2017 timestamp\n\ncy.clock(now)\ncy.visit('/index.html')\ncy.get('#date').contains('2017-04-14')\n</code></pre>"}, {"location": "cypress/#simulate-errors", "title": "Simulate errors", "text": "<p>End-to-end tests are excellent for testing \u201chappy path\u201d scenarios and the most important application features.</p> <p>However, there are unexpected situations, and when they occur, the application cannot completely \"break\".</p> <p>Such situations can occur due to errors on the server or the network, to name a few.</p> <p>With Cypress, we can easily simulate error situations.</p> <p>Below are examples of tests for server and network errors.</p> <pre><code>context('Errors', () =&gt; {\n  const errorMsg = 'Oops! Try again later'\n\n  it('simulates a server error', () =&gt; {\n    cy.intercept(\n      'GET',\n      '**/search?query=cypress',\n      { statusCode: 500 }\n    ).as('getServerFailure')\n\n    cy.visit('https://example.com/search')\n\n    cy.get('[data-cy=\"search-field\"]')\n      .should('be.visible')\n      .type('cypress{enter}')\n    cy.wait('@getServerFailure')\n\n    cy.contains(errorMsg)\n      .should('be.visible')\n  })\n\n  it('simulates a network failure', () =&gt; {\n    cy.intercept(\n      'GET',\n      '**/search?query=cypressio',\n      { forceNetworkError: true }\n    ).as('getNetworkFailure')\n\n    cy.visit('https://example.com/search')\n\n    cy.get('[data-cy=\"search-field\"]')\n      .should('be.visible')\n      .type('cypressio{enter}')\n    cy.wait('@getNetworkFailure')\n\n    cy.contais(errorMsg)\n      .should('be.visible')\n  })\n})\n</code></pre> <p>In the above tests, the HTTP request of type GET to the search endpoint is intercepted. In the first test, we use the <code>statusCode</code> option with the value <code>500</code>. In the second test, we use the <code>forceNewtworkError</code> option with the value of <code>true</code>. After that, you can test that the correct message is visible to the user.</p>"}, {"location": "cypress/#sending-different-responses", "title": "Sending different responses", "text": "<p>To return different responses from a single <code>GET /todos</code> intercept, you can place all prepared responses into an array, and then use Array.prototype.shift to return and remove the first item.</p> <pre><code>it('returns list with more items on page reload', () =&gt; {\n  const replies = [{ fixture: 'articles.json' }, { statusCode: 404 }]\n  cy.intercept('GET', '/api/inbox', req =&gt; req.reply(replies.shift()))\n})\n</code></pre>"}, {"location": "cypress/#component-testing", "title": "Component testing", "text": "<p>Component testing in Cypress is similar to end-to-end testing. The notable differences are:</p> <ul> <li>There's no need to navigate to a URL. You don't need to call <code>cy.visit()</code> in your test.</li> <li>Cypress provides a blank canvas where we can <code>mount</code> components in isolation.</li> </ul> <p>For example:</p> <pre><code>import { mount } from '@cypress/vue'\nimport TodoList from './components/TodoList'\n\ndescribe('TodoList', () =&gt; {\n  it('renders the todo list', () =&gt; {\n    mount(&lt;TodoList /&gt;)\n    cy.get('[data-testid=todo-list]').should('exist')\n  })\n\n  it('contains the correct number of todos', () =&gt; {\n    const todos = [\n      { text: 'Buy milk', id: 1 },\n      { text: 'Learn Component Testing', id: 2 },\n    ]\n\n    mount(&lt;TodoList todos={todos} /&gt;)\n\n    cy.get('[data-testid=todos]').should('have.length', todos.length)\n  })\n})\n</code></pre> <p>If you are using Cypress Component Testing in a project that also has tests written with the Cypress End-to-End test runner, you may want to configure some Component Testing specific defaults.</p> <p>It doesn't yet work with vuetify</p>"}, {"location": "cypress/#install", "title": "Install", "text": "<p>Run:</p> <pre><code>npm install --save-dev cypress @cypress/vue @cypress/webpack-dev-server webpack-dev-server\n</code></pre> <p>You will also need to configure the component testing framework of your choice by installing the corresponding component testing plugin.</p> <pre><code>// cypress/plugins/index.js\n\nmodule.exports = (on, config) =&gt; {\n  if (config.testingType === 'component') {\n    const { startDevServer } = require('@cypress/webpack-dev-server')\n\n    // Vue's Webpack configuration\n    const webpackConfig = require('@vue/cli-service/webpack.config.js')\n\n    on('dev-server:start', (options) =&gt;\n      startDevServer({ options, webpackConfig })\n    )\n  }\n}\n</code></pre>"}, {"location": "cypress/#usage_1", "title": "Usage", "text": "<p><pre><code>// components/HelloWorld.spec.js\nimport { mount } from '@cypress/vue'\nimport { HelloWorld } from './HelloWorld.vue'\ndescribe('HelloWorld component', () =&gt; {\n  it('works', () =&gt; {\n    mount(HelloWorld)\n    // now use standard Cypress commands\n    cy.contains('Hello World!').should('be.visible')\n  })\n})\n</code></pre> You can pass additional styles, css files and external stylesheets to load, see docs/styles.md for full list.</p> <pre><code>import Todo from './Todo.vue'\nconst todo = {\n  id: '123',\n  title: 'Write more tests',\n}\n\nmount(Todo, {\n  propsData: { todo },\n  stylesheets: [\n    'https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.2/css/bulma.css',\n  ],\n})\n</code></pre>"}, {"location": "cypress/#visual-testing", "title": "Visual testing", "text": "<p>Cypress is a functional Test Runner. It drives the web application the way a user would, and checks if the app functions as expected: if the expected message appears, an element is removed, or a CSS class is added after the appropriate user action. Cypress does NOT see how the page actually looks though.</p> <p>You could technically write a functional test asserting the CSS properties using the have.css assertion, but these may quickly become cumbersome to write and maintain, especially when visual styles rely on a lot of CSS styles.</p> <p>Visual testing can be done through plugins that do visual regression testing, which is to take an image snapshot of the entire application under test or a specific element, and then compare the image to a previously approved baseline image. If the images are the same (within a set pixel tolerance), it is determined that the web application looks the same to the user. If there are differences, then there has been some change to the DOM layout, fonts, colors or other visual properties that needs to be investigated.</p> <p>If you want to test if your app is responsive use parametrization to have maintainable tests.</p> <p>For more information on how to do visual regression testing read this article.</p> <p>As of 2022-04-23 the most popular tools that don't depend on third party servers are:</p> <ul> <li>cypress-plugin-snapshots:     It looks to be the best plugin as it allows you to update the screenshots     directly through the Cypress interface, but it is unmaintained</li> <li>cypress-visual-regression:     Maintained but it doesn't show the differences in the cypress interface and     you have to interact with them through the command line.</li> <li>cypress-image-snapshot:     Most popular but it looks unmaintained     (1,     2)</li> </ul> <p>Check the Visual testing plugins list to see all available solutions. Beware of the third party solutions like  Percy and Applitools as they send your pictures to their servers on each test.</p>"}, {"location": "cypress/#cypress-visual-regression", "title": "<code>cypress-visual-regression</code>", "text": ""}, {"location": "cypress/#installation_1", "title": "Installation", "text": "<pre><code>npm install --save-dev cypress-visual-regression\n</code></pre> <p>Add the following config to your <code>cypress.json</code> file:</p> <pre><code>{\n  \"screenshotsFolder\": \"./cypress/snapshots/actual\",\n  \"trashAssetsBeforeRuns\": true\n}\n</code></pre> <p>Add the plugin to <code>cypress/plugins/index.js</code>:</p> <pre><code>const getCompareSnapshotsPlugin = require('cypress-visual-regression/dist/plugin');\n\nmodule.exports = (on, config) =&gt; {\n  getCompareSnapshotsPlugin(on, config);\n};\n</code></pre> <p>Add the command to <code>cypress/support/commands.js</code>:</p> <pre><code>const compareSnapshotCommand = require('cypress-visual-regression/dist/command');\n\ncompareSnapshotCommand();\n</code></pre> <p>Make sure you import <code>commands.js</code> in <code>cypress/support/index.js</code>:</p> <pre><code>import './commands'\n</code></pre>"}, {"location": "cypress/#use", "title": "Use", "text": "<p>Add <code>cy.compareSnapshot('home')</code> in your tests specs whenever you want to test for visual regressions, making sure to replace home with a relevant name. You can also add an optional error threshold: Value can range from 0.00 (no difference) to 1.00 (every pixel is different). So, if you enter an error threshold of 0.51, the test would fail only if &gt; 51% of pixels are different. For example:</p> <pre><code>it('should display the login page correctly', () =&gt; {\n  cy.visit('/03.html');\n  cy.get('H1').contains('Login');\n  cy.compareSnapshot('login', 0.0);\n  cy.compareSnapshot('login', 0.1);\n});\n</code></pre> <p>You can target a single HTML element as well:</p> <pre><code>cy.get('#my-header').compareSnapshot('just-header')\n</code></pre> <p>Check more examples here</p> <p>You need to take or update the base images, do it with:</p> <pre><code>npx cypress run \\\n    --env type=base \\\n    --config screenshotsFolder=cypress/snapshots/base,testFiles=\\\"**/*regression-tests.js\\\"\n</code></pre> <p>To find regressions run:</p> <pre><code>npx cypress run --env type=actual\n</code></pre> <p>Or if you want to just check a subset of tests use:</p> <pre><code>npx cypress run --env type=actual --spec \"cypress\\integration\\visual-tests.spec.js\"\nnpx cypress run --env type=actual --spec \"cypress\\integration\\test1.spec.js\",\"cypress\\integration\\test2.spec.js\"\nnpx cypress run --env type=actual --spec \"cypress\\integration\\**\\*.spec.js\n</code></pre>"}, {"location": "cypress/#third-party-component-testing", "title": "Third party component testing", "text": "<p>Other examples of testing third party components</p> <ul> <li>Testing HTML emails</li> </ul>"}, {"location": "cypress/#configuration", "title": "Configuration", "text": "<p>Cypress saves it's configuration in the <code>cypress.json</code> file.</p> <pre><code>{\n  \"baseUrl\": \"http://localhost:8080\"\n}\n</code></pre> <p>Where:</p> <ul> <li><code>baseUrl</code>: Will be prefixed on <code>cy.visit()</code> and <code>cy.requests()</code>.</li> </ul>"}, {"location": "cypress/#environment-variables", "title": "Environment variables", "text": "<p>Environment variables are useful when:</p> <ul> <li>Values are different across developer machines.</li> <li>Values are different across multiple environments: (dev, staging, qa, prod).</li> <li>Values change frequently and are highly dynamic.</li> </ul> <p>Instead of hard coding this in your tests:</p> <pre><code>cy.request('https://api.acme.corp') // this will break on other environments\n</code></pre> <p>We can move this into a Cypress environment variable:</p> <pre><code>cy.request(Cypress.env('EXTERNAL_API')) // points to a dynamic env var\n</code></pre> <p>Any key/value you set in your configuration file under the <code>env</code> key will become an environment variable.</p> <pre><code>{\n  \"projectId\": \"128076ed-9868-4e98-9cef-98dd8b705d75\",\n  \"env\": {\n    \"login_url\": \"/login\",\n    \"products_url\": \"/products\"\n  }\n}\n</code></pre> <p>To access it use:</p> <pre><code>Cypress.env() // {login_url: '/login', products_url: '/products'}\nCypress.env('login_url') // '/login'\nCypress.env('products_url') // '/products'\n</code></pre>"}, {"location": "cypress/#configure-component-testing", "title": "Configure component testing", "text": "<p>You can configure or override Component Testing defaults in your configuration file using the <code>component</code> key.</p> <pre><code>{\n  \"testFiles\": \"cypress/integration/*.spec.js\",\n  \"component\": {\n    \"componentFolder\": \"src\",\n    \"testFiles\": \".*/__tests__/.*spec.tsx\",\n    \"viewportHeight\": 500,\n    \"viewportWidth\": 700\n  }\n}\n</code></pre>"}, {"location": "cypress/#debugging", "title": "Debugging", "text": ""}, {"location": "cypress/#using-the-debugger", "title": "Using the debugger", "text": "<p>Use the <code>.debug()</code> command directly BEFORE the action.</p> <pre><code>// break on a debugger before the action command\ncy.get('button').debug().click()\n</code></pre>"}, {"location": "cypress/#step-through-test-commands", "title": "Step through test commands", "text": "<p>You can run the test command by command using the <code>.pause()</code> command.</p> <pre><code>it('adds items', () =&gt; {\n  cy.pause()\n  cy.get('.new-todo')\n  // more commands\n})\n</code></pre> <p>This allows you to inspect the web application, the DOM, the network, and any storage after each command to make sure everything happens as expected.</p>"}, {"location": "cypress/#issues", "title": "Issues", "text": "<ul> <li>Allow rerun only failed     tests: Until it's ready     use <code>it.only</code> on the test you want to run.</li> </ul>"}, {"location": "cypress/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li>Examples of usage</li> <li>Cypress API</li> <li>Real World Application Cypress testing example</li> <li>Tutorial on writing     tests</li> <li>Video tutorials</li> </ul>"}, {"location": "digital_garden/", "title": "Digital Garden", "text": "<p>Digital Garden is a method of storing  and maintaining knowledge in an maintainable, scalable and searchable way. They  are also known as second brains.</p> <p>Unlike in common blogging where you write an article and forget about it, posts are treated as plants in various stages of growth and nurturing. Some might wither and die, and others will flourish and provide a source of continued knowledge for the gardener and folks in the community that visit.</p> <p>The content is diverse, you can find ideas, articles, investigations, snippets, resources, thoughts, collections, and other bits and pieces that I find interesting and useful.</p> <p>It's my personal Stock, the content that\u2019s as interesting in two months (or two years) as it is today. It\u2019s what people discover via search. It\u2019s what spreads slowly but surely, building fans over time.</p> <p>They are a metaphor for thinking about writing and creating that focuses less on the resulting \"showpiece\" and more on the process, care, and craft it takes to get there.</p>"}, {"location": "digital_garden/#existing-digital-gardens", "title": "Existing digital gardens", "text": "<p>If you look for inspiration check my favourite digital gardens:</p> <ul> <li>Nikita's Everything I know: It's awesome     both in content quality and length, as the way he presents it.</li> <li>Gwern's site: The way he presents content is     unique and gorgeous. I've found myself not very hooked to the content, but     when you find something you like it's awesome, such as the about     page, his article on     spaced repetition or the essay     of Death Note: L, Anonymity &amp; Eluding     Entropy.</li> </ul> <p>Or browse the following lists:</p> <ul> <li>Best-of Digital gardens</li> <li>Maggie Appleton's compilation</li> <li>Nikita's     compilation</li> <li>Richard Litt's compilation</li> <li>KasperZutterman's compilation</li> </ul> <p>Or the digital garden's reddit.</p>"}, {"location": "digital_garden/#references", "title": "References", "text": "<ul> <li>Joel Hooks article on Digital Gardens</li> <li>Tom Critchlow article on Digital Gardens</li> </ul>"}, {"location": "diversity/", "title": "Diversity", "text": "<p>Diversity, equity, and inclusion (DEI) can be defined as:</p> <ul> <li> <p>Diversity is the representation and acknowledgement of the multitudes of     identities, experiences, and ways of moving through the world. This     includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal     record and/or incarceration, educational attainment, ethnicity, gender,     geographical location, language, nationality, political affiliation,     religion, race, sexuality, socioeconomic status, and veteran status.     Further, we recognize that each individual's experience is informed by     intersections across multiple identities.</p> </li> <li> <p>Equity  seeks to ensure respect and equal opportunity for all, using all     resources and tools to elevate the voices of under-represented and/or     disadvantaged groups.</p> </li> <li> <p>Inclusion is fostering an environment in which people of all identities are     welcome, valued, and supported. An inclusive organization solicits, listens     to, learns from, and acts on the contributions of all its stakeholders.</p> </li> </ul>"}, {"location": "diversity/#references", "title": "References", "text": "<ul> <li>Pulitzer center DEI     page</li> <li>Journalist's Toolbox DEI links</li> </ul>"}, {"location": "docker/", "title": "Docker", "text": "<p>Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. Because all of the containers share the services of a single operating system kernel, they use fewer resources than virtual machines.</p>"}, {"location": "docker/#how-to-keep-containers-updated", "title": "How to keep containers updated", "text": ""}, {"location": "docker/#with-renovate", "title": "With Renovate", "text": "<p>Renovate is a program that does automated dependency updates. Multi-platform and multi-language.</p>"}, {"location": "docker/#with-watchtower", "title": "With Watchtower", "text": "<p>With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially.</p> <p>Run the watchtower container with the next command:</p> <pre><code>docker run -d \\\n--name watchtower \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /etc/localtime:/etc/localtime:ro \\\n-e WATCHTOWER_NOTIFICATIONS=email \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_FROM={{ email.from }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_TO={{ email.to }} \\\\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER=mail.riseup.net \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT=587 \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER={{ email.user }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD={{ email.password }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_DELAY=2 \\\ncontainrrr/watchtower:latest --no-restart --no-startup-message\n</code></pre> <p>Use the <code>--no-restart</code> flag if you use systemd to manage the dockers, and <code>--no-startup-message</code> if you don't want watchtower to send you an email each time it starts the update process.</p> <p>Keep in mind that if the containers don't have good migration scripts, upgrading may break the service. To enable this feature, make sure you have frequent backups and a tested rollback process. If you're not sure one of the containers is going to behave well, you can only monitor it or disable it by using docker labels.</p> <p>The first check will be done by default in the next 24 hours, to check that everything works use the <code>--run-once</code> flag.</p> <p>Another alternative is Diun, which is a CLI application written in Go and delivered as a single executable (and a Docker image) to receive notifications when a Docker image is updated on a Docker registry.</p> <p>They don't yet support Prometheus metrics but it surely looks promising.</p>"}, {"location": "docker/#logging-in", "title": "[Logging in", "text": "<p>automatically](https://docs.docker.com/engine/reference/commandline/login/#provide-a-password-using-stdin)</p> <p>To log in automatically without entering the password, you need to have the password stored in your personal password store (not in root's!), imagine it's in the <code>dockerhub</code> entry. Then you can use:</p> <pre><code>pass show dockerhub | docker login --username foo --password-stdin\n</code></pre>"}, {"location": "docker/#snippets", "title": "Snippets", "text": ""}, {"location": "docker/#attach-a-docker-to-many-networks", "title": "Attach a docker to many networks", "text": "<p>You can't do it through the <code>docker run</code> command, there you can only specify one network. However, you can attach a docker to a network with the command:</p> <pre><code>docker network attach network-name docker-name\n</code></pre>"}, {"location": "docker/#get-the-output-of-docker-ps-as-a-json", "title": "Get the output of <code>docker ps</code> as a json", "text": "<p>To get the complete json for reference.</p> <pre><code>docker ps -a --format \"{{json .}}\" | jq -s\n</code></pre> <p>To get only the required columns in the output with tab separated version</p> <pre><code>docker ps -a --format \"{{json .}}\" | jq -r -c '[.ID, .State, .Names, .Image]'\n</code></pre> <p>To get also the image's ID you can use:</p> <pre><code>docker inspect --format='{{json .}}' $(docker ps -aq) | jq -r -c '[.Id, .Name, .Config.Image, .Image]'\n</code></pre>"}, {"location": "docker/#connect-multiple-docker-compose-files", "title": "Connect multiple docker compose files", "text": "<p>You can connect services defined across multiple docker-compose.yml files.</p> <p>In order to do this you\u2019ll need to:</p> <ul> <li>Create an external network with <code>docker network create &lt;network name&gt;</code></li> <li>In each of your <code>docker-compose.yml</code> configure the default network to use your     externally created network with the networks top-level key.</li> <li>You can use either the service name or container name to connect between containers.</li> </ul> <p>Let's do it with an example:</p> <ul> <li> <p>Creating the network</p> <pre><code>$ docker network create external-example\n2af4d92c2054e9deb86edaea8bb55ecb74f84a62aec7614c9f09fee386f248a6\n</code></pre> </li> <li> <p>Create the first docker-compose file</p> <pre><code>version: '3'\nservices:\n  service1:\n    image: busybox\n    command: sleep infinity\n\nnetworks:\n  default:\n    external:\n      name: external-example\n</code></pre> </li> <li> <p>Bring the service up</p> <pre><code>$ docker-compose up -d\nCreating compose1_service1_1 ... done\n</code></pre> </li> <li> <p>Create the second docker-compose file with network configured</p> <pre><code>version: '3'\nservices:\n  service2:\n    image: busybox\n    command: sleep infinity\n\nnetworks:\n  default:\n    external:\n      name: external-example\n</code></pre> </li> <li> <p>Bring the service up</p> <pre><code>$ docker-compose up -d\nCreating compose2_service2_1 ... done\n</code></pre> </li> </ul> <p>After running <code>docker-compose up -d</code> on both docker-compose.yml files, we see that no new networks were created.</p> <pre><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n25e0c599d5e5        bridge              bridge              local\n2af4d92c2054        external-example    bridge              local\n7df4631e9cff        host                host                local\n194d4156d7ab        none                null                local\n</code></pre> <p>With the containers using the external-example network, they are able to ping one another.</p> <pre><code># By service name\n$ docker exec -it compose1_service1_1 ping service2\nPING service2 (172.24.0.3): 56 data bytes\n64 bytes from 172.24.0.3: seq=0 ttl=64 time=0.054 ms\n^C\n--- service2 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.054/0.054/0.054 ms\n\n# By container name\n$ docker exec -it compose1_service1_1 ping compose2_service2_1\nPING compose2_service2_1 (172.24.0.2): 56 data bytes\n64 bytes from 172.24.0.2: seq=0 ttl=64 time=0.042 ms\n^C\n--- compose2_service2_1 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.042/0.042/0.042 ms\n</code></pre> <p>The other way around works too.</p>"}, {"location": "docker/#troubleshooting", "title": "Troubleshooting", "text": "<p>If you are using a VPN and docker, you're going to have a hard time.</p> <p>The <code>docker</code> systemd service logs <code>systemctl status docker.service</code> usually doesn't give much information. Try to start the daemon directly with <code>sudo /usr/bin/dockerd</code>.</p>"}, {"location": "docker/#dont-store-credentials-in-plaintext", "title": "Don't store credentials in plaintext", "text": "<p>It doesn't work, don't go this painful road and assume that docker is broken.</p> <p>The official steps are horrible, and once you've spent two hours debugging it, you won't be able to push or pull images with your user.</p> <p>When you use <code>docker login</code> and introduce the user and password you get the next warning:</p> <pre><code>WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n</code></pre> <p>I got a nice surprise when I saw that <code>pass</code> was suggested in the link of the warning, to be used as a backend to store the password. But that feeling soon faded.</p> <p>To make docker understand that you want to use <code>pass</code> you need to use the <code>docker-credential-pass</code> script. A Go script \"maintained\" by docker, whose last commit was two years ago , has the CI broken and many old unanswered issues. Setting it up it's not easy either and it's ill documented.</p> <p>Furthermore, the script doesn't do what I expected, which is to store the password of your registry user in a pass entry. Instead, you need to create an empty pass entry in <code>docker-credential-helpers/docker-pass-initialized-check</code>, and when you use <code>docker login</code>, manually introducing your data, it creates another entry, as you can see in the next <code>pass</code> output:</p> <pre><code>Password Store\n\u2514\u2500\u2500 docker-credential-helpers\n    \u251c\u2500\u2500 aHR0cHM6Ly9pbmRleC5kb2NrZXIuaW8vdjEv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 lyz\n    \u2514\u2500\u2500 docker-pass-initialized-check\n</code></pre> <p>That entry is removed when you use <code>docker logout</code> so the next time you log in you need to introduce the user and password <code>(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b</code>.</p>"}, {"location": "docker/#installing-docker-credential-pass", "title": "Installing docker-credential-pass", "text": "<p>You first need to install the script:</p> <pre><code># Check for later releases at https://github.com/docker/docker-credential-helpers/releases\nversion=\"v0.6.3\"\narchive=\"docker-credential-pass-$version-amd64.tar.gz\"\nurl=\"https://github.com/docker/docker-credential-helpers/releases/download/$version/$archive\"\n\n# Download cred helper, unpack, make executable, and move it where Docker will find it.\nwget $url \\\n    &amp;&amp; tar -xf $archive \\\n    &amp;&amp; chmod +x docker-credential-pass \\\n    &amp;&amp; mv -f docker-credential-pass /usr/local/bin/\n</code></pre> <p>Another tricky issue is that even if you use a non-root user who's part of the <code>docker</code> group, the script is not aware of that, so it will look in the password store of root instead of the user's. This means that additionally to your own, you need to create a new password store for root. Follow the next steps with the root user:</p> <ul> <li>Create the password with <code>gpg --full-gen</code>, and copy the key id. Use a non     empty password, otherwise you are getting the same security as with the     password in cleartext.</li> <li>Initialize the password store <code>pass init gpg_id</code>, changing <code>gpg_id</code> for the     one of the last step.</li> <li> <p>Create the empty <code>docker-credential-helpers/docker-pass-initialized-check</code>     entry:</p> <pre><code>pass insert docker-credential-helpers/docker-pass-initialized-check\n</code></pre> <p>And press enter twice.</p> </li> </ul> <p>Finally we need to specify in the root's docker configuration that we want to use the <code>pass</code> credential storage.</p> <p>File: /root/.docker/config.json</p> <pre><code>{\n    \"credsStore\": \"pass\"\n}\n</code></pre>"}, {"location": "docker/#testing-it-works", "title": "Testing it works", "text": "<p>To test that docker is able to use pass as backend to store the credentials, run <code>docker login</code> and introduce the user and password. You should see the <code>Login Succeeded</code> message without any warning.</p> <pre><code>Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: lyz\nPassword:\nLogin Succeeded\n</code></pre> <p>Awful experience, wasn't it? Don't worry it gets worse.</p> <p>Now that you're logged in, whenever you try to push an image you're probably going to get an <code>denied: requested access to the resource is denied</code> error. That's because docker is not able to use the password it has stored in the root's password store. If you're using <code>root</code> to push the image (bad idea anyway), you will need to <code>export GPG_TTY=$(tty)</code> so that docker can ask you for your password to unlock root's <code>pass</code> entry. If you're like me that uses a non-root user belonging to the <code>docker</code> group, not even that works, so you've spent all this time reading and trying to fix everything for nothing... Thank you Docker <code>-.-</code>.</p>"}, {"location": "docker/#start-request-repeated-too-quickly", "title": "Start request repeated too quickly", "text": "<p>Shutdown the VPN and it will work. If it doesn't inspect the output of <code>journalctl -eu docker</code>.</p>"}, {"location": "documentation/", "title": "Write good documentation", "text": "<p>It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. Even if they have to use it because they have no choice, without good documentation, they won\u2019t use it effectively or the way you\u2019d like them to.</p> <p>People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all.</p> <p>They first need to get started and see how to solve specific problems. Then they need a way to search the software possibilities in a reference. Finally when they hit a road block, they need to understand how everything works so they can solve it.</p> <p>Each of these sections must be clearly differenced and the writing style must be adapted. The five types of documentation are:</p> <ul> <li>Introduction: A short description with optional pictures or screen casts,     that catches the user's attention and makes them want to use it. Like     the advertisement of your program.</li> <li>Get started: Lessons that allows the newcomer learn how to     start using the software. Like teaching a small child how to cook.</li> <li>How-to guides: Series of steps that show how to solve a specific problem. Like     a recipe in a cookery book.</li> <li>Technical reference: Searchable and organized dry     description of the software's machinery. Like a reference encyclopedia     article.</li> <li>Background information: Discursive explanations     that makes the user understand how the software works and how has it     evolved. Like an article on culinary social history.</li> </ul> <p>This division makes it obvious to both author and reader what material, and what kind of material, goes where. It tells the author how to write, what to write, and where to write it.</p>"}, {"location": "documentation/#introduction", "title": "Introduction", "text": "<p>The introduction is the first gateway for the users to your program, as such, it needs to be eye-catching, otherwise they will walk pass it to one of the other thousand programs or libraries out there.</p> <p>It needs to start with a short phrase that defines the whole project in a way that catches the user's attention.</p> <p>If the short phrase doesn't give enough context, you can add a small paragraph with further information. But don't make it too long, human's attention is weak.</p> <p>It's also a good idea to add a screenshot or screencast showing the usage of the program.</p> <p>Optionally, you can also add a list of features that differentiate your solution from the rest.</p>"}, {"location": "documentation/#get-started", "title": "Get started", "text": "<p>Made of tutorials that take the reader by the hand through a series of steps to complete a meaningful project achievable for a complete beginner. They are what your project needs in order to show a beginner that they can achieve something with it.</p> <p>Tutorials are what will turn your learners into users. A bad or missing tutorial will prevent your project from acquiring new users.</p> <p>They need to be useful for the beginner, easy to follow, meaningful, extremely robust, and kept up-to-date. You might well find that writing and maintaining your tutorials can occupy as much time and energy as the other four parts put together.</p>"}, {"location": "documentation/#how-to-write-good-tutorials", "title": "How to write good tutorials", "text": ""}, {"location": "documentation/#allow-the-user-to-learn-by-doing", "title": "Allow the user to learn by doing", "text": "<p>Your learner needs to do things. The different things that they do while following your tutorial need to cover a wide range of tools and operations, building up from the simplest ones at the start to more complex ones.</p>"}, {"location": "documentation/#get-the-user-started", "title": "Get the user started", "text": "<p>It\u2019s perfectly acceptable if your beginner\u2019s first steps are hand-held baby steps. It\u2019s also good if what you get the beginner to do is not the way an experienced person would, or even if it\u2019s not the \u2018correct\u2019 way.</p> <p>The point of a tutorial is to get your learner started on their journey, not to get them to a final destination.</p>"}, {"location": "documentation/#make-sure-that-your-tutorial-works", "title": "Make sure that your tutorial works", "text": "<p>One of your jobs as a tutor is to inspire the beginner\u2019s confidence: in the software, in the tutorial, in the tutor, and in their own ability to achieve what\u2019s being asked of them.</p> <p>There are many things that contribute to this. A friendly tone helps, as does consistent use of language, and a logical progression through the material. But the single most important thing is that what you ask the beginner to do must work.</p> <p>If the learner\u2019s actions produce an error or unexpected results, your tutorial has failed. When your students are there with you, you can rescue them; if they\u2019re reading your documentation on their own you can\u2019t. So you have to prevent that from happening in advance.</p> <p>One way of achieving this is by adding the snippets in your documentation to the test suite.</p>"}, {"location": "documentation/#ensure-the-user-sees-results-immediately", "title": "Ensure the user sees results immediately", "text": "<p>Everything the learner does should accomplish something comprehensible, however small. If your student has to do strange and incomprehensible things for two pages before they even see a result, that\u2019s much too long. The effect of every action should be visible and evident as soon as possible, and the connection to the action should be clear.</p> <p>The conclusion of each section of a tutorial, or the tutorial as a whole, must be a meaningful accomplishment.</p>"}, {"location": "documentation/#focus-on-concrete-steps-not-abstract-concepts", "title": "Focus on concrete steps, not abstract concepts", "text": "<p>Tutorials need to be concrete, built around specific, particular actions and outcomes.</p> <p>The temptation to introduce abstraction is huge; it is after all how most computing derives its power. But all learning proceeds from the particular and concrete to the general and abstract, and asking the learner to appreciate levels of abstraction before they have even had a chance to grasp the concrete is poor teaching.</p>"}, {"location": "documentation/#provide-the-minimum-necessary-explanation", "title": "Provide the minimum necessary explanation", "text": "<p>Don\u2019t explain anything the learner doesn\u2019t need to know in order to complete the tutorial. Extended discussion is important, just not in a tutorial. In a tutorial, it is an obstruction and a distraction. Only the bare minimum is appropriate. Instead, link to explanations elsewhere in the documentation.</p>"}, {"location": "documentation/#focus-only-on-the-steps-the-user-needs-to-take", "title": "Focus only on the steps the user needs to take", "text": "<p>Your tutorial needs to be focused on the task in hand. Maybe the command you\u2019re introducing has many other options, or maybe there are different ways to access a certain API. It doesn\u2019t matter: right now, your learner does not need to know about those in order to make progress.</p>"}, {"location": "documentation/#how-to-guides", "title": "How-to guides", "text": ""}, {"location": "documentation/#technical-reference", "title": "Technical reference", "text": ""}, {"location": "documentation/#background-information", "title": "Background information", "text": ""}, {"location": "documentation/#references", "title": "References", "text": "<ul> <li>divio's documentation wiki</li> <li>Vue's guidelines</li> <li>FastAPI awesome docs</li> </ul>"}, {"location": "drone/", "title": "Drone", "text": "<p>Drone is a modern Continuous Integration platform that empowers busy teams to automate their build, test and release workflows using a powerful, cloud native pipeline engine.</p>"}, {"location": "drone/#installation", "title": "Installation", "text": "<p>This section explains how to install the Drone server for Gitea.</p> <p>Note</p> <p>They explicitly recommend not to use Gitea and Drone in the same instance, and even less using <code>docker-compose</code> due to network complications. But if you have only a small instance as I do, you'll have to try :P.</p> <ul> <li>Create a Gitea user to be used by the CI.</li> <li>Log in with the drone Gitea user.</li> <li>Create a Gitea OAuth application. The Consumer Key and Consumer Secret are     used to authorize access to Gitea resources.</li> <li> <p>Create a shared secret to authenticate communication between runners and your     central Drone server.</p> <pre><code>openssl rand -hex 16\n</code></pre> </li> <li> <p>Create the required docker networks:     <pre><code>docker network create continuous-delivery\ndocker network create drone\ndocker network create swag\n</code></pre></p> </li> <li> <p>Create the docker-compose file for the server</p> <pre><code>---\nversion: '3'\n\nservices:\n  server:\n    image: drone/drone:2\n    environment:\n      - DRONE_GITEA_SERVER=https://try.gitea.io\n      - DRONE_GITEA_CLIENT_ID=05136e57d80189bef462\n      - DRONE_GITEA_CLIENT_SECRET=7c229228a77d2cbddaa61ddc78d45e\n      - DRONE_RPC_SECRET=super-duper-secret\n      - DRONE_SERVER_HOST=drone.company.com\n      - DRONE_SERVER_PROTO=https\n    container_name: drone\n    restart: always\n    networks:\n      - swag\n      - drone\n      - continuous-delivery\n    volumes:\n      - drone-data:/data\n\nnetworks:\n  continuous-delivery:\n    external:\n      name: continuous-delivery\n  drone:\n    external:\n      name: drone\n  swag:\n    external:\n      name: swag\n\nvolumes:\n  drone-data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /data/drone\n</code></pre> <p>Where we specify where we want the data to be stored at, and the networks to use. We're assuming that you're going to use the linuxserver swag proxy to end the ssl connection (which is accessible through the <code>swag</code> network), and that <code>gitea</code> is in the <code>continuous-delivery</code> network.</p> </li> <li> <p>Add the runners you want to install.</p> </li> <li>Configure your proxy to forward the requests to the correct dockers.</li> <li>Run <code>docker-compose up</code> from the file where your <code>docker-compose.yaml</code> file is     to test everything works. If it does, run <code>docker-compose down</code>.</li> <li>Create a systemd service to start and stop the whole service. For example     create the <code>/etc/systemd/system/drone.service</code> file with the content:     <pre><code>Description=drone\n[Unit]\nDescription=drone\nRequires=gitea.service\nAfter=gitea.service\n\n[Service]\nRestart=always\nUser=root\nGroup=docker\nWorkingDirectory=/data/config/continuous-delivery/drone\n# Shutdown container (if running) when unit is started\nTimeoutStartSec=100\nRestartSec=2s\n# Start container when unit is started\nExecStart=/usr/bin/docker-compose -f docker-compose.yml up\n# Stop container when unit is stopped\nExecStop=/usr/bin/docker-compose -f docker-compose.yml down\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> </ul>"}, {"location": "drone/#drone-runners", "title": "Drone Runners", "text": ""}, {"location": "drone/#docker-runner", "title": "Docker Runner", "text": "<p>Merge the next docker-compose with the one of the server above:</p> <pre><code>---\nversion: '3'\n\nservices:\n  docker_runner:\n    image: drone/drone-runner-docker:1\n    environment:\n      - DRONE_RPC_PROTO=https\n      - DRONE_RPC_HOST=drone.company.com\n      - DRONE_RPC_SECRET=super-duper-secret\n      - DRONE_RUNNER_CAPACITY=2\n      - DRONE_RUNNER_NAME=docker-runner\n    container_name: drone-docker-runner\n    restart: always\n    networks:\n      - drone\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    expose:\n      - \"3000\"\n\nnetworks:\n  drone:\n    external:\n      name: drone\n</code></pre> <p>Use the <code>docker logs</code> command to view the logs and verify the runner successfully established a connection with the Drone server.</p> <pre><code>$ docker logs runner\n\nINFO[0000] starting the server\nINFO[0000] successfully pinged the remote server\n</code></pre>"}, {"location": "drone/#ssh-runner", "title": "SSH Runner", "text": "<p>Merge the next docker-compose with the one of the server above:</p> <pre><code>---\nversion: '3'\n\nservices:\n  ssh_runner:\n    image: drone/drone-runner-ssh:latest\n    environment:\n      - DRONE_RPC_PROTO=https\n      - DRONE_RPC_HOST=drone.company.com\n      - DRONE_RPC_SECRET=super-duper-secret\n    container_name: drone-ssh-runner\n    restart: always\n    networks:\n      - drone\n    expose:\n      - \"3000\"\n\nnetworks:\n  drone:\n    external:\n      name: drone\n</code></pre> <p>Use the <code>docker logs</code> command to view the logs and verify the runner successfully established a connection with the Drone server.</p> <pre><code>$ docker logs runner\n\nINFO[0000] starting the server\nINFO[0000] successfully pinged the remote server\n</code></pre>"}, {"location": "drone/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Home</li> </ul>"}, {"location": "dunst/", "title": "Dunst", "text": "<p>Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection.</p>"}, {"location": "dunst/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install dunst\n</code></pre> <p>Test it's working with:</p> <pre><code>notify-send \"Notification Title\" \"Notification Messages\"\n</code></pre> <p>If your distro version is too old that doesn't have <code>dunstctl</code> or <code>dunstify</code>, you can install it manually:</p> <pre><code>git clone https://github.com/dunst-project/dunst.git\ncd dunst\n\n# Install dependencies\nsudo apt-get install libgdk-pixbuf2.0-0 libnotify-dev librust-pangocairo-dev\n\n# Build the program and install\nmake WAYLAND=0 SYSTEMD=1\nsudo make WAYLAND=0 SYSTEMD=1 install\n</code></pre> <p>Read and tweak the <code>~/.dunst/dunstrc</code> file to your liking.</p>"}, {"location": "dunst/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Home</li> <li>Archwiki page on dunst</li> </ul>"}, {"location": "dynamicdns/", "title": "Dynamic DNS notes", "text": "<p>Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information.</p> <p>There are different DDNS providers, I use Duckdns as it is easy to setup and the Linuxserver people have a docker that makes it work.</p>"}, {"location": "elasticsearch_exporter/", "title": "Blackbox Exporter", "text": "<p>The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus.</p>"}, {"location": "elasticsearch_exporter/#installation", "title": "Installation", "text": "<p>To install the exporter we'll use helmfile to install the prometheus-elasticsearch-exporter chart.</p> <p>Add the following lines to your <code>helmfile.yaml</code>.</p> <pre><code>- name: prometheus-elasticsearch-exporter\n  namespace: monitoring\n  chart: prometheus-community/prometheus-elasticsearch-exporter\n  values:\n    - prometheus-elasticsearch-exporter/values.yaml\n</code></pre> <p>Edit the chart values. <pre><code>mkdir prometheus-elasticsearch-exporter\nhelm inspect values prometheus-community/prometheus-elasticsearch-exporter &gt; prometheus-elasticsearch-exporter/values.yaml\nvi prometheus-elasticsearch-exporter/values.yaml\n</code></pre></p> <p>Comment out all the values you don't edit, so that the chart doesn't break when you upgrade it.</p> <p>Make sure that the <code>serviceMonitor</code> labels match your Prometheus <code>serviceMonitorSelector</code> otherwise they won't be added to the configuration.</p> <pre><code>es:\n  ## Address (host and port) of the Elasticsearch node we should connect to.\n  ## This could be a local node (localhost:9200, for instance), or the address\n  ## of a remote Elasticsearch server. When basic auth is needed,\n  ## specify as: &lt;proto&gt;://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;. e.g., http://admin:pass@localhost:9200.\n  ##\n  uri: http://localhost:9200\n\nserviceMonitor:\n  ## If true, a ServiceMonitor CRD is created for a prometheus operator\n  ## https://github.com/coreos/prometheus-operator\n  ##\n  enabled: true\n  #  namespace: monitoring\n  labels:\n    release: prometheus-operator\n  interval: 30s\n  # scrapeTimeout: 10s\n  # scheme: http\n  # relabelings: []\n  # targetLabels: []\n  metricRelabelings:\n    - sourceLabels: [cluster]\n      targetLabel: cluster_name\n      regex: '.*:(.*)'\n  # sampleLimit: 0\n</code></pre> <p>You can build the <code>cluster</code> label following this instructions, I didn't find the required meta tags, so I've built the <code>cluster_name</code> label for alerting purposes.</p> <p>The grafana dashboard I chose is <code>2322</code>. Taking as reference the grafana helm chart values, add the next yaml under the <code>grafana</code> key in the <code>prometheus-operator</code> <code>values.yaml</code>.</p> <pre><code>grafana:\n  enabled: true\n  defaultDashboardsEnabled: true\n  dashboardProviders:\n    dashboardproviders.yaml:\n      apiVersion: 1\n      providers:\n      - name: 'default'\n        orgId: 1\n        folder: ''\n        type: file\n        disableDeletion: false\n        editable: true\n        options:\n          path: /var/lib/grafana/dashboards/default\n  dashboards:\n    default:\n      elasticsearch:\n        # Ref: https://grafana.com/dashboards/2322\n        gnetId: 2322\n        revision: 4\n        datasource: Prometheus\n</code></pre> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre>"}, {"location": "elasticsearch_exporter/#elasticsearch-exporter-alerts", "title": "Elasticsearch exporter alerts", "text": "<p>Now that we've got the metrics, we can define the alert rules. Most have been tweaked from the Awesome prometheus alert rules collection.</p>"}, {"location": "elasticsearch_exporter/#availability-alerts", "title": "Availability alerts", "text": "<p>The most basic probes, test if the service is healthy</p> <pre><code>- alert: ElasticsearchClusterRed\n  expr: elasticsearch_cluster_health_status{color=\"red\"} == 1\n  for: 0m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch Cluster Red\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elastic Cluster Red status\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchClusterYellow\n  expr: elasticsearch_cluster_health_status{color=\"yellow\"} == 1\n  for: 0m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch Cluster Yellow\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elastic Cluster Yellow status\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchHealthyNodes\n  expr: elasticsearch_cluster_health_number_of_nodes &lt; 3\n  for: 0m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch Healthy Nodes\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Missing node in Elasticsearch cluster\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchHealthyMasterNodes\n  expr: &gt;\n    elasticsearch_cluster_health_number_of_nodes\n    - elasticsearch_cluster_health_number_of_data_nodes &gt; 0 &lt; 3\n  for: 0m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch Healthy Master Nodes &lt; 3\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Missing master node in Elasticsearch cluster\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchHealthyDataNodes\n  expr: elasticsearch_cluster_health_number_of_data_nodes &lt; 3\n  for: 0m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch Healthy Data Nodes\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Missing data node in Elasticsearch cluster\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#performance-alerts", "title": "Performance alerts", "text": "<pre><code>- alert: ElasticsearchCPUUsageTooHigh\n  expr: elasticsearch_os_cpu_percent &gt; 90\n  for: 2m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch Node CPU Usage Too High\n      (cluster {{ $labels.cluster_name }} node {{ $labels.name }})\n    description: |\n      The CPU usage of node {{ $labels.name }} is over 90%\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchCPUUsageWarning\n  expr: elasticsearch_os_cpu_percent &gt; 80\n  for: 2m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch Node CPU Usage Too High\n      (cluster {{ $labels.cluster_name }} node {{ $labels.name }})\n    description: |\n      The CPU usage of node {{ $labels.name }} is over 90%\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchHeapUsageTooHigh\n  expr: &gt;\n    (\n      elasticsearch_jvm_memory_used_bytes{area=\"heap\"}\n      / elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n    ) * 100 &gt; 90\n  for: 2m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch Node Heap Usage Critical\n      (cluster {{ $labels.cluster_name }} node {{ $labels.name }})\n    description: |\n      The heap usage of node {{ $labels.name }} is over 90%\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchHeapUsageWarning\n  expr: &gt;\n    (\n      elasticsearch_jvm_memory_used_bytes{area=\"heap\"}\n      / elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n    ) * 100 &gt; 80\n  for: 2m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch Node Heap Usage Warning\n      (cluster {{ $labels.cluster_name }} node {{ $labels.name }})\n    description: |\n      The heap usage of node {{ $labels.name }} is over 80%\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchDiskOutOfSpace\n  expr: &gt;\n    elasticsearch_filesystem_data_available_bytes\n    / elasticsearch_filesystem_data_size_bytes * 100 &lt; 10\n  for: 0m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch disk out of space\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      The disk usage is over 90%\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchDiskSpaceLow\n  expr: &gt;\n    elasticsearch_filesystem_data_available_bytes\n    / elasticsearch_filesystem_data_size_bytes * 100 &lt; 20\n  for: 2m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch disk space low\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      The disk usage is over 80%\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchRelocatingShardsTooLong\n  expr: elasticsearch_cluster_health_relocating_shards &gt; 0\n  for: 15m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch relocating shards too long\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elasticsearch has been relocating shards for 15min\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchInitializingShardsTooLong\n  expr: elasticsearch_cluster_health_initializing_shards &gt; 0\n  for: 15m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch initializing shards too long\n      (cluster_name {{ $labels.cluster }})\n    description: |\n      Elasticsearch has been initializing shards for 15 min\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchUnassignedShards\n  expr: elasticsearch_cluster_health_unassigned_shards &gt; 0\n  for: 0m\n  labels:\n    severity: critical\n  annotations:\n    summary: &gt;\n      Elasticsearch unassigned shards\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elasticsearch has unassigned shards\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchPendingTasks\n  expr: elasticsearch_cluster_health_number_of_pending_tasks &gt; 0\n  for: 15m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch pending tasks\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elasticsearch has pending tasks. Cluster works slowly.\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n\n- alert: ElasticsearchCountOfJVMGarbageCollectorRuns\n  expr: rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m]) &gt; 5\n  for: 1m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch JVM Garbage Collector runs &gt; 5\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elastic Cluster JVM Garbage Collector runs &gt; 5\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n\n- alert: ElasticsearchCountOfJVMGarbageCollectorTime\n  expr: rate(elasticsearch_jvm_gc_collection_seconds_sum[5m]) &gt; 0.3\n  for: 1m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch JVM Garbage Collector time &gt; 0.3\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elastic Cluster JVM Garbage Collector runs &gt; 0.3\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchJSONParseErrors\n  expr: elasticsearch_cluster_health_json_parse_failures &gt; 0\n  for: 1m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch json parse error\n      (cluster {{ $labels.cluster_name }})\n    description: |\n      Elasticsearch json parse error\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n- alert: ElasticsearchCircuitBreakerTripped\n  expr: rate(elasticsearch_breakers_tripped{}[5m])&gt;0\n  for: 1m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch breaker {{ $labels.breaker }} tripped\n      (cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\n    description: |\n      Elasticsearch breaker {{ $labels.breaker }} tripped\n      (cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#snapshot-alerts", "title": "Snapshot alerts", "text": "<pre><code>- alert: ElasticsearchMonthlySnapshot\n  expr: &gt;\n    time() -\n    elasticsearch_snapshot_stats_snapshot_end_time_timestamp{state=\"SUCCESS\"}\n    &gt; (3600 * 24 * 32)\n  for: 15m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch monthly snapshot failed\n      (cluster {{ $labels.cluster_name }},\n      snapshot {{ $labels.repository }})\n    description: |\n      Last successful elasticsearch snapshot\n      of repository {{ $labels.repository}} is older than 32 days.\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n\n- record: elasticsearch_indices_search_latency:rate1m\n  expr: |\n    increase(elasticsearch_indices_search_query_time_seconds[1m])/\n    increase(elasticsearch_indices_search_query_total[1m])\n- record: elasticsearch_indices_search_rate:rate1m\n  expr: increase(elasticsearch_indices_search_query_total[1m])/60\n- alert: ElasticsearchSlowSearchLatency\n  expr: elasticsearch_indices_search_latency:rate1m &gt; 1\n  for: 2m\n  labels:\n    severity: warning\n  annotations:\n    summary: &gt;\n      Elasticsearch search latency is greater than 1 s\n      (cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\n    description: |\n      Elasticsearch search latency is greater than 1 s\n      (cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\n      VALUE = {{ $value }}\n      LABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#links", "title": "Links", "text": "<ul> <li>Git</li> </ul>"}, {"location": "email_automation/", "title": "Email automation", "text": "<p>Most of the received emails require repetitive actions that can be automated, and you may also want to access your emails through a command line interface and be able to search through them.</p> <p>One of the ways to achieve that goals is to use a combination of tools to synchronize the mailboxes, tag them, and run scripts automatically based on the tags.</p>"}, {"location": "email_automation/#installation", "title": "Installation", "text": "<p>First you need a program that syncs your mailboxes, following pazz's advice , I'll use mbsync. Follow the steps under installation to configure your accounts, taking as an example an account called <code>lyz</code> you should be able to sync all your emails with:</p> <pre><code>mbsync -V lyz\n</code></pre> <p>Now we need to install <code>notmuch</code> a tool to index, search, read, and tag large collections of email messages. Follow the steps under installation under you have created the database that indexes your emails.</p> <p>Once we have that, we need a tool to tag the emails following our desired rules. afew is one way to go. Follow the steps under installation.</p> <p>The remaining step to keep the inboxes synced and tagged is to run all the steps above in a cron. Particularize pazz's script for your usecase:</p> <pre><code>#!/bin/bash\n#\n# Download and index new mail.\n#\n# Copyright (c) 2017 Patrick Totzke\n# Dependencies: flock, nm-online, mbsync, notmuch, afew\n# Example crontab entry:\n#\n#   */2 * * * * /usr/bin/flock -n /home/pazz/.pullmail.lock /home/pazz/bin/pullmail.sh &gt; /home/pazz/.pullmail.log\n#\n\nPATH=/home/pazz/.local/bin:/usr/local/bin/:$PATH\nACCOUNTDIR=/home/pazz/.pullmail/\n\n# this makes the keyring daemon accessible\nfunction keyring-control() {\n        local -a vars=( \\\n                DBUS_SESSION_BUS_ADDRESS \\\n                GNOME_KEYRING_CONTROL \\\n                GNOME_KEYRING_PID \\\n                XDG_SESSION_COOKIE \\\n                GPG_AGENT_INFO \\\n                SSH_AUTH_SOCK \\\n        )\n        local pid=$(ps -C i3 -o pid --no-heading)\n        eval \"unset ${vars[@]}; $(printf \"export %s;\" $(sed 's/\\x00/\\n/g' /proc/${pid//[^0-9]/}/environ | grep $(printf -- \"-e ^%s= \" \"${vars[@]}\")) )\"\n}\n\nfunction log() {\n    notify-send -t 2000  'mail sync:' \"$@\"\n}\n\nfunction die() {\n    notify-send -t 2000 -u critical 'mail sync:' \"$@\"\n    exit 1\n}\n\n# Let's Do stuff\nkeyring-control\n\n# abort as soon as something fails\nset -e\n\n# abort if not online\nnm-online -x -t 0\n\necho ---------------------------------------------------------\ndate\nfor accfile in `ls $ACCOUNTDIR`;\ndo\n    ACC=$(basename $accfile)\n    echo ------------------------  $ACC   ------------------------\n    mbsync -V $ACC || log \"$ACC failed\"\ndone\n\n# index and tag new mails\necho ------------------------ NOTMUCH ------------------------\nnotmuch new 2&gt;/dev/null || die \"NOTMUCH new failed\"\n\necho ------------------------  AFEW   ------------------------\nafew -v --tag --new || die \"AFEW died\"\n\necho ---------------------------------------------------------\necho \"all done, goodbye.\"\n</code></pre> <p>Where <code>flock</code> is a tool to manage locks from shell scripts.</p> <p>And add the entry in your <code>crontab -e</code>.</p> <p>If you want to process your emails with this system through a command line interface, you can configure alot.</p>"}, {"location": "email_management/", "title": "Email management", "text": "<p>Email can be one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification backend of services that don't need to be acted upon immediately or when more powerful mechanisms are not available.</p> <p>If not used wisely, it can be a sink of productivity.</p>"}, {"location": "email_management/#analyze-how-often-you-need-to-check-it", "title": "Analyze how often you need to check it", "text": "<p>Follow the interruption analysis to discover how often you need to check it and if you need the notifications. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.</p>"}, {"location": "email_management/#workflow", "title": "Workflow", "text": "<p>Each time I decide to go through my emails I follow the inbox processing guidelines. I understand the email inbox are items that need to be taken care of. If an email doesn't fall in that category I either archive or delete it. That way the inbox has the smallest number of items, and if everything went well, it is empty. Having an empty inbox helps you a lot to reduce the mental load for many reasons:</p> <ul> <li>When you look at it and don't see any mail, you get the small satisfaction     that you have done everything.</li> <li>When there is something new, it stands out, without the distraction of other     email subjects that can drift your attention.</li> </ul>"}, {"location": "email_management/#accounts-shared-by-many-people", "title": "Accounts shared by many people", "text": "<p>On email accounts managed by many people, I delete/archive emails that I know that need no interaction by any of them. If there is nothing for me to do, I mark them as read and wait for them to archive/delete them. If an email is left unread for 3 or 4 days I ask by other channels what should we do with that event.</p>"}, {"location": "email_management/#use-email-to-transport-information-not-to-store-it", "title": "Use email to transport information, not to store it", "text": "<p>Email was envisioned as a protocol for person A to send information to person B. The fact that the \"free email providers\" such as Google allow users to have almost no limit on their inbox has driven people to store all their emails and use it as a knowledge repository. This approach has many problems:</p> <ul> <li>As most people don't use end to end encryption (GPG), the data of their emails     is available for the email provider to read. This is a privacy violation     that leads to scary behaviours, such as targeted adds or google suggestions     based on the content of recent emails. You could improve the situation by     using POP3 instead of IMAP, but that'll force you to only use one device to     check your email, something that's becoming uncommon.</li> <li>The decent email providers that respect you, such as RiseUp,     Autistici or     Disroot, are maintained by communities and can     only offer a limited storage, so you're forced to empty your emails     periodically to be able to receive new ones.</li> <li>If you don't spend time and effort classifying your emails, searching between     them is a nightmare. It is even if you classify them. There are more     efficient knowledge repositories to store your information.</li> </ul> <p>On my personal emails, I forward the information to my archive, task manager or knowledge manager, deleting the email afterwards. At work, they use an indecent provider, encrypts most of emails with GPG and trust the provider to hold the rest of the data. I try to leak the least amount of personal information and I archive every email because you don't know when you're going to need them.</p>"}, {"location": "email_management/#use-key-bindings", "title": "Use key bindings", "text": "<p>Using the mouse to interact with the email client graphical interface is not efficient, try to learn the key bindings and use them as much as possible.</p>"}, {"location": "email_management/#environment-setup", "title": "Environment setup", "text": ""}, {"location": "email_management/#account-management", "title": "Account management", "text": "<p>It's common to have more than one account to check. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities.</p> <p>For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird. Once you choose one, try to master it.</p>"}, {"location": "email_management/#isolate-your-work-and-personal-environments", "title": "Isolate your work and personal environments", "text": "<p>Make sure that you set your environment so that you can't check your personal email when you're working and the other way around. For example, you could set two Thunderbird profiles, or you could avoid configuring the work email in your personal phone.</p>"}, {"location": "email_management/#automatic-filtering-and-processing", "title": "Automatic filtering and processing", "text": "<p>Inbox management is time consuming, so you want to reduce the number of emails to process. From the interruption analysis you'll know which ones don't give you any value, our goal is to make them disappear before we open our inbox.</p> <p>You can get rid of them by:</p> <ul> <li>Preventing the sender to send them: Unsubscribe from the newsletters you no     longer read or fix the configuration of the services that send you     notifications that don't want.</li> <li>Tweak your spam filter: If you have no control on the source, tweak your spam     filter so that it filters them out for you.</li> <li>Use your email client filtering and processing features: If you want to     receive the emails for archival purposes, configure your email client to     match them by regular expressions on the sender or subject, mark them as     read and move them to the desired directory.</li> <li>Use email automation software: If you want to run automatic processes     triggered by emails, use email automation     solutions.</li> </ul>"}, {"location": "email_management/#use-your-preferred-editor-to-write-the-emails", "title": "Use your preferred editor to write the emails", "text": "<p>You'll probably be less efficient with the email client's editor in comparison with your own. If you use vim or emacs, there's a good chance that the email client has a plugin that allows you to use it. Or you can always migrate to a command line client. I'll probably do that once I set up the email automation system.</p>"}, {"location": "emojis/", "title": "Emojis", "text": "<p>Curated list of emojis to copy paste.</p>"}, {"location": "emojis/#angry", "title": "Angry", "text": "<pre><code>(\u0482\u2323\u0300_\u2323\u0301)\n\n( &gt;\u0434&lt;)\n\n\u0295\u2022\u0300o\u2022\u0301\u0294\n\n\u30fd(\u2267\u0414\u2266)\u30ce\n\n\u1559(\u21c0\u2038\u21bc\u2036)\u1557\n\n\u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6\n</code></pre>"}, {"location": "emojis/#annoyed", "title": "Annoyed", "text": "<pre><code>(\u2256\u035e_\u2256\u0325)\n(&gt;_&lt;)\n</code></pre>"}, {"location": "emojis/#awesome", "title": "Awesome", "text": "<pre><code>( \u00b7_\u00b7)\n( \u00b7_\u00b7)       --\u25a0-\u25a0\n( \u00b7_\u00b7)--\u25a0-\u25a0\n(-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH\n</code></pre>"}, {"location": "emojis/#conforting", "title": "Conforting", "text": "<pre><code>(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)\n</code></pre>"}, {"location": "emojis/#congratulations", "title": "Congratulations", "text": "<pre><code>( \u141b )\u0648\n\n\uff3c\\ \u0669( \u141b )\u0648 /\uff0f\n</code></pre>"}, {"location": "emojis/#crying", "title": "Crying", "text": "<pre><code>(\u2565\ufe4f\u2565)\n(\u0ca5\ufe4f\u0ca5)\n</code></pre>"}, {"location": "emojis/#excited", "title": "Excited", "text": "<pre><code>(((o(*\uff9f\u25bd\uff9f*)o)))\n\no(\u2267\u2207\u2266o)\n</code></pre>"}, {"location": "emojis/#dance", "title": "Dance", "text": "<pre><code>(~\u203e\u25bf\u203e)~   ~(\u203e\u25bf\u203e)~   ~(\u203e\u25bf\u203e~)\n\n\u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518\n\n\u01aa(\u02d8\u2323\u02d8)\u2510    \u01aa(\u02d8\u2323\u02d8)\u0283    \u250c(\u02d8\u2323\u02d8)\u0283\n\n(&gt;'-')&gt;\n&lt;('-'&lt;)\n^('-')^\nv('-')v\n(&gt;'-')&gt;\n (^-^)\n</code></pre>"}, {"location": "emojis/#happy", "title": "Happy", "text": "<pre><code>\u1555( \u141b )\u1557\n\n\u0295\u2022\u1d25\u2022\u0294\n\n(\u2022\u203f\u2022)\n\n(\u25e1\u203f\u25e1\u273f)\n\n(\u273f\u25e0\u203f\u25e0)\n\n\u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a\n</code></pre>"}, {"location": "emojis/#kisses", "title": "Kisses", "text": "<pre><code>(\u3065\uffe3 \u00b3\uffe3)\u3065\n\n( \u02d8 \u00b3\u02d8)\u2665\n</code></pre>"}, {"location": "emojis/#love", "title": "Love", "text": "<pre><code>\u2764\n</code></pre>"}, {"location": "emojis/#pride", "title": "Pride", "text": "<pre><code>&lt;(\uffe3\uff3e\uffe3)&gt;\n</code></pre>"}, {"location": "emojis/#relax", "title": "Relax", "text": "<pre><code>_\u3078__(\u203e\u25e1\u25dd )&gt;\n</code></pre>"}, {"location": "emojis/#sad", "title": "Sad", "text": "<pre><code>\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61\n\n(\u25de\u2038\u25df\uff1b)\n</code></pre>"}, {"location": "emojis/#scared", "title": "Scared", "text": "<pre><code>\u30fd(\uff9f\u0414\uff9f)\uff89\n\n\u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f\n</code></pre>"}, {"location": "emojis/#sleepy", "title": "Sleepy", "text": "<pre><code>(\u1d17\u02f3\u1d17)\n</code></pre>"}, {"location": "emojis/#smug", "title": "Smug", "text": "<pre><code>\uff08\uffe3\uff5e\uffe3\uff09\n</code></pre>"}, {"location": "emojis/#whyyyy", "title": "Whyyyy?", "text": "<pre><code>(/\uff9f\u0414\uff9f)/\n</code></pre>"}, {"location": "emojis/#surprised", "title": "Surprised", "text": "<pre><code>(\\_/)\n(O.o)\n(&gt; &lt;)\n\n(\u2299_\u2609)\n\n(\u00ac\u00ba-\u00b0)\u00ac\n\n(\u2609_\u2609)\n\n(\u2022 \u0325\u0306\u2006\u2022)\n\n\u00af\\(\u00b0_o)/\u00af\n\n(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002)\n\n(*\uff9f\u25ef\uff9f*)\n</code></pre>"}, {"location": "emojis/#who-cares", "title": "Who cares", "text": "<pre><code>\u00af\\_(\u30c4)_/\u00af\n</code></pre>"}, {"location": "emojis/#wtf", "title": "WTF", "text": "<pre><code>(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b\n\n\u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8\n</code></pre>"}, {"location": "emojis/#links", "title": "Links", "text": "<ul> <li>Japanese Emoticons</li> </ul>"}, {"location": "environmentalism/", "title": "Environmentalism", "text": ""}, {"location": "environmentalism/#measure-the-carbon-footprint-of-your-travels", "title": "Measure the carbon footprint of your travels", "text": "<p>https://www.carbonfootprint.com/</p> <p>There are also some calculators for events itself:</p> <p>https://co2.myclimate.org/en/event_calculators/new https://psci.princeton.edu/events-emissions-calculator</p>"}, {"location": "environmentalism/#saving-water", "title": "Saving water", "text": "<p>Here are some small things I'm doing to save some water each day:</p> <ul> <li>Use the watering can or a bucket to gather the shower water until it's warm   enough. I use this water to flush the toilet. It would be best if it were   possible to fill up the toilet's deposit, but it's not easy.</li> <li>Use a glass of water to wet the toothbrush and rinse my mouth instead of using   running water.</li> </ul>"}, {"location": "fastapi/", "title": "FastAPI", "text": "<p>FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.</p> <p>The key features are:</p> <ul> <li>Fast: Very high performance, on par with NodeJS and Go (thanks to Starlette   and Pydantic). One of the fastest Python frameworks available.</li> <li>Fast to code: Increase the speed to develop features by about 200% to 300%.</li> <li>Fewer bugs: Reduce about 40% of human (developer) induced errors.</li> <li>Intuitive: Great editor support. Completion everywhere. Less time debugging.</li> <li>Easy: Designed to be easy to use and learn. Less time reading docs.</li> <li>Short: Minimize code duplication. Multiple features from each parameter   declaration. Fewer bugs.</li> <li>Robust: Get production-ready code. With automatic interactive documentation.</li> <li>Standards-based: Based on (and fully compatible with) the open standards for   APIs: OpenAPI (previously known as Swagger) and JSON Schema.</li> <li>Authentication with JWT:   with a super nice tutorial on how to set it up.</li> </ul>"}, {"location": "fastapi/#installation", "title": "Installation", "text": "<pre><code>pip install fastapi\n</code></pre> <p>You will also need an ASGI server, for production such as Uvicorn or Hypercorn.</p> <pre><code>pip install uvicorn[standard]\n</code></pre>"}, {"location": "fastapi/#simple-example", "title": "Simple example", "text": "<ul> <li>Create a file <code>main.py</code> with:</li> </ul> <pre><code>from typing import Optional\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Optional[str] = None):\n    return {\"item_id\": item_id, \"q\": q}\n</code></pre> <ul> <li>Run the server:</li> </ul> <pre><code>uvicorn main:app --reload\n</code></pre> <ul> <li>Open your browser at http://127.0.0.1:8000/items/5?q=somequery. You will see   the JSON response as:</li> </ul> <pre><code>{\n  \"item_id\": 5,\n  \"q\": \"somequery\"\n}\n</code></pre> <p>You already created an API that:</p> <ul> <li>Receives HTTP requests in the paths <code>/</code> and <code>/items/{item_id}</code>.</li> <li>Both paths take GET operations (also known as HTTP methods).</li> <li>The path <code>/items/{item_id}</code> has a path parameter <code>item_id</code> that should be an   <code>int</code>.</li> <li>The path <code>/items/{item_id}</code> has an optional <code>str</code> query parameter <code>q</code>.</li> <li>Has interactive API docs made for you:</li> <li>Swagger: http://127.0.0.1:8000/docs.</li> <li>Redoc: http://127.0.0.1:8000/redoc.</li> </ul> <p>You will see the automatic interactive API documentation (provided by Swagger UI):</p>"}, {"location": "fastapi/#sending-data-to-the-server", "title": "Sending data to the server", "text": "<p>When you need to send data from a client (let's say, a browser) to your API, you have three basic options:</p> <ul> <li>As path parameters in the URL (<code>/items/2</code>).</li> <li>As query parameters in the URL (<code>/items/2?skip=true</code>).</li> <li>In the body of a POST request.</li> </ul> <p>To send simple data use the first two, to send complex or sensitive data, use the last.</p> <p>It also supports sending data through cookies and headers.</p>"}, {"location": "fastapi/#path-parameters", "title": "Path Parameters", "text": "<p>You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings:</p> <pre><code>@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    return {\"item_id\": item_id}\n</code></pre> <p>If you define the type hints of the function arguments, FastAPI will use pydantic data validation.</p> <p>If you need to use a Linux path as an argument, check this workaround, but be aware that it's not supported by OpenAPI.</p>"}, {"location": "fastapi/#order-matters", "title": "Order matters", "text": "<p>Because path operations are evaluated in order, you need to make sure that the path for the fixed endpoint <code>/users/me</code> is declared before the variable one <code>/users/{user_id}</code>:</p> <pre><code>@app.get(\"/users/me\")\nasync def read_user_me():\n    return {\"user_id\": \"the current user\"}\n\n\n@app.get(\"/users/{user_id}\")\nasync def read_user(user_id: str):\n    return {\"user_id\": user_id}\n</code></pre> <p>Otherwise, the path for <code>/users/{user_id}</code> would match also for <code>/users/me</code>, \"thinking\" that it's receiving a parameter user_id with a value of \"me\".</p>"}, {"location": "fastapi/#predefined-values", "title": "Predefined values", "text": "<p>If you want the possible valid path parameter values to be predefined, you can use a standard Python <code>Enum</code>.</p> <pre><code>from enum import Enum\n\n\nclass ModelName(str, Enum):\n    alexnet = \"alexnet\"\n    resnet = \"resnet\"\n    lenet = \"lenet\"\n\n\n@app.get(\"/models/{model_name}\")\ndef get_model(model_name: ModelName):\n    if model_name == ModelName.alexnet:\n        return {\"model_name\": model_name, \"message\": \"Deep Learning FTW!\"}\n\n    if model_name.value == \"lenet\":\n        return {\"model_name\": model_name, \"message\": \"LeCNN all the images\"}\n\n    return {\"model_name\": model_name, \"message\": \"Have some residuals\"}\n</code></pre> <p>These are the basics, FastAPI supports more complex path parameters and string validations.</p>"}, {"location": "fastapi/#query-parameters", "title": "Query Parameters", "text": "<p>When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters.</p> <pre><code>fake_items_db = [{\"item_name\": \"Foo\"}, {\"item_name\": \"Bar\"}, {\"item_name\": \"Baz\"}]\n\n\n@app.get(\"/items/\")\nasync def read_item(skip: int = 0, limit: int = 10):\n    return fake_items_db[skip : skip + limit]\n</code></pre> <p>The query is the set of key-value pairs that go after the <code>?</code> in a URL, separated by <code>&amp;</code> characters.</p> <p>For example, in the URL: http://127.0.0.1:8000/items/?skip=0&amp;limit=10</p> <p>These are the basics, FastAPI supports more complex query parameters and string validations.</p>"}, {"location": "fastapi/#request-body", "title": "Request Body", "text": "<p>To declare a request body, you use Pydantic models with all their power and benefits.</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    name: str\n    description: Optional[str] = None\n    price: float\n    tax: Optional[float] = None\n\n\n@app.post(\"/items/\")\nasync def create_item(item: Item):\n    return item\n</code></pre> <p>With just that Python type declaration, FastAPI will:</p> <ul> <li>Read the body of the request as JSON.</li> <li>Convert the corresponding types (if needed).</li> <li>Validate the data: If the data is invalid, it will return a nice and clear   error, indicating exactly where and what was the incorrect data.</li> <li>Give you the received data in the parameter <code>item</code>.</li> <li>Generate JSON Schema definitions for your model.</li> <li>Those schemas will be part of the generated OpenAPI schema, and used by the   automatic documentation UIs.</li> </ul> <p>These are the basics, FastAPI supports more complex patterns such as:</p> <ul> <li>Using multiple models in the same query.</li> <li>Additional validations of the pydantic models.</li> <li>Nested models.</li> </ul>"}, {"location": "fastapi/#sending-data-to-the-client", "title": "Sending data to the client", "text": "<p>When you create a FastAPI path operation you can normally return any data from it: a <code>dict</code>, a <code>list</code>, a Pydantic model, a database model, etc.</p> <p>By default, FastAPI would automatically convert that return value to JSON using the <code>jsonable_encoder</code>.</p> <p>To return custom responses such as a direct string, xml or html use <code>Response</code>:</p> <pre><code>from fastapi import FastAPI, Response\n\napp = FastAPI()\n\n\n@app.get(\"/legacy/\")\ndef get_legacy_data():\n    data = \"\"\"&lt;?xml version=\"1.0\"?&gt;\n    &lt;shampoo&gt;\n    &lt;Header&gt;\n        Apply shampoo here.\n    &lt;/Header&gt;\n    &lt;Body&gt;\n        You'll have to use soap here.\n    &lt;/Body&gt;\n    &lt;/shampoo&gt;\n    \"\"\"\n    return Response(content=data, media_type=\"application/xml\")\n</code></pre>"}, {"location": "fastapi/#handling-errors", "title": "Handling errors", "text": "<p>There are many situations in where you need to notify an error to a client that is using your API.</p> <p>In these cases, you would normally return an HTTP status code in the range of 400 (from 400 to 499).</p> <p>This is similar to the 200 HTTP status codes (from 200 to 299). Those \"200\" status codes mean that somehow there was a \"success\" in the request.</p> <p>To return HTTP responses with errors to the client you use <code>HTTPException</code>.</p> <pre><code>from fastapi import HTTPException\n\nitems = {\"foo\": \"The Foo Wrestlers\"}\n\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: str):\n    if item_id not in items:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return {\"item\": items[item_id]}\n</code></pre>"}, {"location": "fastapi/#updating-data", "title": "Updating data", "text": ""}, {"location": "fastapi/#update-replacing-with-put", "title": "Update replacing with PUT", "text": "<p>To update an item you can use the HTTP PUT operation.</p> <p>You can use the <code>jsonable_encoder</code> to convert the input data to data that can be stored as JSON (e.g. with a NoSQL database). For example, converting datetime to str.</p> <pre><code>from typing import List, Optional\n\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    price: Optional[float] = None\n    tax: float = 10.5\n    tags: List[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.put(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    update_item_encoded = jsonable_encoder(item)\n    items[item_id] = update_item_encoded\n    return update_item_encoded\n</code></pre>"}, {"location": "fastapi/#partial-updates-with-patch", "title": "Partial updates with PATCH", "text": "<p>You can also use the HTTP PATCH operation to partially update data.</p> <p>This means that you can send only the data that you want to update, leaving the rest intact.</p>"}, {"location": "fastapi/#configuration", "title": "Configuration", "text": ""}, {"location": "fastapi/#application-configuration", "title": "Application configuration", "text": "<p>In many cases your application could need some external settings or configurations, for example secret keys, database credentials, credentials for email services, etc.</p> <p>You can load these configurations through environmental variables, or you can use the awesome Pydantic settings management, whose advantages are:</p> <ul> <li>Do Pydantic's type validation on the fields.</li> <li>Automatically reads the missing values from environmental variables.</li> <li>Supports reading variables from   Dotenv files.</li> <li>Supports secrets.</li> </ul> <p>First you define the <code>Settings</code> class with all the fields:</p> <p>File: <code>config.py</code>:</p> <pre><code>from pydantic import BaseSettings\n\n\nclass Settings(BaseSettings):\n    verbose: bool = True\n    database_url: str = \"tinydb://~/.local/share/pyscrobbler/database.tinydb\"\n</code></pre> <p>Then in the api definition, set the dependency.</p> <p>File: <code>api.py</code>:</p> <pre><code>from functools import lru_cache\nfrom fastapi import Depends, FastAPI\n\n\napp = FastAPI()\n\n\n@lru_cache()\ndef get_settings() -&gt; Settings:\n    \"\"\"Configure the program settings.\"\"\"\n    return Settings()\n\n\n@app.get(\"/verbose\")\ndef verbose(settings: Settings = Depends(get_settings)) -&gt; bool:\n    return settings.verbose\n</code></pre> <p>Where:</p> <ul> <li> <p><code>get_settings</code> is the dependency function that configures the <code>Settings</code>   object. The endpoint <code>verbose</code> is   dependant of <code>get_settings</code>.</p> </li> <li> <p>The <code>@lru_cache</code> decorator   changes the function it decorates to return the same value that was returned   the first time, instead of computing it again, executing the code of the   function every time.</p> </li> </ul> <p>So, the function will be executed once for each combination of arguments. And   then the values returned by each of those combinations of arguments will be   used again and again whenever the function is called with exactly the same   combination of arguments.</p> <p>Creating the <code>Settings</code> object is a costly operation as it needs to check the   environment variables or read a file, so we want to do it just once, not on   each request.</p> <p>This setup makes it easy to inject testing configuration so as not to break production code.</p>"}, {"location": "fastapi/#openapi-configuration", "title": "OpenAPI configuration", "text": ""}, {"location": "fastapi/#define-title-description-and-version", "title": "Define title, description and version", "text": "<pre><code>from fastapi import FastAPI\n\napp = FastAPI(\n    title=\"My Super Project\",\n    description=\"This is a very fancy project, with auto docs for the API and everything\",\n    version=\"2.5.0\",\n)\n</code></pre>"}, {"location": "fastapi/#define-path-tags", "title": "Define path tags", "text": "<p>You can add tags to your path operation, pass the parameter tags with a list of <code>str</code> (commonly just one <code>str</code>):</p> <pre><code>from typing import Optional, Set\n\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    name: str\n    description: Optional[str] = None\n    price: float\n    tax: Optional[float] = None\n    tags: Set[str] = []\n\n\n@app.post(\"/items/\", response_model=Item, tags=[\"items\"])\nasync def create_item(item: Item):\n    return item\n\n\n@app.get(\"/items/\", tags=[\"items\"])\nasync def read_items():\n    return [{\"name\": \"Foo\", \"price\": 42}]\n\n\n@app.get(\"/users/\", tags=[\"users\"])\nasync def read_users():\n    return [{\"username\": \"johndoe\"}]\n</code></pre> <p>They will be added to the OpenAPI schema and used by the automatic documentation interfaces.</p>"}, {"location": "fastapi/#add-metadata-to-the-tags", "title": "Add metadata to the tags", "text": "<pre><code>tags_metadata = [\n    {\n        \"name\": \"users\",\n        \"description\": \"Operations with users. The **login** logic is also here.\",\n    },\n    {\n        \"name\": \"items\",\n        \"description\": \"Manage items. So _fancy_ they have their own docs.\",\n        \"externalDocs\": {\n            \"description\": \"Items external docs\",\n            \"url\": \"https://fastapi.tiangolo.com/\",\n        },\n    },\n]\n</code></pre> <p>app = FastAPI(openapi_tags=tags_metadata)</p>"}, {"location": "fastapi/#add-a-summary-and-description", "title": "Add a summary and description", "text": "<pre><code>@app.post(\"/items/\", response_model=Item, summary=\"Create an item\")\nasync def create_item(item: Item):\n    \"\"\"\n    Create an item with all the information:\n\n    - **name**: each item must have a name\n    - **description**: a long description\n    - **price**: required\n    - **tax**: if the item doesn't have tax, you can omit this\n    - **tags**: a set of unique tag strings for this item\n    \"\"\"\n    return item\n</code></pre>"}, {"location": "fastapi/#response-description", "title": "Response description", "text": "<pre><code>@app.post(\n    \"/items/\",\n    response_description=\"The created item\",\n)\nasync def create_item(item: Item):\n    return item\n</code></pre>"}, {"location": "fastapi/#deprecate-a-path-operation", "title": "Deprecate a path operation", "text": "<p>When you need to mark a path operation as deprecated, but without removing it</p> <pre><code>@app.get(\"/elements/\", tags=[\"items\"], deprecated=True)\nasync def read_elements():\n    return [{\"item_id\": \"Foo\"}]\n</code></pre>"}, {"location": "fastapi/#deploy-with-docker", "title": "Deploy with Docker.", "text": "<p>FastAPI has it's own optimized docker, which makes the deployment of your applications really easy.</p> <ul> <li>In your project directory create the <code>Dockerfile</code> file:</li> </ul> <pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n\nCOPY ./app /app\n</code></pre> <ul> <li> <p>Go to the project directory (in where your Dockerfile is, containing your app   directory).</p> </li> <li> <p>Build your FastAPI image:</p> </li> </ul> <pre><code>docker build -t myimage .\n</code></pre> <ul> <li>Run a container based on your image:</li> </ul> <pre><code>docker run -d --name mycontainer -p 80:80 myimage\n</code></pre> <p>Now you have an optimized FastAPI server in a Docker container. Auto-tuned for your current server (and number of CPU cores).</p>"}, {"location": "fastapi/#installing-dependencies", "title": "Installing dependencies", "text": "<p>If your program needs other dependencies, use the next dockerfile:</p> <pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n\nCOPY ./requirements.txt /app\nRUN pip install -r requirements.txt\n\nCOPY ./app /app\n</code></pre>"}, {"location": "fastapi/#other-project-structures", "title": "Other project structures", "text": "<p>The previous examples assume that you have followed the FastAPI project structure. If instead you've used mine your application will be defined in the <code>app</code> variable in the <code>src/program_name/entrypoints/api.py</code> file.</p> <p>To make things simpler make the <code>app</code> variable available on the root of your package, so you can do <code>from program_name import app</code> instead of <code>from program_name.entrypoints.api import app</code>. To do that we need to add <code>app</code> to the <code>__all__</code> internal python variable of the <code>__init__.py</code> file of our package.</p> <p>File: <code>src/program_name/__init__.py</code>:</p> <pre><code>from .entrypoints.ap\nimport app\n\n__all__: List[str] = ['app']\n</code></pre> <p>The image is configured through environmental variables</p> <p>So we will need to use:</p> <pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n\nENV MODULE_NAME=\"program_name\"\n\nCOPY ./src/program_name /app/program_name\n</code></pre>"}, {"location": "fastapi/#testing", "title": "Testing", "text": "<p>FastAPI gives a <code>TestClient</code> object borrowed from Starlette to do the integration tests on your application.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.testclient import TestClient\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def read_main():\n    return {\"msg\": \"Hello World\"}\n\n\n@pytest.fixture(name=\"client\")\ndef client_() -&gt; TestClient:\n    \"\"\"Configure FastAPI TestClient.\"\"\"\n    return TestClient(app)\n\n\ndef test_read_main(client: TestClient):\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert response.json() == {\"msg\": \"Hello World\"}\n</code></pre>"}, {"location": "fastapi/#test-a-post-request", "title": "Test a POST request", "text": "<pre><code>result = client.post(\n    \"/items/\",\n    headers={\"X-Token\": \"coneofsilence\"},\n    json={\"id\": \"foobar\", \"title\": \"Foo Bar\", \"description\": \"The Foo Barters\"},\n)\n</code></pre>"}, {"location": "fastapi/#inject-testing-configuration", "title": "Inject testing configuration", "text": "<p>If your application follows the application configuration section, injecting testing configuration is easy with dependency injection.</p> <p>Imagine you have a <code>db_tinydb</code> fixture that sets up the testing database:</p> <pre><code>@pytest.fixture(name=\"db_tinydb\")\ndef db_tinydb_(tmp_path: Path) -&gt; str:\n    \"\"\"Create an TinyDB database engine.\n\n    Returns:\n        database_url: Url used to connect to the database.\n    \"\"\"\n    tinydb_file_path = str(tmp_path / \"tinydb.db\")\n    return f\"tinydb:///{tinydb_file_path}\"\n</code></pre> <p>You can override the default <code>database_url</code> with:</p> <pre><code>@pytest.fixture(name=\"client\")\ndef client_(db_tinydb: str) -&gt; TestClient:\n    \"\"\"Configure FastAPI TestClient.\"\"\"\n\n    def override_settings() -&gt; Settings:\n        \"\"\"Inject the testing database in the application settings.\"\"\"\n        return Settings(database_url=db_tinydb)\n\n    app.dependency_overrides[get_settings] = override_settings\n    return TestClient(app)\n</code></pre>"}, {"location": "fastapi/#add-endpoints-only-on-testing-environment", "title": "Add endpoints only on testing environment", "text": "<p>Sometimes you want to have some API endpoints to populate the database for end to end testing the frontend. If your <code>app</code> config has the <code>environment</code> attribute, you could try to do:</p> <pre><code>app = FastAPI()\n\n\n@lru_cache()\ndef get_config() -&gt; Config:\n    \"\"\"Configure the program settings.\"\"\"\n    # no cover: the dependency are injected in the tests\n    log.info(\"Loading the config\")\n    return Config()  # pragma: no cover\n\n\nif get_config().environment == \"testing\":\n\n    @app.get(\"/seed\", status_code=201)\n    def seed_data(\n        repo: Repository = Depends(get_repo),\n        empty: bool = True,\n        num_articles: int = 3,\n        num_sources: int = 2,\n    ) -&gt; None:\n        \"\"\"Add seed data for the end to end tests.\n\n        Args:\n            repo: Repository to store the data.\n        \"\"\"\n        services.seed(\n            repo=repo, empty=empty, num_articles=num_articles, num_sources=num_sources\n        )\n        repo.close()\n</code></pre> <p>But the injection of the dependencies is only done inside the functions, so <code>get_config().environment</code> will always be the default value. I ended up doing that check inside the endpoint, which is not ideal.</p> <pre><code>@app.get(\"/seed\", status_code=201)\ndef seed_data(\n    config: Config = Depends(get_config),\n    repo: Repository = Depends(get_repo),\n    empty: bool = True,\n    num_articles: int = 3,\n    num_sources: int = 2,\n) -&gt; None:\n    \"\"\"Add seed data for the end to end tests.\n\n    Args:\n        repo: Repository to store the data.\n    \"\"\"\n    if config.environment != \"testing\":\n        repo.close()\n        raise HTTPException(status_code=404)\n    ...\n</code></pre>"}, {"location": "fastapi/#tips-and-tricks", "title": "Tips and tricks", "text": ""}, {"location": "fastapi/#create-redirections", "title": "Create redirections", "text": "<p>Returns an HTTP redirect. Uses a 307 status code (Temporary Redirect) by default.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import RedirectResponse\n\napp = FastAPI()\n\n\n@app.get(\"/typer\")\nasync def read_typer():\n    return RedirectResponse(\"https://typer.tiangolo.com\")\n</code></pre>"}, {"location": "fastapi/#test-that-your-application-works-locally", "title": "Test that your application works locally", "text": "<p>Once you have your application built and tested, everything should work right? well, sometimes it don't. If you need to use <code>pdb</code> to debug what's going on, you can't use the docker as you won't be able to interact with the debugger.</p> <p>Instead, launch an uvicorn application directly with:</p> <pre><code>uvicorn program_name:app --reload\n</code></pre> <p>Note: The command is assuming that your <code>app</code> is available at the root of your package, look at the deploy section if you feel lost.</p>"}, {"location": "fastapi/#resolve-the-307-error", "title": "Resolve the 307 error", "text": "<p>Probably you've introduced an ending <code>/</code> to the endpoint, so instead of asking for <code>/my/endpoint</code> you tried to do <code>/my/endpoint/</code>.</p>"}, {"location": "fastapi/#resolve-the-409-error", "title": "Resolve the 409 error", "text": "<p>Probably an exception was raised in the backend, use <code>pdb</code> to follow the trace and catch where it happened.</p>"}, {"location": "fastapi/#resolve-the-422-error", "title": "Resolve the 422 error", "text": "<p>You're probably passing the wrong arguments to the POST request, to solve it see the <code>text</code> attribute of the result. For example:</p> <pre><code># client: TestClient\nresult = client.post(\n    \"/source/add\",\n    json={\"body\": body},\n)\n\nresult.text\n# '{\"detail\":[{\"loc\":[\"query\",\"url\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}'\n</code></pre> <p>The error is telling us that the required <code>url</code> parameter is missing.</p>"}, {"location": "fastapi/#logging", "title": "Logging", "text": "<p>By default the application log messages are not shown in the uvicorn log, you need to add the next lines to the file where your app is defined:</p> <p>File: <code>src/program_name/entrypoints/api.py</code>:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.logger import logger\nimport logging\n\nlog = logging.getLogger(\"gunicorn.error\")\nlogger.handlers = log.handlers\nif __name__ != \"main\":\n    logger.setLevel(log.level)\nelse:\n    logger.setLevel(logging.DEBUG)\n\napp = FastAPI()\n\n# rest of the application...\n</code></pre>"}, {"location": "fastapi/#logging-to-sentry", "title": "Logging to Sentry", "text": "<p>FastAPI can integrate with Sentry or similar application loggers through the ASGI middleware.</p>"}, {"location": "fastapi/#run-a-fastapi-server-in-the-background-for-testing-purposes", "title": "Run a FastAPI server in the background for testing purposes", "text": "<p>Sometimes you want to launch a web server with a simple API to test a program that can't use the testing client. First define the API to launch with:</p> <p>File: <code>tests/api_server.py</code>:</p> <pre><code>from fastapi import FastAPI, HTTPException\n\napp = FastAPI()\n\n\n@app.get(\"/existent\")\nasync def existent():\n    return {\"msg\": \"exists!\"}\n\n\n@app.get(\"/inexistent\")\nasync def inexistent():\n    raise HTTPException(status_code=404, detail=\"It doesn't exist\")\n</code></pre> <p>Then create the fixture:</p> <p>File: <code>tests/conftest.py</code>:</p> <pre><code>from multiprocessing import Process\n\nfrom typing import Generator\nimport pytest\nimport uvicorn\n\nfrom .api_server import app\n\n\ndef run_server() -&gt; None:\n    \"\"\"Command to run the fake api server.\"\"\"\n    uvicorn.run(app)\n\n\n@pytest.fixture()\ndef _server() -&gt; Generator[None, None, None]:\n    \"\"\"Start the fake api server.\"\"\"\n    proc = Process(target=run_server, args=(), daemon=True)\n    proc.start()\n    yield\n    proc.kill()  # Cleanup after test\n</code></pre> <p>Now you can use the <code>server: None</code> fixture in your tests and run your queries against <code>http://localhost:8000</code>.</p>"}, {"location": "fastapi/#interesting-features-to-explore", "title": "Interesting features to explore", "text": "<ul> <li>Structure big applications.</li> <li>Dependency injection.</li> <li>Running background tasks after the request is finished.</li> <li>Return a different response model.</li> <li>Upload files.</li> <li>Set authentication.</li> <li>Host behind a proxy.</li> <li>Static files.</li> </ul>"}, {"location": "fastapi/#issues", "title": "Issues", "text": "<ul> <li>FastAPI does not log messages:   update <code>pyscrobbler</code> and any other maintained applications and remove the   snippet defined in the logging section.</li> </ul>"}, {"location": "fastapi/#references", "title": "References", "text": "<ul> <li> <p>Docs</p> </li> <li> <p>Git</p> </li> <li> <p>Awesome FastAPI</p> </li> <li> <p>Testdriven.io course: suggested   by the developer.</p> </li> </ul>"}, {"location": "ferdium/", "title": "Ferdium", "text": "<p>Ferdium is a desktop application to have all your services in one place. It's similar to Rambox, Franz or Ferdi only that it's maintained by the community and respects your privacy.</p>"}, {"location": "ferdium/#installation", "title": "Installation", "text": "<p>Download the deb package and run</p> <pre><code>sudo dpkg -i /path/to/your/file.deb\n</code></pre>"}, {"location": "ferdium/#security", "title": "Security", "text": "<p>In terms of security the Ferdium master password lock will only prevent an attacker from accessing your passwords if it has very few time to do the attack. They encrypt the password and save it in the config file along with a property <code>lockingFeatureEnabled</code> which is set to <code>true</code> when you activate this feature. Nevertheless if an attacker were to change this value to <code>false</code>, then they'll be able to access your Ferdium instance.</p> <p>Therefore I think that it's better to rely on locking your computer when leaving it and encrypting your hard drive. Adding the master password will only make the life harder for you for no substantial increase in security.</p> <p>Keep in mind that Ferdium stores the cookies to automatically log in the sites and that the information is accessible by an attacker that has access to your device. So only add services that are not critical to you.</p>"}, {"location": "ferdium/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "ffmpeg/", "title": "ffmpeg", "text": "<p>ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video</p> <p>You can run <code>ffmpeg -formats</code> to get a list of every format that is supported.</p>"}, {"location": "ffmpeg/#cut", "title": "Cut", "text": ""}, {"location": "ffmpeg/#cut-video-file-into-a-shorter-clip", "title": "Cut video file into a shorter clip", "text": "<p>You can use the time offset parameter <code>-ss</code> to specify the start time stamp in <code>HH:MM:SS.ms</code> format while the <code>-t</code> parameter is for specifying the actual duration of the clip in seconds.</p> <pre><code>ffmpeg -i input.mp4 -ss 00:00:50.0 -codec copy -t 20 output.mp4\n</code></pre>"}, {"location": "ffmpeg/#split-a-video-into-multiple-parts", "title": "Split a video into multiple parts", "text": "<p>The next command will split the source video into 2 parts. One ending at 50s from the start and the other beginning at 50s and ending at the end of the input video.</p> <pre><code>ffmpeg -i video.mp4 -t 00:00:50 -c copy small-1.mp4 -ss 00:00:50 -codec copy small-2.mp4\n</code></pre>"}, {"location": "ffmpeg/#crop-an-audio-file", "title": "Crop an audio file", "text": "<p>To create a 30 second audio file starting at 90 seconds from the original audio file without transcoding use:</p> <pre><code>ffmpeg -ss 00:01:30 -t 30 -acodec copy -i inputfile.mp3 outputfile.mp3\n</code></pre>"}, {"location": "ffmpeg/#join", "title": "Join", "text": ""}, {"location": "ffmpeg/#join-concatenate-video-files", "title": "Join (concatenate) video files", "text": "<p>If you have multiple audio or video files encoded with the same codecs, you can join them into a single file. Create a input file with a list of all source files that you wish to concatenate and then run this command.</p> <p>Create first the file list with a Bash for loop:</p> <pre><code>for f in ./*.wav; do echo \"file '$f'\" &gt;&gt; mylist.txt; done\n</code></pre> <p>Then convert</p> <pre><code>ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4\n</code></pre>"}, {"location": "ffmpeg/#merge-an-audio-and-video-file", "title": "Merge an audio and video file", "text": "<p>You can also specify the -shortest switch to finish the encoding when the shortest clip ends.</p> <pre><code>ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental output.mp4\nffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental -shortest output.mp4\n</code></pre>"}, {"location": "ffmpeg/#mute", "title": "Mute", "text": "<p>Use the <code>-an</code> parameter to disable the audio portion of a video stream.</p> <pre><code>ffmpeg -i video.mp4 -an mute-video.mp4\n</code></pre>"}, {"location": "ffmpeg/#convert", "title": "Convert", "text": ""}, {"location": "ffmpeg/#convert-video-from-one-format-to-another", "title": "Convert video from one format to another", "text": "<p>You can use the <code>-vcodec</code> parameter to specify the encoding format to be used for the output video. Encoding a video takes time but you can speed up the process by forcing a preset though it would degrade the quality of the output video.</p> <pre><code>ffmpeg -i youtube.flv -c:v libx264 filename.mp4\nffmpeg -i video.wmv -c:v libx264 -preset ultrafast video.mp4\n</code></pre>"}, {"location": "ffmpeg/#convert-a-x265-file-into-x264", "title": "Convert a x265 file into x264", "text": "<pre><code>for i in *.mkv ; do\n    ffmpeg -i \"$i\" -bsf:v h264_mp4toannexb -vcodec libx264 \"$i.x264.mkv\"\ndone\n</code></pre> <ul> <li><code>ffmpeg -i \"$i\"</code>: Executes the program ffmpeg and calls for files to be     processed.</li> <li><code>-bsf:v</code>: Activates the video bit stream filter to be used.</li> <li> <p><code>h264_mp4toannexb</code>: Is the bit stream filter that is activated.     Convert an H.264 bitstream from length prefixed mode to start code prefixed     mode (as defined in the Annex B of the ITU-T H.264 specification).</p> <p>This is required by some streaming formats, typically the MPEG-2 transport stream format (mpegts) processing MKV h.264 (currently)requires this, if is not included you will get an error in the terminal window instructing you to use it. * <code>-vcodec libx264</code> This tells ffmpeg to encode the output to H.264. * <code>\"$i.ts\"</code> Saves the output to .ts format, this is useful so as not to overwrite your source files.</p> </li> </ul>"}, {"location": "ffmpeg/#convert-vob-to-mkv", "title": "Convert VOB to mkv", "text": "<ul> <li> <p>Unify your VOBs     <pre><code>cat *.VOB &gt; output.vob\n</code></pre></p> </li> <li> <p>Identify the streams</p> <pre><code>ffmpeg -analyzeduration 100M -probesize 100M -i output.vob\n</code></pre> <p>Select the streams that you are interested in, imagine that is 1, 3, 4, 5 and 6.</p> </li> <li> <p>Encoding</p> <pre><code>ffmpeg \\\n  -analyzeduration 100M -probesize 100M \\\n  -i output.vob \\\n  -map 0:1 -map 0:3 -map 0:4 -map 0:5 -map 0:6 \\\n  -metadata:s:a:0 language=ita -metadata:s:a:0 title=\"Italian stereo\" \\\n  -metadata:s:a:1 language=eng -metadata:s:a:1 title=\"English stereo\" \\\n  -metadata:s:s:0 language=ita -metadata:s:s:0 title=\"Italian\" \\\n  -metadata:s:s:1 language=eng -metadata:s:s:1 title=\"English\" \\\n  -codec:v libx264 -crf 21 \\\n  -codec:a libmp3lame -qscale:a 2 \\\n  -codec:s copy \\\n  output.mkv\n</code></pre> </li> </ul>"}, {"location": "ffmpeg/#convert-a-video-into-animated-gif", "title": "Convert a video into animated GIF", "text": "<pre><code>ffmpeg -ss 30 -t 3 -i input.mp4 -vf \"fps=10,scale=480:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -loop 0 output.gif\n</code></pre> <ul> <li>This example will skip the first 30 seconds (-ss 30) of the input and create     a 3 second output (-t 3).</li> <li>fps filter sets the frame rate. A rate of 10 frames per second is used in the     example.</li> <li>Scale filter will resize the output to 320 pixels wide and automatically     determine the height while preserving the aspect ratio. The lanczos scaling     algorithm is used in this example.</li> <li>Palettegen and paletteuse filters will generate and use a custom palette     generated from your input. These filters have many options, so refer to the     links for a list of all available options and values. Also see the Advanced     options section below.</li> <li>Split filter will allow everything to be done in one command and avoids having     to create a temporary PNG file of the palette.</li> <li>Control looping with -loop output option but the values are confusing. A value     of 0 is infinite looping, -1 is no looping, and 1 will loop once meaning it     will play twice. So a value of 10 will cause the GIF to play 11 times.</li> </ul>"}, {"location": "ffmpeg/#convert-video-into-images", "title": "Convert video into images", "text": "<p>You can use FFmpeg to automatically extract image frames from a video every <code>n</code> seconds and the images are saved in a sequence. This command saves image frame after every 4 seconds.</p> <pre><code>ffmpeg -i movie.mp4 -r 0.25 frames_%04d.png\n</code></pre>"}, {"location": "ffmpeg/#convert-a-single-image-into-a-video", "title": "Convert a single image into a video", "text": "<p>Use the <code>-t</code> parameter to specify the duration of the video.</p> <pre><code>ffmpeg -loop 1 -i image.png -c:v libx264 -t 30 -pix_fmt yuv420p video.mp4\n</code></pre>"}, {"location": "ffmpeg/#convert-opus-or-wav-to-mp3", "title": "Convert opus or wav to mp3", "text": "<pre><code>ffmpeg -i input.wav -vn -ar 44100 -ac 2 -b:a 320k output.mp3\n</code></pre> <ul> <li><code>-i</code>: input file.</li> <li><code>-vn</code>: Disable video, to make sure no video (including album cover image) is     included if the source would be a video file.</li> <li><code>-ar</code>: Set the audio sampling frequency. For output streams it is set by     default to the frequency of the corresponding input stream. For input     streams this option only makes sense for audio grabbing devices and raw     demuxers and is mapped to the corresponding demuxer options.</li> <li><code>-ac</code>: Set the number of audio channels. For output streams it is set by     default to the number of input audio channels. For input streams this option     only makes sense for audio grabbing devices and raw demuxers and is mapped     to the corresponding demuxer options. So used here to make sure it is stereo     (2 channels).</li> <li><code>-b:a</code>: Converts the audio bitrate to be exact 320kbit per second.</li> </ul>"}, {"location": "ffmpeg/#extract", "title": "Extract", "text": ""}, {"location": "ffmpeg/#extract-the-audio-from-video", "title": "Extract the audio from video", "text": "<p>The <code>-vn</code> switch extracts the audio portion from a video and we are using the <code>-ab</code> switch to save the audio as a 256kbps MP3 audio file.</p> <pre><code>ffmpeg -i video.mp4 -vn -ab 256 audio.mp3\n</code></pre>"}, {"location": "ffmpeg/#extract-image-frames-from-a-video", "title": "Extract image frames from a video", "text": "<p>This command will extract the video frame at the 15s mark and saves it as a 800px wide JPEG image. You can also use the -s switch (like -s 400\u00d7300) to specify the exact dimensions of the image file though it will probably create a stretched image if the image size doesn\u2019t follow the aspect ratio of the original video file.</p> <pre><code>ffmpeg -ss 00:00:15 -i video.mp4 -vf scale=800:-1 -vframes 1 image.jpg\n</code></pre>"}, {"location": "ffmpeg/#extract-metadata-of-video", "title": "Extract metadata of video", "text": "<pre><code>ffprobe {{ file }}\n</code></pre>"}, {"location": "ffmpeg/#resize", "title": "Resize", "text": ""}, {"location": "ffmpeg/#resize-a-video", "title": "Resize a video", "text": ""}, {"location": "ffmpeg/#change-the-constat-rate-factor", "title": "Change the Constat Rate Factor", "text": "<p>Setting the Constant Rate Factor, which lowers the average bit rate, but retains better quality. Vary the CRF between around 18 and 24 \u2014 the lower, the higher the bitrate.</p> <pre><code>ffmpeg -i input.mp4 -vcodec libx265 -crf 24 output.mp4\n</code></pre> <p>Change the codec as needed - libx264 may be available if libx265 is not, at the cost of a slightly larger resultant file size.</p>"}, {"location": "ffmpeg/#change-video-resolution", "title": "Change video resolution", "text": "<p>Use the size <code>-s</code> switch with ffmpeg to resize a video while maintaining the aspect ratio.</p> <pre><code>ffmpeg -i input.mp4 -s 480x320 -c:a copy output.mp4\n</code></pre>"}, {"location": "ffmpeg/#presentation", "title": "Presentation", "text": ""}, {"location": "ffmpeg/#create-video-slideshow-from-images", "title": "Create video slideshow from images", "text": "<p>This command creates a video slideshow using a series of images that are named as <code>img001.png</code>, <code>img002.png</code>, etc. Each image will have a duration of 5 seconds (-r \u2155).</p> <pre><code>ffmpeg -r 1/5 -i img%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p slideshow.mp4\n</code></pre>"}, {"location": "ffmpeg/#add-a-poster-image-to-audio", "title": "Add a poster image to audio", "text": "<p>You can add a cover image to an audio file and the length of the output video will be the same as that of the input audio stream. This may come handy for uploading MP3s to YouTube.</p> <pre><code>ffmpeg -loop 1 -i image.jpg -i audio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4\n</code></pre>"}, {"location": "ffmpeg/#add-subtitles-to-a-movie", "title": "Add subtitles to a movie", "text": "<p>This will take the subtitles from the <code>.srt</code> file. FFmpeg can decode most common subtitle formats.</p> <pre><code>ffmpeg -i movie.mp4 -i subtitles.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mkv\n</code></pre>"}, {"location": "ffmpeg/#change-the-audio-volume", "title": "Change the audio volume", "text": "<p>You can use the volume filter to alter the volume of a media file using FFmpeg. This command will half the volume of the audio file.</p> <pre><code>ffmpeg -i input.wav -af 'volume=0.5' output.wav\n</code></pre>"}, {"location": "ffmpeg/#rotate-a-video", "title": "Rotate a video", "text": "<p>This command will rotate a video clip 90\u00b0 clockwise. You can set transpose to 2 to rotate the video 90\u00b0 anti-clockwise.</p> <pre><code>ffmpeg -i input.mp4 -filter:v 'transpose=1' rotated-video.mp4\n</code></pre> <p>This will rotate the video 180\u00b0 counter-clockwise.</p> <pre><code>ffmpeg -i input.mp4 -filter:v 'transpose=2,transpose=2' rotated-video.mp4\n</code></pre>"}, {"location": "ffmpeg/#speed-up-or-slow-down-the-video", "title": "Speed up or Slow down the video", "text": "<p>You can change the speed of your video using the setpts (set presentation time stamp) filter of FFmpeg. This command will make the video 8x (\u215b) faster or use setpts=4*PTS to make the video 4x slower.</p> <pre><code>ffmpeg -i input.mp4 -filter:v \"setpts=0.125*PTS\" output.mp4\n</code></pre>"}, {"location": "ffmpeg/#speed-up-or-slow-down-the-audio", "title": "Speed up or Slow down the audio", "text": "<p>For changing the speed of audio, use the atempo audio filter. This command will double the speed of audio. You can use any value between 0.5 and 2.0 for audio.</p> <p><code>bash ffmpeg -i input.mkv -filter:a \"atempo=2.0\" -vn output.mkv</code></p> <p>Stack Exchange has a good overview to get you started with FFmpeg. You should also check out the official documentation at ffmpeg.org or the wiki at trac.ffmpeg.org to know about all the possible things you can do with FFmpeg.</p>"}, {"location": "ffmpeg/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "finnix/", "title": "Finnix", "text": "<p>Finnix is a live Linux distribution specialized in the recovery, maintenance, testing of systems.</p>"}, {"location": "finnix/#installation", "title": "Installation", "text": "<ul> <li>Download the latest version from the web</li> <li>Load it into a usb:    <pre><code>dd if=/path/to/your/finnix.iso of=/dev/path/to/your/disk\n</code></pre></li> </ul> <p>!!! warning \"Be sure that the <code>/dev/path/to/your/disk</code> is the one you want to overwrite, you may end up fucking your device hard drive instead!\"</p>"}, {"location": "finnix/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "fitness_band/", "title": "Fitness tracker", "text": "<p>Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness-related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer.</p> <p>As with anything that can be bought, I usually first try a cheap model to see if I need the advanced features that the expensive ones offer. After a quick model review, I went for the Amazfit band 5.</p> <p>I've now discovered wasp-os an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge, Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing.</p> <p>Currently it support the following devices:</p> <ul> <li>Colmi P8</li> <li>Senbono K9</li> <li>Pine64 PineTime</li> </ul> <p>Pinetime seems to be a work in progress, Colmi P8 looks awesome, and the Senbono K9 looks good too but wasp-os is lacking touch screen support.</p> <p>So if I had to choose now, I'd try the Colmi P8, the only thing that I'd miss is the possible voice assistant support. They say that you can take one for 18$ in aliexpress.</p>"}, {"location": "flakeheaven/", "title": "Flakeheaven", "text": "<p>Flakeheaven is a Flake8 wrapper to make it cool.</p> <p>Some of it's features are:</p> <ul> <li>Lint md, rst, ipynb, and     more.</li> <li>Shareable and remote     configs.</li> <li>Legacy-friendly:     ability to get report only about new errors.</li> <li>Caching for much better performance.</li> <li>Use only specified     plugins, not     everything installed.</li> <li>Make output beautiful.</li> <li>pyproject.toml support.</li> <li>Check that all required plugins are     installed.</li> <li>Syntax highlighting in messages and code     snippets.</li> <li>PyLint integration.</li> <li>Remove unused noqa.</li> <li>Powerful GitLab support.</li> <li>Codes management:<ul> <li>Manage codes per plugin.</li> <li>Enable and disable plugins and codes by wildcard.</li> <li>Show codes for installed plugins.</li> <li>Show all messages and codes for a plugin.</li> <li>Allow codes intersection for different plugins.</li> </ul> </li> </ul> <p>You can use this cookiecutter template to create a python project with <code>flakeheaven</code> already configured.</p>"}, {"location": "flakeheaven/#installation", "title": "Installation", "text": "<pre><code>pip install flakeheaven\n</code></pre>"}, {"location": "flakeheaven/#configuration", "title": "Configuration", "text": "<p>Flakeheaven can be configured in pyproject.toml. You can specify any Flake8 options and Flakeheaven-specific parameters.</p>"}, {"location": "flakeheaven/#plugins", "title": "Plugins", "text": "<p>In <code>pyproject.toml</code> you can specify <code>[tool.flakeheaven.plugins]</code> table. It's a list of flake8 plugins and associated to them rules.</p> <p>Key can be exact plugin name or wildcard template. For example <code>\"flake8-commas\"</code> or <code>\"flake8-*\"</code>. Flakeheaven will choose the longest match for every plugin if possible. In the previous example, <code>flake8-commas</code> will match to the first pattern, <code>flake8-bandit</code> and <code>flake8-bugbear</code> to the second, and <code>pycodestyle</code> will not match to any pattern.</p> <p>Value is a list of templates for error codes for this plugin. First symbol in every template must be <code>+</code> (include) or <code>-</code> (exclude). The latest matched pattern wins. For example, <code>[\"+*\", \"-F*\", \"-E30?\", \"-E401\"]</code> means \"Include everything except all checks that starts with <code>F</code>, check from <code>E301</code> to <code>E310</code>, and <code>E401</code>\".</p> <p>Example: pyproject.toml</p> <pre><code>[tool.flakeheaven]\n# specify any flake8 options. For example, exclude \"example.py\":\nexclude = [\"example.py\"]\n# make output nice\nformat = \"grouped\"\n# don't limit yourself\nmax_line_length = 120\n# show line of source code in output\nshow_source = true\n\n# list of plugins and rules for them\n[tool.flakeheaven.plugins]\n# include everything in pyflakes except F401\npyflakes = [\"+*\", \"-F401\"]\n# enable only codes from S100 to S199\nflake8-bandit = [\"-*\", \"+S1??\"]\n# enable everything that starts from `flake8-`\n\"flake8-*\" = [\"+*\"]\n# explicitly disable plugin\nflake8-docstrings = [\"-*\"]\n\n# disable some checks for tests\n[tool.flakeheaven.exceptions.\"tests/\"]\npycodestyle = [\"-F401\"]     # disable a check\npyflakes = [\"-*\"]           # disable a plugin\n\n# do not disable `pyflakes` for one file in tests\n[tool.flakeheaven.exceptions.\"tests/test_example.py\"]\npyflakes = [\"+*\"]           # enable a plugin\n</code></pre> <p>Check a complete list of flake8 extensions.</p> <ul> <li>flake8-bugbear: Finding likely bugs     and design problems in your program. Contains warnings that don't belong in     pyflakes and pycodestyle.</li> <li>flake8-fixme: Check for FIXME,     TODO and other temporary developer notes.</li> <li>flake8-debugger: Check for     <code>pdb</code> or <code>idbp</code> imports and set traces.</li> <li>flake8-mutable: Checks for     mutable default     arguments anti-pattern.</li> <li>flake8-pytest: Check for uses of     Django-style assert-statements in tests. So no more <code>self.assertEqual(a, b)</code>,     but instead <code>assert a == b</code>.</li> <li>flake8-pytest-style: Checks     common style issues or inconsistencies with pytest-based tests.</li> <li>flake8-simplify: Helps you     to simplify code.</li> <li>flake8-variables-names:     Helps to make more readable variables names.</li> <li>pep8-naming: Check your code against     PEP 8 naming conventions.</li> <li>flake8-expression-complexity:     Check expression complexity.</li> <li>flake8-use-fstring:     Checks you're using f-strings.</li> <li>flake8-docstrings: adds an     extension for the fantastic     pydocstyle tool to Flake8.</li> <li>flake8-markdown: lints     GitHub-style Python code blocks in Markdown files using flake8.</li> <li>pylint is a Python static code analysis     tool which looks for programming errors, helps enforcing a coding standard,     sniffs for code smells and offers simple refactoring suggestions.</li> <li>dlint: Encourage best coding practices     and helping ensure Python code is secure.</li> <li>flake8-aaa: Checks Python tests     follow the Arrange-Act-Assert     pattern.</li> <li>flake8-annotations-complexity:     Report on too complex type annotations.</li> <li>flake8-annotations: Detects the     absence of PEP 3107-style function annotations and PEP 484-style type     comments.</li> <li>flake8-typing-imports:     Checks that typing imports are properly guarded.</li> <li>flake8-comprehensions:     Help you write better list/set/dict comprehensions.</li> <li>flake8-eradicate:  find     commented out (or so called \"dead\") code.</li> </ul>"}, {"location": "flakeheaven/#usage", "title": "Usage", "text": "<p>When using Flakeheaven, I frequently use the following commands:</p> <code>flakeheaven lint</code> Runs the linter, similar to the flake8 command. <code>flakeheaven plugins</code> Lists all the plugins used, and their configuration status. <code>flakeheaven missed</code> Shows any plugins that are in the configuration but not installed properly. <code>flakeheaven code S322</code> (or any other code) Shows the explanation for that specific warning code. <code>flakeheaven yesqa</code> Removes unused codes from <code># noqa</code> and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice."}, {"location": "flakeheaven/#integrations", "title": "Integrations", "text": "<p>Flakeheaven checks can be run in:</p> <ul> <li> <p>In Vim though the ALE plugin.</p> </li> <li> <p>Through a pre-commit:</p> <pre><code>  - repo: https://github.com/flakeheaven/flakeheaven\n    rev: master\n    hooks:\n      - name: Run flakeheaven static analysis tool\n        id: flakeheaven\n</code></pre> </li> <li> <p>In the CI:     <pre><code>  - name: Test linters\n    run: make lint\n</code></pre></p> <p>Assuming you're using a Makefile like the one in my cookiecutter-python-project.</p> </li> </ul>"}, {"location": "flakeheaven/#issues", "title": "Issues", "text": "<ul> <li>ImportError: cannot import name 'MergedConfigParser' from     'flake8.options.config':     remove the dependency pin in cookiecutter template and propagate to all     projects.</li> <li> <p>'Namespace' object has no attribute 'extended_default_ignore'     error:     Until it's fixed either use a version below or equal to 3.9.0, or add to     your <code>pyproject.toml</code>:</p> <pre><code>[tool.flakeheaven]\nextended_default_ignore=[]  # add this\n</code></pre> <p>Once it's fixed, remove the patch from the maintained projects.</p> </li> </ul>"}, {"location": "flakeheaven/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "flakeheaven/#namespace-object-has-no-attribute", "title": "['Namespace' object has no attribute", "text": "<p>'extended_default_ignore'](https://githubmemory.com/repo/flakeheaven/flakeheaven/issues/10)</p> <p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.flakeheaven]\nextended_default_ignore=[]\n</code></pre>"}, {"location": "flakeheaven/#references", "title": "References", "text": "<ul> <li>Git</li> <li> <p>Docs</p> </li> <li> <p>Using Flake8 and pyproject.toml with Flakeheaven article by Jonathan     Bowman</p> </li> </ul>"}, {"location": "food_management/", "title": "Food management", "text": "<p>As humans diet is an important factor in our health, we need to eat daily around three times a day, as such, each week we need to invest time into managing how to get food in front of us. Tasks like thinking what do you want to eat, buying the ingredients and cooking them make use a non negligible amount of time. Also something to keep in mind, is that eating is one of the great pleasures in our lives, so doing it poorly is a waste. The last part of the equation is that to eat good you either need time or money.</p> <p>This article explores my thoughts and findings on how to optimize the use of time, money and mental load in food management while keeping the desired level of quality to enjoy each meal, being healthy and following the principles of ecology and sustainability. I'm no expert at all on either of these topics. I'm learning and making my mind while writing these lines.</p>"}, {"location": "food_management/#choosing-your-diet", "title": "Choosing your diet", "text": ""}, {"location": "food_management/#the-broad-picture-of-diet", "title": "The broad picture of diet", "text": "<p>Seasonal, vegetarian, proximity, responsible</p>"}, {"location": "food_management/#the-details-of-diet", "title": "The details of diet", "text": "<p>How to choose what to eat this week</p>"}, {"location": "food_management/#buying-the-products", "title": "Buying the products", "text": "<p>Check grocy management for more details.</p>"}, {"location": "food_management/#cooking", "title": "Cooking", "text": "<p>Week batch cooking, yearly batch cooking</p>"}, {"location": "free_knowledge/", "title": "Free Knowledge", "text": "<p>One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub, bookwarrior of Library Genesis, Aaron Swartz, and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science.</p> <p>Some days ago, a post appeared on reddit to rescue Sci-Hub by increasing the seeders of the 850 scihub torrents. The plan is to follow the steps done last year to move Libgen to IPFS to make it more difficult for the states to bring down this marvelous collection.</p> <p>A good way to start is to look at the most ill torrents and fix their state. If you follow this path, take care of IP leaking, they're surely monitoring who's sharing.</p> <p>Another way to contribute is by following the guidelines of freeread.org and contribute to the IPFS free library. Beware though, the guidelines don't explain how to install IPFS behind a VPN or Tor. This could be contributed to the site.</p> <p>Something that is needed is a command line tool that reads the list of ill torrents, and downloads the torrents that have a low number of seeders and DHT peers. The number of torrents to download could be limited by the amount the user wants to share. A second version could have an interaction with the torrent client so that when a torrent is no longer ill, it's automatically replaced with one that is.</p>"}, {"location": "free_knowledge/#references", "title": "References", "text": "<ul> <li>FreeRead.org</li> <li>Libgen reddit</li> <li>Sci-Hub reddit</li> <li>DataHoarder reddit</li> </ul>"}, {"location": "frontend_development/", "title": "Frontend Development", "text": "<p>I've recently started learning how to make web frontends, two years ago I learned a bit of HTML, CSS, Javascript and React, but it didn't stick that much.</p> <p>This time I'm full in with Vue which in my opinion is by far prettier than React.</p>"}, {"location": "frontend_development/#newbie-tips", "title": "Newbie tips", "text": "<p>I feel completely lost xD, I don't know even how to search well what I need, it's like going back to programming 101. Funnily though, it's bringing me closer to the people I mentor on Python, I'm getting frustrated with similar things that they do, those things that you don't see when you already are proficient in a language, so here are some tips.</p>"}, {"location": "frontend_development/#dont-resize-your-browser-window", "title": "Don't resize your browser window", "text": "<p>As I use i3wm, I've caught myself resizing the browser by adding terminals above and besides the browser to see how does the site react to different screen sizes. I needed the facepalm of a work colleague, which kindly suggested to spawn the Developer Tools and move that around. If you need to resize in the other direction, change the position of the Developer tools and grow it in that direction.</p> <p>A better feature yet is to use the  Responsive Design mode, which lets you select screen sizes of existent devices, and it's easy to resize the screen.</p>"}, {"location": "frontend_development/#your-frontend-probably-doesnt-talk-to-your-backend", "title": "Your frontend probably doesn't talk to your backend", "text": "<p>If you're using Vue or a similar framework, your frontend is just a webserver (like nginx) that has some html, js and css static files whose only purpose is to serve those static files to the user. It is the user's browser the one that does all the queries, even to the backend.</p> <p>Imagine that we have a frontend application that uses a backend API behind the scenes. In the front application you'll do the queries on <code>/api</code> and depending on the environment two different things will happen:</p> <ul> <li> <p>In your development environment, when you run the development server to     manually interact with it with the browser, you configure it so that     whatever request you do to <code>/api</code> is redirected to the backend endpoint,     which usually is listening on another port on <code>localhost</code>.</p> <p>If you are doing unit or integration tests, you'll probably use your test runner to intercept those calls and mock the result.</p> <p>If you are doing E2E tests, your test runner will probably understand your development configuration and forward the requests to the backend service.</p> </li> <li> <p>In production you'll have an SSL proxy, for example linuxserver's     swag, that will forward <code>/api</code>     to the backend and the rest to the frontend.</p> </li> </ul>"}, {"location": "frontend_development/#ux-design", "title": "UX design", "text": "<p>The most popular tool out there is <code>Figma</code> but it's closed sourced, the alternative (quite popular in github) is <code>penpot</code>.</p>"}, {"location": "frontend_development/#testing", "title": "Testing", "text": ""}, {"location": "frontend_development/#write-testable-code", "title": "Write testable code", "text": "<p>Every test you write will include selectors for elements. To save yourself a lot of headaches, you should write selectors that are resilient to changes.</p> <p>Oftentimes we see users run into problems targeting their elements because:</p> <ul> <li>Your application may use dynamic classes or ID's that change.</li> <li>Your selectors break from development changes to CSS styles or JS behavior.</li> </ul> <p>Luckily, it is possible to avoid both of these problems.</p> <ul> <li>Don't target elements based on CSS attributes such as: id, class, tag.</li> <li>Don't target elements that may change their <code>textContent</code>.</li> <li>Add <code>data-*</code> attributes to make it easier to target elements.</li> </ul> <p>Given a button that we want to interact with:</p> <pre><code>&lt;button\n  id=\"main\"\n  class=\"btn btn-large\"\n  name=\"submission\"\n  role=\"button\"\n  data-cy=\"submit\"\n&gt;\n  Submit\n&lt;/button&gt;\n</code></pre> <p>Let's investigate how we could target it: | Selector                         | Recommended | Notes                                              | | cy.get('button').click()         | Never       | Worst - too generic, no context.                   | | cy.get('.btn.btn-large').click() | Never       | Bad. Coupled to styling. Highly subject to change. | | cy.get('#main').click() | Sparingly | Better. But still coupled to styling or JS event listeners. | | cy.get('[name=submission]').click() | Sparingly | Coupled to the name attribute which has HTML semantics. | | cy.contains('Submit').click() |   Depends |   Much better. But still coupled to text content that may change. | | cy.get('[data-cy=submit]').click() |  Always |    Best. Isolated from all changes. |</p>"}, {"location": "frontend_development/#conditional-testing", "title": "Conditional testing", "text": "<p>Conditional testing refers to the common programming pattern:</p> <pre><code>If X, then Y, else Z\n</code></pre> <p>Here are some examples:</p> <ul> <li>How do I do something different whether an element does or doesn't exist?</li> <li>My application does A/B testing, how do I account for that?</li> <li>My users receive a \"welcome wizard\", but existing ones don't. Can I always     close the wizard in case it's shown, and ignore it when it's not?</li> <li>I want to automatically find all  elements and based on which ones I find, I want to check that each link works. <p>The problem is - while first appearing simple, writing tests in this fashion often leads to flaky tests, random failures, and difficult to track down edge cases.</p> <p>Some interesting cases and their solutions:</p> <ul> <li>Welcome wizard</li> <li>A/B Campaign</li> </ul>"}, {"location": "frontend_learning/", "title": "Frontend learning", "text": "<p>This section is the particularization of the Development learning article for a frontend developer, in particular a Vue developer.</p>"}, {"location": "frontend_learning/#what-is-a-frontend-developer", "title": "What is a Frontend developer?", "text": "<p>A Front-End Developer is someone who creates websites and web applications. It's main responsibility is to create what the user sees.</p> <p>The basic languages for Front-End Development are HTML, CSS, and JavaScript. Nowadays writing interfaces with only the basic languages makes no sense as there are other languages and frameworks that make better and quicker solutions. One of them is Vue, which is the one I learnt, so the whole document will be focused on this path, nevertheless there are others popular ones like: Bootstrap, React, jQuery or Angular.</p> <p>The difference between Front-End and Back-End is that Front-End refers to how a web page looks, while back-end refers to how it works.</p>"}, {"location": "frontend_learning/#roadmap", "title": "Roadmap", "text": ""}, {"location": "frontend_learning/#first-steps", "title": "First steps", "text": ""}, {"location": "frontend_learning/#setup-your-development-environment", "title": "Setup your development environment", "text": ""}, {"location": "frontend_learning/#learn-the-basics", "title": "Learn the basics", "text": "<p>In order to write Vue code you first need to understand the foundations it's built upon, that means learning the basics of:</p> <ul> <li>HTML: For example following the W3     tutorial, at least until the     HTML Forms section.</li> <li>CSS: For example following the W3     tutorial until CSS Advanced.</li> <li>Javascript: For example using the W3     tutorial until JS Versions.</li> </ul> <p>If you follow other learning methods, make sure that they cover more less the same concepts.</p>"}, {"location": "fun/", "title": "Fun Stuff", "text": ""}, {"location": "fun/#coding", "title": "Coding", "text": "<p> (source)</p>"}, {"location": "gadgetbridge/", "title": "Gadgetbridge", "text": "<p>Gadgetbridge is an Android (4.4+) application which will allow you to use your Pebble, Mi Band, Amazfit Bip and HPlus device (and more) without the vendor's closed source application and without the need to create an account and transmit any of your data to the vendor's servers.</p> <p>It wont be ever be as good as the proprietary application, but it supports a good range of features, and supports a huge range of bands.</p>"}, {"location": "gadgetbridge/#data-extraction", "title": "Data extraction", "text": "<p>You can use the Data export or Data Auto export to get copy of your data.</p> <p>Here is an example of a Python program that post processes the data. Also is this post explaining how to reverse engineer the miband2 with this or this scripts. If you start the path of reverse engineering the Bluetooth protocol look at gadgetbridge guidelines.</p> <p>If you start to think on how to avoid the connection with an android phone and directly extract or interact from a linux device through python, I'd go with pybluez for the bluetooth interface, understand the band code of Gadgetbridge porting the logic to the python module, and reverse engineering the call you want to process. There isn't much in the internet following this approach, I've found an implementation for the Mi Band 4 though, which can be a good start.</p>"}, {"location": "gadgetbridge/#heartrate-measurement", "title": "Heartrate measurement", "text": "<p>Follow the official instructions.</p>"}, {"location": "gadgetbridge/#sleep", "title": "Sleep", "text": "<p>It looks that they don't yet support smart alarms.</p>"}, {"location": "gadgetbridge/#weather", "title": "Weather", "text": "<p>Follow the official instructions</p>"}, {"location": "gadgetbridge/#events", "title": "Events", "text": "<p>I haven't figured out yet how to let the events show in the \"events\" tab. Mobile calendar events show up as notifications, but you can't see the list of the next ones.</p> <p>For the Amazfit band 5, there is a bug that prevents events from showing in the reminders tab. Notifications work well though.</p>"}, {"location": "gadgetbridge/#setup", "title": "Setup", "text": "<p>In the case of the Amazfit band 5, we need to use the Huami server pairing:</p> <ul> <li>Install the Zepp application</li> <li>Create an account through the application.</li> <li>Pair your band and wait for the firmware update</li> <li>Use the python script to extract     the credentials<ul> <li><code>git clone https://github.com/argrento/huami-token.git</code></li> <li><code>pip install -r requirements.txt</code></li> <li>Run script with your credentials: <code>python huami_token.py --method amazfit --email youemail@example.com --password your_password --bt_keys</code></li> </ul> </li> <li>Do not unpair the band/watch from MiFit/Amazfit/Zepp app</li> <li>Kill or uninstall the MiFit/Amazfit/Zepp app</li> <li>Ensure GPS/location services are enabled</li> <li>The official instructions tell you to unpair the band/watch from your phone's     bluetooth but I didn't have to do it.</li> <li>Add the band in gadgetbridge.</li> <li>Under Auth key add your key.</li> </ul>"}, {"location": "gadgetbridge/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Home</li> <li>Issue tracker</li> <li>Blog, although the     RSS is not     working.</li> </ul>"}, {"location": "gajim/", "title": "Gajim", "text": "<p>Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO.</p>"}, {"location": "gajim/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install gajim gajim-omemo\n</code></pre> <p>Once you open it, you need to enable the plugin in the main program dropdown.</p> <p>The only problem I've encountered so far is that OMEMO is not enabled by default, they made a PR but closed it because it encountered some errors that was not able to solve. It's a crucial feature, so if you have some spare time and know a bit of Python please try to fix it!</p>"}, {"location": "gajim/#developing", "title": "Developing", "text": "<p>I've found the Developing section in the wiki to get started.</p>"}, {"location": "gajim/#issues", "title": "Issues", "text": "<ul> <li>Enable encryption by     default: Nothing to     do.</li> </ul>"}, {"location": "gajim/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "gettext/", "title": "Gettext", "text": "<p>Gettext is the defacto universal solution for internationalization (I18N) and localization (L10N), offering a set of tools that provides a framework to help other packages produce multi-lingual messages. It gives an opinionated way of how programs should be written to support translated message strings and a directory and file naming organisation for the messages that need to be translated.</p> <p>In regards to directory conventions, we need to have a place to put our localised translations based on the specified locale language. For example, let\u2019s say we need to support 2 languages English and Greek. Their language codes are <code>en</code> and <code>el</code> respectively.</p> <p>We can create a directory named <code>locales</code> and inside we need to create directories for each language code and each folder will contain another directory named each <code>LC_MESSAGES</code>  with one or multiple <code>.po</code> files.</p> <p>So, the file structure should look like this:</p> <pre><code>locales/\n\u251c\u2500\u2500 el\n\u2502   \u2514\u2500\u2500 LC_MESSAGES\n\u2502       \u2514\u2500\u2500 base.po\n\u2514\u2500\u2500 en\n    \u2514\u2500\u2500 LC_MESSAGES\n        \u2514\u2500\u2500 base.po\n</code></pre> <p>A PO file contains a number of messages, partly independent text segments to be translated, which have been grouped into one file according to some logical division of what is being translated. Those groups are called domains. In the example above, we have only one domain named as <code>base</code>. The PO files themselves are also called message catalogs. The PO format is a plain text format.</p> <p>Apart from PO files, you might sometimes encounter <code>.mo</code> files. MO, or Machine Object is a binary data file that contains object data referenced by a program. It is typically used to translate program code, and can be loaded or imported into the GNU gettext program.</p> <p>In addition, there are also <code>.pot</code> files. These are the template files for PO files. They will have all the translation strings left empty. A POT file is essentially an empty PO file without the translations, with just the original strings. In practice we have the <code>.pot</code> files be generated from some tools and we should not modify them directly.</p>"}, {"location": "gettext/#usage", "title": "Usage", "text": "<p>The <code>gettext</code> module comes shipped with Python. It exposes two APIs. The first one is the basic API that supports the GNU gettext catalog API. The second one is the higher level one, class-based API that may be more appropriate for Python files. The class bases API offers more flexibility and greater convenience than the GNU gettext API and it is the recommended way of localizing your Python applications and modules.</p> <p>In order to provide multilingual messages for your Python programs, you need to take the next steps:</p> <ul> <li>Mark all translatable strings in your program with a wrapper function.</li> <li>Run a suite of tools over your marked files to generate raw messages catalogs     or POT files.</li> <li>Duplicate the POT files into specific locale folders and write the     translations.</li> <li>Import and use the gettext module so that message strings are properly     translated.</li> </ul> <p>Let\u2019s start with a function that prints some strings.</p> <pre><code># main.py\ndef print_some_strings():\n    print(\"Hello world\")\n    print(\"This is a translatable string\")\n\nif __name__ == '__main__':\n    print_some_strings()\n</code></pre> <p>Now as it is you cannot provide localization options using <code>gettext</code>.</p> <p>The first step is to specially mark all translatable strings in the program. To do that we need to wrap all the translatable strings inside <code>_()</code>.</p> <pre><code># main.py\nimport gettext\n_ = gettext.gettext\ndef print_some_strings():\n    print(_(\"Hello world\"))\n    print(_(\"This is a translatable string\"))\nif __name__=='__main__':\n    print_some_strings()\n</code></pre> <p>Notice that we imported <code>gettext</code> and assigned <code>_</code>  as <code>gettext.gettext</code>. This is to ensure that our program compiles as well.</p> <p>If you run the program, you will see that nothing has changed:</p> <pre><code>$: python main.py\nHello world\nThis is a translatable string\n</code></pre> <p>However, now we are able to proceed to the next steps which are extracting the translatable messages in a POT file.</p>"}, {"location": "gettext/#create-the-pot-files", "title": "Create the POT files", "text": "<p>For the purpose of automating the process of generating raw translatable messages from wrapped strings throughout the applications, the <code>gettext</code> library authors have provided a set to tools that help to parse the source files and to extract the messages in a general message catalog.</p> <p>The Python distribution includes some specific programs called <code>pygettext.py</code> and <code>msgfmt.py</code> that recognize only python source code and not other languages.</p> <p>Call it specifying the file you want to parse the strings for:</p> <pre><code>$: pygettext -d base -o locales/base.pot src/main.py\n</code></pre> <p>If you want to search for other strings than <code>_</code>, use the <code>-k</code> flag, for example <code>-k gettext</code>.</p> <p>That will generate a <code>base.pot</code> file in the <code>locales</code> directory taken from our <code>main.py</code> program. Remember that POT files are just templates and we should not touch them. Let us inspect the contents of the <code>base.pot</code> file:</p> <pre><code># SOME DESCRIPTIVE TITLE.\n# Copyright (C) YEAR ORGANIZATION\n# FIRST AUTHOR &lt;EMAIL@ADDRESS&gt;, YEAR.\n#\nmsgid \"\"\nmsgstr \"\"\n\"Project-Id-Version: PACKAGE VERSION\\n\"\n\"POT-Creation-Date: 2018-01-28 16:47+0000\\n\"\n\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\n\"\n\"Last-Translator: FULL NAME &lt;EMAIL@ADDRESS&gt;\\n\"\n\"Language-Team: LANGUAGE &lt;LL@li.org&gt;\\n\"\n\"MIME-Version: 1.0\\n\"\n\"Content-Type: text/plain; charset=UTF-8\\n\"\n\"Content-Transfer-Encoding: 8bit\\n\"\n\"Generated-By: pygettext.py 1.5\\n\"\n#: src/main.py:5\nmsgid \"Hello world\"\nmsgstr \"\"\n#: src/main.py:6\nmsgid \"This is a translatable string\"\nmsgstr \"\"\n</code></pre> <p>In a bigger program, we would have many translatable strings following.  Here we specified a domain called base because the application is only one file. In bigger ones, I would use multiple domains in order to logically separate the different messages based on the application scope.</p> <p>Notice that we have a simple convention for our translatable strings. <code>msgid</code> is the original string wrapped in <code>_()</code> . <code>msgstr</code> is the translation we need to provide.</p>"}, {"location": "gettext/#create-the-po-files", "title": "Create the PO files", "text": "<p>Now we are ready to create our translations. Because we have the template generated for us, the next step is to create the required directory structure and copy the template into the right spot. We\u2019ve seen the recommended file structure before. We are going to create 2 additional directories inside <code>locales</code> with the structure <code>locales/$language/LC_MESSAGES/$domain.po</code></p> <p>Where:</p> <ul> <li><code>$language</code> is the language identifier such as <code>en</code> or <code>el</code></li> <li><code>$domain</code> is <code>base</code>.</li> </ul> <p>Copy and rename the <code>base.pot</code> into the following directories <code>locales/en/LC_MESSAGES/base.po</code> and <code>locales/el/LC_MESSAGES/base.po</code>. Then modify their headers to include more information about the locale. For example, this is the Greek translation.</p> <pre><code># My App.\n# Copyright (C) 2018\n#\nmsgid \"\"\nmsgstr \"\"\n\"Project-Id-Version: 1.0\\n\"\n\"POT-Creation-Date: 2018-01-28 16:47+0000\\n\"\n\"PO-Revision-Date: 2018-01-28 16:48+0000\\n\"\n\"Last-Translator: me &lt;johndoe@example.com&gt;\\n\"\n\"Language-Team: Greek &lt;yourteam@example.com&gt;\\n\"\n\"MIME-Version: 1.0\\n\"\n\"Content-Type: text/plain; charset=UTF-8\\n\"\n\"Content-Transfer-Encoding: 8bit\\n\"\n\"Generated-By: pygettext.py 1.5\\n\"\n#: main.py:5\nmsgid \"Hello world\"\nmsgstr \"\u03a7\u03ad\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5\"\n#: main.py:6\nmsgid \"This is a translatable string\"\nmsgstr \"\u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf\"\n</code></pre>"}, {"location": "gettext/#updating-pot-and-po-files", "title": "Updating POT and PO files", "text": "<p>Once you add more strings or change some strings in your program, you execute again <code>pygettext</code> which regenerates the template file:</p> <pre><code>pygettext main.py -o po/hello.pot\n</code></pre> <p>Then you can update individual translation files to match newly created templates (this includes reordering the strings to match new template) with <code>msgmerge</code>:</p> <pre><code>msgmerge --previous --update po/cs.po po/hello.pot\n</code></pre>"}, {"location": "gettext/#create-the-mo-files", "title": "Create the MO files", "text": "<p>The catalog is built from the <code>.po</code> file using a tool called <code>msgformat.py</code>. This tool will parse the <code>.po</code> file and generate an equivalent <code>.mo</code> file.</p> <pre><code>$: msgfmt -o base.mo base\n</code></pre> <p>This command will generate a <code>base.mo</code> file in the same folder as the <code>base.po</code> file.</p> <p>So, the final file structure should look like this:</p> <pre><code>locales\n\u251c\u2500\u2500 el\n\u2502   \u2514\u2500\u2500 LC_MESSAGES\n\u2502       \u251c\u2500\u2500 base.mo\n\u2502       \u2514\u2500\u2500 base.po\n\u251c\u2500\u2500 en\n\u2502   \u2514\u2500\u2500 LC_MESSAGES\n\u2502       \u251c\u2500\u2500 base.mo\n\u2502       \u2514\u2500\u2500 base.po\n\u2514\u2500\u2500 base.pot\n</code></pre>"}, {"location": "gettext/#switching-locale", "title": "Switching Locale", "text": "<p>To have the ability to switch locales in our program we need to actually use the Class based <code>gettext</code> API. One of it's methods is <code>gettext.translation</code>, it accepts some parameters that can be used to load the associated <code>.mo</code> files of a particular language. If no <code>.mo</code> file is found, it raises an error.</p> <p>Add the following code to the program:</p> <pre><code>import gettext\nel = gettext.translation('base', localedir='locales', languages=['el'])\nel.install()\n_ = el.gettext # Greek\n</code></pre> <p>The first argument base is the domain and the method will look for a <code>.po</code>  file with the same name in our locale directory. If you don\u2019t specify a domain it will fallback to the messages domain. The <code>localedir</code> parameter is the directory location of the <code>locales</code> directory you created. The <code>languages</code> parameter is a hint for the searching mechanism to load particular language code more resiliently.</p> <p>If you run the program again you will see the translations happening:</p> <pre><code>$ python main.py\n\u03a7\u03b1\u03af\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5\n\u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf\n</code></pre> <p>The install method will cause all the <code>_()</code> calls to return the Greek translated strings globally into the built-in namespace. This is because we assigned <code>_</code> to point to the Greek dictionary of translations. To go back to the English just assign <code>_</code> to be the original <code>gettext</code> object.</p> <pre><code>_ = gettext.gettext\n</code></pre>"}, {"location": "gettext/#finding-message-catalogs", "title": "Finding Message Catalogs", "text": "<p>When there are cases where you need to locate all translation files at runtime, you can use the <code>find</code> function as provided by the class-based API. This function takes a few parameters in order to retrieve from the disk a list of <code>.mo</code> files available.</p> <p>You can pass a <code>localedir</code>, a <code>domain</code> and a list of <code>languages</code>. If you don\u2019t, the library module will use the respective defaults, which is not what you intended to do in most cases. For example, if you don\u2019t specify a <code>localdir</code> parameter, it will fallback to <code>sys.prefix + \u2018/share/locale\u2019</code>  which is a global locale dir that can contain a lot of random files.</p> <p>The <code>language</code> portion of the path is taken from one of several environment variables that can be used to configure localization features (LANGUAGE, LC_ALL, LC_MESSAGES, and LANG). The first variable found to be set is used. Multiple languages can be selected by separating the values with a colon :.</p> <pre><code>&gt;&gt;&gt; os.environ['LANGUAGE']='el:en'\n&gt;&gt;&gt; gettext.find('base', 'locales')\n'locales/el/LC_MESSAGES/base.mo'\n&gt;&gt;&gt; gettext.find('base', 'locales', all=True)\n ['locales/el/LC_MESSAGES/base.mo', 'locales/en/LC_MESSAGES/base.mo']\n</code></pre>"}, {"location": "gettext/#using-f-strings", "title": "Using f-strings", "text": "<p>You can't use f-strings inside <code>gettext</code>, you'll get an <code>Seen unexpected token \"f\"</code> error, you need to use the old <code>format</code> method:</p> <pre><code>_('Hey {},').format(username)\n</code></pre>"}, {"location": "gettext/#integrations", "title": "Integrations", "text": "<p>You can use it with weblate.</p>"}, {"location": "gettext/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Reference</li> <li>Phrase blog on Localizing with GNU gettext</li> </ul>"}, {"location": "gh/", "title": "Github cli client", "text": "<p><code>gh</code> is GitHub\u2019s official command line tool.</p>"}, {"location": "gh/#installation", "title": "Installation", "text": "<p>Get the <code>deb</code> file from the releases page and install it with <code>sudo dpkg -i</code></p> <p>Authenticate the command following the steps of:</p> <pre><code>gh auth login\n</code></pre>"}, {"location": "gh/#usage", "title": "Usage", "text": ""}, {"location": "gh/#create-a-pull-request", "title": "Create a pull request", "text": "<p>Whenever you do a git push it will show you the link to create the pull request. To avoid going into the browser, you can use <code>gh pr create</code>.</p> <p>You can also merge a ready pull request with <code>gh pr merge</code></p>"}, {"location": "gh/#workflow-runs", "title": "Workflow runs", "text": "<p>With <code>gh run list</code> you can get the list of the last workflow runs. If you want to see the logs of one of the runs, get the id and run <code>gh run view {{ run_id }}</code>.</p> <p>To see what failed, run <code>gh run view {{ run_id }} --log-failed</code>.</p>"}, {"location": "gh/#pull-request-checks", "title": "Pull request checks", "text": "<p><code>gh</code> allows you to check the status of the checks of a pull requests, this is useful to get an alert once the checks are done. For example you can use the next bash/zsh function:</p> <pre><code>function checks(){\n    while true; do\n        gh pr checks\n        if [[ -z \"$(gh pr status --json statusCheckRollup | grep IN_PROGRESS)\" ]]; then\n          break\n        fi\n        sleep 1\n        echo\n    done\n    gh pr checks\n    echo -e '\\a'\n}\n</code></pre>"}, {"location": "gh/#trigger-a-workflow-run", "title": "Trigger a workflow run", "text": "<p>To manually trigger a workflow you need to first configure it to allow <code>workflow_dispatch</code> events.</p> <pre><code>on:\n    workflow_dispatch:\n</code></pre> <p>Then you can trigger the workflow with <code>gh workflow run {{ workflow_name }}</code>, where you can get the <code>workflow_name</code> with <code>gh workflow list</code></p>"}, {"location": "gh/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "git/", "title": "Git", "text": "<p>Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems).</p>"}, {"location": "git/#learning-git", "title": "Learning git", "text": "<p>Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible.</p> <p>Depending on how you like to learn I've found these options:</p> <ul> <li>Written courses: W3 git course</li> <li>Interactive tutorials: Learngitbranching interactive     tutorial</li> <li>Written article: Freecode camp article</li> <li>Video courses: Code academy and Udemy</li> </ul>"}, {"location": "git/#pull-request-process", "title": "Pull Request Process", "text": "<p>This part of the doc is shamefully edited from the source. It was for the k8s project but they are good practices that work for all the projects. It explains the process and best practices for submitting a PR It should serve as a reference for all contributors, and be useful especially to new and infrequent submitters.</p>"}, {"location": "git/#before-you-submit-a-pr", "title": "Before You Submit a PR", "text": "<p>This guide is for contributors who already have a PR to submit. If you're looking for information on setting up your developer environment and creating code to contribute to the project, search the development guide.</p> <p>Make sure your PR adheres to the projects best practices. These include following project conventions, making small PRs, and commenting thoroughly.</p>"}, {"location": "git/#run-local-verifications", "title": "Run Local Verifications", "text": "<p>You can run the tests in local before you submit your PR to predict the pass or fail of continuous integration.</p>"}, {"location": "git/#why-is-my-pr-not-getting-reviewed", "title": "Why is my PR not getting reviewed?", "text": "<p>A few factors affect how long your PR might wait for review.</p> <p>If it's the last few weeks of a milestone, we need to reduce churn and stabilize.</p> <p>Or, it could be related to best practices. One common issue is that the PR is too big to review. Let's say you've touched 39 files and have 8657 insertions. When your would-be reviewers pull up the diffs, they run away - this PR is going to take 4 hours to review and they don't have 4 hours right now. They'll get to it later, just as soon as they have more free time (ha!).</p> <p>There is a detailed rundown of best practices, including how to avoid too-lengthy PRs, in the next section.</p> <p>But, if you've already followed the best practices and you still aren't getting any PR love, here are some things you can do to move the process along:</p> <ul> <li> <p>Make sure that your PR has an assigned reviewer (assignee in GitHub). If not,   reply to the PR comment stream asking for a reviewer to be assigned.</p> </li> <li> <p>Ping the assignee (@username) on the PR comment stream, and ask for an   estimate of when they can get to the review.</p> </li> <li> <p>Ping the assignee by email (many of us have publicly available email   addresses).</p> </li> <li> <p>If you're a member of the organization ping the team (via @team-name) that   works in the area you're submitting code.</p> </li> <li> <p>If you have fixed all the issues from a review, and you haven't heard back,   you should ping the assignee on the comment stream with a \"please take another   look\" (<code>PTAL</code>) or similar comment indicating that you are ready for another   review.</p> </li> </ul> <p>Read on to learn more about how to get faster reviews by following best practices.</p>"}, {"location": "git/#best-practices-for-faster-reviews", "title": "Best Practices for Faster Reviews", "text": "<p>You've just had a brilliant idea on how to make a project better. Let's call that idea Feature-X. Feature-X is not even that complicated. You have a pretty good idea of how to implement it. You jump in and implement it, fixing a bunch of stuff along the way. You send your PR - this is awesome! And it sits. And sits. A week goes by and nobody reviews it. Finally, someone offers a few comments, which you fix up and wait for more review. And you wait. Another week or two go by. This is horrible.</p> <p>Let's talk about best practices so your PR gets reviewed quickly.</p>"}, {"location": "git/#familiarize-yourself-with-project-conventions", "title": "Familiarize yourself with project conventions", "text": "<ul> <li>Search for the Development guide</li> <li>Search for the Coding conventions</li> <li>Search for the API conventions</li> </ul>"}, {"location": "git/#is-the-feature-wanted-make-a-design-doc-or-sketch-pr", "title": "Is the feature wanted? Make a Design Doc or Sketch PR", "text": "<p>Are you sure Feature-X is something the project team wants or will accept? Is it implemented to fit with other changes in flight? Are you willing to bet a few days or weeks of work on it?</p> <p>It's better to get confirmation beforehand. There are two ways to do this:</p> <ul> <li>Make a proposal doc (in docs/proposals; for example the QoS   proposal), or reach out to the affected special   interest group (SIG). Some projects have that</li> <li>Coordinate your effort with SIG Docs ahead of time</li> <li>Make a sketch PR (e.g., just the API or Go interface). Write or code up just   enough to express the idea and the design and why you made those choices</li> </ul> <p>Or, do all of the above.</p> <p>Be clear about what type of feedback you are asking for when you submit a proposal doc or sketch PR.</p> <p>Now, if we ask you to change the design, you won't have to re-write it all.</p>"}, {"location": "git/#smaller-is-better-small-commits-small-prs", "title": "Smaller Is Better: Small Commits, Small PRs", "text": "<p>Small commits and small PRs get reviewed faster and are more likely to be correct than big ones.</p> <p>Attention is a scarce resource. If your PR takes 60 minutes to review, the reviewer's eye for detail is not as keen in the last 30 minutes as it was in the first. It might not get reviewed at all if it requires a large continuous block of time from the reviewer.</p> <p>Breaking up commits</p> <p>Break up your PR into multiple commits, at logical break points.</p> <p>Making a series of discrete commits is a powerful way to express the evolution of an idea or the different ideas that make up a single feature. Strive to group logically distinct ideas into separate commits.</p> <p>For example, if you found that Feature-X needed some prefactoring to fit in, make a commit that JUST does that prefactoring. Then make a new commit for Feature-X.</p> <p>Strike a balance with the number of commits. A PR with 25 commits is still very cumbersome to review, so use judgment.</p> <p>Breaking up PRs</p> <p>Or, going back to our prefactoring example, you could also fork a new branch, do the prefactoring there and send a PR for that. If you can extract whole ideas from your PR and send those as PRs of their own, you can avoid the painful problem of continually rebasing.</p> <p>Multiple small PRs are often better than multiple commits. Don't worry about flooding us with PRs. We'd rather have 100 small, obvious PRs than 10 unreviewable monoliths.</p> <p>We want every PR to be useful on its own, so use your best judgment on what should be a PR vs. a commit.</p> <p>As a rule of thumb, if your PR is directly related to Feature-X and nothing else, it should probably be part of the Feature-X PR. If you can explain why you are doing seemingly no-op work (\"it makes the Feature-X change easier, I promise\") we'll probably be OK with it. If you can imagine someone finding value independently of Feature-X, try it as a PR. (Do not link pull requests by <code>#</code> in a commit description, because GitHub creates lots of spam. Instead, reference other PRs via the PR your commit is in.)</p>"}, {"location": "git/#open-a-different-pr-for-fixes-and-generic-features", "title": "Open a Different PR for Fixes and Generic Features", "text": "<p>Put changes that are unrelated to your feature into a different PR.</p> <p>Often, as you are implementing Feature-X, you will find bad comments, poorly named functions, bad structure, weak type-safety, etc.</p> <p>You absolutely should fix those things (or at least file issues, please) - but not in the same PR as your feature. Otherwise, your diff will have way too many changes, and your reviewer won't see the forest for the trees.</p> <p>Look for opportunities to pull out generic features.</p> <p>For example, if you find yourself touching a lot of modules, think about the dependencies you are introducing between packages. Can some of what you're doing be made more generic and moved up and out of the Feature-X package? Do you need to use a function or type from an otherwise unrelated package? If so, promote! We have places for hosting more generic code.</p> <p>Likewise, if Feature-X is similar in form to Feature-W which was checked in last month, and you're duplicating some tricky stuff from Feature-W, consider prefactoring the core logic out and using it in both Feature-W and Feature-X. (Do that in its own commit or PR, please.)</p>"}, {"location": "git/#comments-matter", "title": "Comments Matter", "text": "<p>In your code, if someone might not understand why you did something (or you won't remember why later), comment it. Many code-review comments are about this exact issue.</p> <p>If you think there's something pretty obvious that we could follow up on, add a TODO.</p>"}, {"location": "git/#test", "title": "Test", "text": "<p>Nothing is more frustrating than starting a review, only to find that the tests are inadequate or absent. Very few PRs can touch code and NOT touch tests.</p> <p>If you don't know how to test Feature-X, please ask!  We'll be happy to help you design things for easy testing or to suggest appropriate test cases.</p>"}, {"location": "git/#squashing-and-commit-titles", "title": "Squashing and Commit Titles", "text": "<p>Your reviewer has finally sent you feedback on Feature-X.</p> <p>Make the fixups, and don't squash yet. Put them in a new commit, and re-push. That way your reviewer can look at the new commit on its own, which is much faster than starting over.</p> <p>We might still ask you to clean up your commits at the very end for the sake of a more readable history, but don't do this until asked: typically at the point where the PR would otherwise be tagged <code>LGTM</code>.</p> <p>Each commit should have a good title line (<code>&lt;70</code> characters) and include an additional description paragraph describing in more detail the change intended.</p> <p>General squashing guidelines:</p> <ul> <li>Sausage =&gt; squash</li> </ul> <p>Do squash when there are several commits to fix bugs in the original commit(s),  address reviewer feedback, etc. Really we only want to see the end state and  commit message for the whole PR.</p> <ul> <li>Layers =&gt; don't squash</li> </ul> <p>Don't squash when there are independent changes layered to achieve a single  goal. For instance, writing a code munger could be one commit, applying it  could be another, and adding a precommit check could be a third. One could  argue they should be separate PRs, but there's really no way to test/review the  munger without seeing it applied, and there needs to be a precommit check to  ensure the munged output doesn't immediately get out of date.</p> <p>A commit, as much as possible, should be a single logical change.</p>"}, {"location": "git/#kiss-yagni-mvp-etc", "title": "KISS, YAGNI, MVP, etc.", "text": "<p>Sometimes we need to remind each other of core tenets of software design - Keep It Simple, You Aren't Gonna Need It, Minimum Viable Product, and so on. Adding a feature \"because we might need it later\" is antithetical to software that ships. Add the things you need NOW and (ideally) leave room for things you might need later - but don't implement them now.</p>"}, {"location": "git/#its-ok-to-push-back", "title": "It's OK to Push Back", "text": "<p>Sometimes reviewers make mistakes. It's OK to push back on changes your reviewer requested. If you have a good reason for doing something a certain way, you are absolutely allowed to debate the merits of a requested change. Both the reviewer and reviewee should strive to discuss these issues in a polite and respectful manner.</p> <p>You might be overruled, but you might also prevail. We're pretty reasonable people. Mostly.</p> <p>Another phenomenon of open-source projects (where anyone can comment on any issue) is the dog-pile - your PR gets so many comments from so many people it becomes hard to follow. In this situation, you can ask the primary reviewer (assignee) whether they want you to fork a new PR to clear out all the comments. You don't HAVE to fix every issue raised by every person who feels like commenting, but you should answer reasonable comments with an explanation.</p>"}, {"location": "git/#common-sense-and-courtesy", "title": "Common Sense and Courtesy", "text": "<p>No document can take the place of common sense and good taste. Use your best judgment, while you put a bit of thought into how your work can be made easier to review. If you do these things your PRs will get merged with less friction.</p>"}, {"location": "git/#split-long-pr-into-smaller-ones", "title": "Split long PR into smaller ones", "text": "<ul> <li>Start a new branch from where you want to merge.</li> <li> <p>Start an interactive rebase on HEAD:     <pre><code>git rebase -i HEAD\n</code></pre></p> </li> <li> <p>Get the commits you want: Now comes the clever part, we are going to pick out     all the commits we care about from 112-new-feature-branch using the     following command:</p> <pre><code>git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/ spec/models\n</code></pre> <p>Woah thats quite the line! Let\u2019s dissect it first:</p> <ul> <li><code>git log</code> shows a log of what you have done in your project.</li> <li><code>--online</code> formats the output from a few lines (including author and time of   commit), to just \u201c[sha-hash-of-commit] [description-of-commit]\u201d</li> <li><code>--reverse</code> reverses the log output chronologically (so oldest commit first,   newest last).</li> <li><code>112-new-feature-branch..HEAD</code> shows the difference in commits from your   current branch (HEAD) and the branch you are interested in   112-new-feature-branch.</li> <li><code>-- app/models/ spec/models</code> Only show commits that changed files in   app/models/ or spec/models So that we confine the changes to our model and its   tests.</li> </ul> <p>Now if you are using vim (or vi or neovim) you can put the results of this command directly into your rebase-todo (which was opened when starting the rebase) using the :r command like so:</p> <pre><code>:r !git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/\n</code></pre> </li> <li> <p>Review the commits you want: Now you have a chance to go though your todo once     again. First you should remove the noop from above, since you actually do     something now. Second you should check the diffs of the sha-hashes.</p> <p>Note: If you are using vim, you might already have the fugitive plug-in. If you haven\u2019t changed the standard configuration, you can just move your cursor over the sha-hashes and press K (note that its capitalized) to see the diff of that commit.</p> <p>If you don\u2019t have fugitive or don\u2019t use vim, you can check the diff using git show SHA-HASH (for example <code>git show c4f74d0</code>), which shows the commits data.</p> <p>Now you can prepend and even rearrange the commits (Be careful rearranging or leaving out commits, you might have to fix conflicts later).</p> </li> <li> <p>Execute the rebase: Now you can save and exit the editor and git will try to     execute the rebase. If you have conflicts you can fix them just like you do     with merges and then continue using git rebase --continue.</p> <p>If you feel like something is going terribly wrong (for example you have a bunch of conflicts in just a few commits), you can abort the rebase using git rebase --abort and it will be like nothing ever happened.</p> </li> </ul>"}, {"location": "git/#git-workflow", "title": "Git workflow", "text": "<p>There are many ways of using git, one of the most popular is git flow, please read this article to understand it before going on.</p> <p>Unless you are part of a big team that delivers software that needs to maintain many versions, it's not worth using git flow as it's too complex and cumbersome. Instead I suggest a variation of the Github workflow.</p> <p>To carry out a reliable continuous delivery we must work to comply with the following list of best practices:</p> <ul> <li>Everything must be in the git server: source code, tests, pipelines, scripts,   templates and documentation.</li> <li>There is only a main branch (main) whose key is that everything is in this   branch must be always stable and deployable into production at any time.</li> <li>New branches are created from main in order to develop new features that   should be merged into main branch in short development cycles.</li> <li>It is highly recommended to do small commits to have more control over what is   being done and to avoid discarding many lines of code if a rollback has to be   done.</li> <li>A commit message policy should be set so that they are clear and conform the   same pattern, for example semantic versioning.</li> <li><code>main</code> is blocked to reject direct pushes as well as to protect it of   catastrophic deletion. Only pre-validated merge requests are accepted.</li> <li>When a feature is ready, we will open a merge request to merge changes into   <code>main</code> branch.</li> <li>Use webhooks to automate the execution of tests and validation tasks in the CI   server before/after adding changes in main.</li> <li>It is not needed to discard a merge request if any of the validation tasks   failed. We check the code and when the changes are pushed, the CI server will   relaunch the validation tasks.</li> <li>If all validation tasks pass, we will assign the merge request to two team   developers to review the feature code.</li> <li>After both reviewers validate the code, the merge request can be accepted and   the feature branch may be deleted.</li> <li>A clear versioning policy must be adopted for all generated artifacts.</li> <li>Each artifact must be generated once and be promoted to the different   environments in different stages.</li> </ul> <p>When a developer wants to add code to main should proceed as follows:</p> <ul> <li>Wait until the pipeline execution ends if it exists. If that process fails,   then the developer must help to other team members to fix the issue before   requesting a new merge request.</li> <li>Pull the changes from main and resolve the conflicts locally before pushing   the code to the new feature branch.</li> <li>Run a local script that compiles and executes the tests before committing   changes. This task can be done executing it manually by the developer or using   a git precommit.</li> <li>Open a new merge request setting the feature branch as source branch and   main as target branch.</li> <li>The CI server is notified of the new merge request and executes the pipeline   which compiles the source code, executes the tests, deploys the artifact, etc.</li> <li>If there are errors in the previous step, the developer must fix the code and   push it to the git server as soon as possible so that the CI server validate   once again the merge request.</li> <li>If no errors, the CI server will mark the merge request as OK and the   developer can assign it to two other team members to review the feature code.</li> <li>At this point, the developer can start with other task.</li> </ul> <p>Considerations</p> <p>The build process and the execution of the tests have to be pretty fast. It should not exceed about 10 minutes.</p> <p>Unit tests must be guarantee that they are completely unitary; they must be executed without starting the context of the application, they must not access to the DDBB, external systems, file system, etc.</p>"}, {"location": "git/#naming-conventions", "title": "Naming conventions", "text": "<p>The best idea is to use Semantic Versioning to define the names of the branches, for example: <code>feat/add-command-line-support</code> or <code>fix/correct-security-issue</code>, and also for the commit messages.</p>"}, {"location": "git/#tag-versioning-policy", "title": "Tag versioning policy", "text": "<p>We will also adopt semantic versioning policy on version management.</p>"}, {"location": "git/#versioning-control", "title": "Versioning control", "text": "<p>When a branch is merged into main, the CI server launches a job which generates a new artifact release as follow:</p> <ul> <li>The new version number is calculated taken into account the above   considerations.</li> <li>Generates a new artifact named as appname-major.minor.patch.build</li> <li>Upload the previous artifact to the artifact repository manager.</li> <li>Create a git tag on the repository with the same version identifier,   major.minor.patch.build</li> <li>Automatically deploy the artifact on the desired environment (dev, pre, etc)</li> </ul>"}, {"location": "git/#hotfixing", "title": "Hotfixing", "text": "<p>Hotfix should be developed and fixed using one of the next cases, which has been defined by preference order:</p>"}, {"location": "git/#case-1", "title": "Case 1", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production and we want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Merge the new branch to \"main\"</li> <li>Deploy main branch</li> </ul>"}, {"location": "git/#case-2", "title": "Case 2", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production but we  don't want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Deploy main branch</li> <li>Merge the new branch to \"main.</li> </ul>"}, {"location": "git/#case-3", "title": "Case 3", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Deploy main branch</li> <li>Merge the new branch to \"main.</li> </ul>"}, {"location": "git/#git-housekeeping", "title": "Git housekeeping", "text": "<p>The best option is to:</p> <pre><code>git fetch --prune\ngit-sweep cleanup\n</code></pre> <p>To remove the local branches you can:</p> <pre><code>cd myrepo\ngit remote add local $(pwd)\ngit-sweep cleanup --origin=local\n</code></pre> <ul> <li>git-sweep: For local branches</li> <li>archaeologit: Tool to search   strings in the history of a github user</li> <li>jessfraz made a tool ghb0t: For github</li> </ul>"}, {"location": "git/#submodules", "title": "Submodules", "text": "<p>Shamefully edited from the docs</p> <p>It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.</p> <p>Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available.</p> <p>Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.</p> <p>It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.</p> <p>Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available.</p> <p>Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.</p>"}, {"location": "git/#submodule-tips", "title": "Submodule tips", "text": ""}, {"location": "git/#submodule-foreach", "title": "Submodule Foreach", "text": "<p>There is a foreach submodule command to run some arbitrary command in each submodule. This can be really helpful if you have a number of submodules in the same project.</p> <p>For example, let\u2019s say we want to start a new feature or do a bugfix and we have work going on in several submodules. We can easily stash all the work in all our submodules.</p> <pre><code>git submodule foreach 'git stash'\n</code></pre> <p>Then we can create a new branch and switch to it in all our submodules.</p> <pre><code>git submodule foreach 'git checkout -b featureA'\n</code></pre> <p>You get the idea. One really useful thing you can do is produce a nice unified diff of what is changed in your main project and all your subprojects as well.</p> <pre><code>git diff; git submodule foreach 'git diff'\n</code></pre>"}, {"location": "git/#useful-aliases", "title": "Useful Aliases", "text": "<p>You may want to set up some aliases for some of these commands as they can be quite long and you can\u2019t set configuration options for most of them to make them defaults. We covered setting up Git aliases in Git Aliases, but here is an example of what you may want to set up if you plan on working with submodules in Git a lot.</p> <pre><code>git config alias.sdiff '!'\"git diff &amp;&amp; git submodule foreach 'git diff'\"\ngit config alias.spush 'push --recurse-submodules=on-demand'\ngit config alias.supdate 'submodule update --remote --merge'\n</code></pre> <p>This way you can simply run git supdate when you want to update your submodules, or git spush to push with submodule dependency checking.</p>"}, {"location": "git/#encrypt-sensitive-information", "title": "Encrypt sensitive information", "text": "<p>Use git-crypt.</p>"}, {"location": "git/#use-different-git-configs", "title": "Use different git configs", "text": "<p>Include in your <code>~/.gitconfig</code></p> <pre><code>[includeIf \"gitdir:~/company_A/\"]\n  path = ~/.config/git/company_A.config\n</code></pre> <p>Every repository you create under that directory it will append the other configuration</p>"}, {"location": "git/#renaming-from-master-to-main", "title": "Renaming from master to main", "text": "<p>There's been a movement to migrate from <code>master</code> to <code>main</code>, the reason behind it is that the initial branch name, <code>master</code>, is offensive to some people and we empathize with those hurt by the use of that term.</p> <p>Existing versions of Git are capable of working with any branch name; there's nothing special about <code>master</code> except that it has historically been the name used for the first branch when creating a new repository from scratch (with the <code>git init</code> command). Thus many projects use it to represent the primary line of development. We support and encourage projects to switch to branch names that are meaningful and inclusive.</p> <p>To configure <code>git</code> to use <code>main</code> by default run:</p> <pre><code>git config --global init.defaultBranch main\n</code></pre> <p>It only works on since git version 2.28.0, so you're stuck with manually changing it if you have an earlier version.</p>"}, {"location": "git/#changes-controversy", "title": "Change's Controversy", "text": "<p>The change is not free of controversy, for example in the PDM project some people are not sure that it's needed for many reasons. Let's see each of them:</p> <ul> <li> <p>The reason people are implementing the change is because other people are     doing it: After a quick search I found that the first one to do the change     was the software freedom conservancy with the Git     project. You can     also see Python,     Django,     Redis,     Drupal,     CouchDB and     Github's     statements.</p> <p>As we're not part of the deciding organisms of the collectives doing the changes, all we can use are their statements and discussions to guess what are the reasons behind their support of the change. Despite that some of them do use the argument that other communities do support the change to emphasize the need of the change, all of them mention that the main reason is that the term is offensive to some people.</p> </li> <li> <p>I don't see an issue using the term master: If you relate to this statement     it can be because you're not part of the communities that suffer the     oppression tied to the term, and that makes you blind to the issue. It's     a lesson I learned on my own skin throughout the years. There are thousand     of situations, gestures, double meaning words and sentences that went     unnoticed by me until I started discussing it with the people that are     suffering them (women, racialized people, LGTBQI+, ...). Throughout my     experience I've seen that the more privileged you are, the blinder you     become. You can read more on privileged blindness     here,     here or     here     (I've skimmed through the articles, and are the first articles I've found,     there are probably better references).</p> <p>I'm not saying that privileged people are not aware of the issues or that they can even raise them. We can do so and more we read, discuss and train ourselves, the better we'll detect them. All I'm saying is that a non privileged person will always detect more because they suffer them daily.</p> <p>I understand that for you there is no issue using the word master, there wasn't an issue for me either until I saw these projects doing the change, again I was blinded to the issue as I'm not suffering it. That's because change is not meant for us, as we're not triggered by it. The change is targeted to the people that do perceive that <code>master</code> is an offensive term. What we can do is empathize with them and follow this tiny tiny tiny gesture. It's the least we can do.</p> <p>Think of a term that triggers you, such as heil hitler, imagine that those words were being used to define the main branch of your code, and that everyday you sit in front of your computer you see them. You'll probably be reminded of the historic events, concepts, feelings that are tied to that term each time you see it, and being them quite negative it can slowly mine you. Therefore it's legit that you wouldn't want to be exposed to that negative effects.</p> </li> <li> <p>I don't see who will benefit from this change: Probably the people that     belongs to communities that are and have been under constant oppression for     a very long time, in this case, specially the racialized ones which have     suffered slavery.</p> <p>Sadly you will probably won't see many the affected people speak in these discussions, first because there are not that many, sadly the IT world is dominated by middle aged, economically comfortable, white, cis, hetero, males. Small changes like this are meant to foster diversity in the community by allowing them being more comfortable. Secondly because when they see these debates they move on as they are so fed up on teaching privileged people of their privileges. They not only have to suffer the oppression, we also put the burden on their shoulders to teach us.</p> </li> </ul> <p>As and ending thought, if you see yourself being specially troubled by the change, having a discomfort feeling and strong reactions. In my experience these signs are characteristic of privileged people that feel that their privileges are being threatened, I've felt them myself countless times. When I feel it, I usually do two things, fight them as strong as I can, or embrace them, analyze them, and go to the root of them. Depending on how much energy I have I go with the easy or the hard one. I'm not saying that it's you're case, but it could be.</p>"}, {"location": "git/#references", "title": "References", "text": "<ul> <li>FAQ</li> <li>Funny FAQ</li> <li>Nvie post on branching model</li> </ul>"}, {"location": "git/#courses", "title": "Courses", "text": "<ul> <li>W3 git course</li> <li>Learngitbranching interactive tutorial</li> <li>katakoda</li> <li>Code academy</li> <li>Udemy</li> <li>Freecode camp article</li> </ul>"}, {"location": "git/#tools", "title": "Tools", "text": "<ul> <li>git-extras</li> </ul>"}, {"location": "gitea/", "title": "Gitea", "text": "<p>Gitea is a community managed lightweight code hosting solution written in Go. It's the best self hosted Github alternative in my opinion.</p>"}, {"location": "gitea/#installation", "title": "Installation", "text": "<p>Gitea provides automatically updated Docker images within its Docker Hub organisation.</p>"}, {"location": "gitea/#using-docker-compose", "title": "Using Docker Compose", "text": ""}, {"location": "gitea/#connect-with-oicd", "title": "Connect with OICD", "text": "<p>Gitea doesn't yet support the mapping of OICD groups to organizations.</p>"}, {"location": "gitea/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Docs</li> </ul>"}, {"location": "goaccess/", "title": "goaccess", "text": "<p>goaccess is a fast terminal-based log analyzer.</p> <p>Its core idea is to quickly analyze and view web server statistics in real time without needing to use your browser (great if you want to do a quick analysis of your access log via SSH, or if you simply love working in the terminal).</p> <p>While the terminal output is the default output, it has the capability to generate a complete, self-contained real-time HTML report (great for analytics, monitoring and data visualization), as well as a JSON, and CSV report.</p>"}, {"location": "goaccess/#installation", "title": "Installation", "text": "<pre><code>apt-get install goaccess\n</code></pre>"}, {"location": "goaccess/#usage", "title": "Usage", "text": ""}, {"location": "goaccess/#custom-log-format", "title": "Custom log format", "text": "<p>Sometimes the log format isn't supported, then you'll have to specify the log format. For example:</p> <pre><code>goaccess \\\n    --log-format='%^,%^,%^: %h:%^ %^ [%d:%t.%^] %^ %^/%^ %^/%^/%^/%^/%^ %s %b - - ---- %^/%^/%^/%^/%^ %^/%^ {%v|} %^ %m \"\"%U\"\" \"%q\"' \\\n    --date-format '%d/%b/%Y' \\\n    --time-format '%H:%M:%S' \\\n    file.log\n</code></pre>"}, {"location": "goaccess/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li> <p>Docs</p> </li> <li> <p>Tweaking goaccess for analytics post</p> </li> </ul>"}, {"location": "goodconf/", "title": "GoodConf", "text": "<p>goodconf is a thin wrapper over Pydantic's settings management. Allows you to define configuration variables and load them from environment or JSON/YAML file. Also generates initial configuration files and documentation for your defined configuration.</p>"}, {"location": "goodconf/#installation", "title": "Installation", "text": "<p><code>pip install goodconf</code> or <code>pip install goodconf[yaml]</code> if parsing/generating YAML files is required.</p>"}, {"location": "goodconf/#basic-usage", "title": "Basic Usage", "text": "<p>Define the configuration object in <code>config.py</code>:</p> <pre><code>import base64\nimport os\n\nfrom goodconf import GoodConf, Field\nfrom pydantic import PostgresDsn\n\n\nclass AppConfig(GoodConf):  # type: ignore\n    \"\"\"Configure my application.\"\"\"\n\n    debug: bool\n    database_url: PostgresDsn = \"postgres://localhost:5432/mydb\"\n    secret_key: str = Field(\n        initial=lambda: base64.b64encode(os.urandom(60)).decode(),\n        description=\"Used for cryptographic signing. \"\n        \"https://docs.djangoproject.com/en/2.0/ref/settings/#secret-key\",\n    )\n\n    class Config:\n        \"\"\"Define the default files to check.\"\"\"\n\n        default_files = [\n            os.path.expanduser(\"~/.local/share/your_program/config.yaml\"),\n            \"config.yaml\",\n        ]\n\n\nconfig = AppConfig()\n</code></pre> <p>To load the configuration use <code>config.load()</code>. If you don't pass any file to <code>load()</code>, then the <code>default_files</code> will be read in order.</p> <p>Remember that environment variables always take precedence over variables in the configuration files.</p> <p>For more details see Pydantic's docs for examples of loading:</p> <ul> <li>Dotenv (.env) files.</li> <li>Docker secrets.</li> </ul>"}, {"location": "goodconf/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "grapheneos/", "title": "GrapheneOS", "text": "<p>GrapheneOS is a private and secure mobile operating system with Android app compatibility. Developed as a non-profit open source project.</p> <p>GrapheneOS is a private and secure mobile operating system with great functionality and usability. It starts from the strong baseline of the Android Open Source Project (AOSP) and takes great care to avoid increasing attack surface or hurting the strong security model. GrapheneOS makes substantial improvements to both privacy and security through many carefully designed features built to function against real adversaries. The project cares a lot about usability and app compatibility so those are taken into account for all of our features.</p> <p>GrapheneOS is also hard at work on filling in gaps from not bundling Google apps and services into the OS. We aren't against users using Google services but it doesn't belong integrated into the OS in an invasive way. GrapheneOS won't take the shortcut of simply bundling a very incomplete and poorly secured third party reimplementation of Google services into the OS. That wouldn't ever be something users could rely upon. It will also always be chasing a moving target while offering poorer security than the real thing if the focus is on simply getting things working without great care for doing it robustly and securely.</p>"}, {"location": "grapheneos/#features", "title": "Features", "text": "<p>These are a subset some of the features of GrapheneOS beyond what's provided by version 13 of the Android Open Source Project. It only covers our improvements to AOSP and not baseline features. This section doesn't list features like the standard app sandbox, verified boot, exploit mitigations (ASLR, SSP, Shadow Call Stack, Control Flow Integrity, etc.), permission system (foreground-only and one-time permission grants, scoped file access control, etc.) and so on but rather only our improvements to modern Android.</p>"}, {"location": "grapheneos/#defending-against-exploitation-of-unknown-vulnerabilities", "title": "Defending against exploitation of unknown vulnerabilities", "text": "<p>The first line of defense is attack surface reduction. Removing unnecessary code or exposed attack surface eliminates many vulnerabilities completely. GrapheneOS avoids removing any useful functionality for end users, but we can still disable lots of functionality by default and require that users opt-in to using it to eliminate it for most of them. An example we landed upstream in Android is disallowing using the kernel's profiling support by default, since it was and still is a major source of Linux kernel vulnerabilities.</p> <p>The next line of defense is preventing an attacker from exploiting a vulnerability, either by making it impossible, unreliable or at least meaningfully harder to develop. The vast majority of vulnerabilities are well understood classes of bugs and exploitation can be prevented by avoiding the bugs via languages/tooling or preventing exploitation with strong exploit mitigations. In many cases, vulnerability classes can be completely wiped out while in many others they can at least be made meaningfully harder to exploit. Android does a lot of work in this area and GrapheneOS has helped to advance this in Android and the Linux kernel.</p> <p>The final line of defense is containment through sandboxing at various levels: fine-grained sandboxes around a specific context like per site browser renderers, sandboxes around a specific component like Android's media codec sandbox and app / workspace sandboxes like the Android app sandbox used to sandbox each app which is also the basis for user/work profiles. GrapheneOS improves all of these sandboxes through fortifying the kernel and other base OS components along with improving the sandboxing policies.</p> <p>Preventing an attacker from persisting their control of a component or the OS / firmware through verified boot and avoiding trust in persistent state also helps to mitigate the damage after a compromise has occurred.</p>"}, {"location": "grapheneos/#attack-surface-reduction", "title": "Attack surface reduction", "text": "<ul> <li>Greatly reduced remote, local and proximity-based attack surface by stripping   out unnecessary code, making more features optional and disabling optional   features by default (NFC, Bluetooth, etc.), when the screen is locked   (connecting new USB peripherals, camera access) and optionally after a timeout   (Bluetooth, Wi-Fi)</li> <li>Option to disable native debugging (ptrace) to reduce local attack surface   (still enabled by default for compatibility)</li> </ul>"}, {"location": "grapheneos/#downsides", "title": "Downsides", "text": "<p>It looks that the community behind GrapheneOS is not the kindest one, they are sometimes harsh and when they are questioned they enter a defensive position. This can be seen in the discussions regarding whether or not to use the screen pattern lock (1, 2).</p>"}, {"location": "grapheneos/#recommended-devices", "title": "Recommended devices", "text": "<p>They strongly recommend only purchasing one of the following devices for GrapheneOS due to better security and a minimum 5 year guarantee from launch for full security updates and other improvements:</p> <ul> <li>Pixel 7 Pro</li> <li>Pixel 7</li> <li>Pixel 6a</li> <li>Pixel 6 Pro</li> <li>Pixel 6</li> </ul> <p>!!! note \"Check the source as this section is probably outdated\"</p> <p>Newer devices have more of their 5 year minimum guarantee remaining but the actual support time may be longer than the minimum guarantee.</p> <p>The Pixel 7 and Pixel 7 Pro are all around improvements over the Pixel 6 and Pixel 6 Pro with a significantly better GPU and cellular radio along with an incremental CPU upgrade. The 7<sup>th</sup> generation Pixels are far more similar to the previous generation than any prior Pixels.</p> <p>The Pixel 6 and Pixel 6 Pro are flagship phones with much nicer hardware than previous generation devices (cameras, CPU, GPU, display, battery).</p> <p>The cheaper Pixel 6 has extremely competitive pricing for the flagship level hardware especially with the guaranteed long term support. Pixel 6 Pro has 50% more memory (12GB instead of 8GB), a higher end screen, a 3<sup>rd</sup> rear camera with 4x optical zoom and a higher end front camera. Both devices have the same SoC (CPU, GPU, etc.) and the same main + ultrawide rear cameras. The Pixel 6 is quite large and the Pixel 6 Pro is larger.</p> <p>The Pixel 6a is a budget device with the same 5 years of guaranteed full security support from launch as the flagship 6<sup>th</sup> generation Pixels. It also has the same flagship SoC as the higher end devices, the same main rear and front cameras as the Pixel 5 and a rear wide angle lens matching the flagship 6<sup>th</sup> generation Pixels. Compared to the 5<sup>th</sup> generation Pixels, it has 5 years of full security support remaining instead of less than 2 years and the CPU is 2x faster. We strongly recommend buying the Pixel 6a rather than trying to get a deal with older generation devices. You'll be able to use the Pixel 6a much longer before it needs to be replaced due to lack of support.</p> <p>It's funny though that in the search for security and privacy you end up buying a Google device. If you also reached this thought, you're not alone. Summing up, the Pixel's are in fact the devices that are more secure and that potentially respect your privacy.</p>"}, {"location": "grapheneos/#installation", "title": "Installation", "text": "<p>I was not able to follow the web instructions so I had to follow the cli ones.</p> <p>Whenever I run a <code>fastboot</code> command it got stuck in <code>&lt; waiting for devices &gt;</code>, so I added the next rules on the <code>udev</code> configuration at <code>/etc/udev/rules.d/51-android.rules</code></p> <pre><code>SUBSYSTEM==\"usb\", ATTR{idVendor}==\"18d1\", ATTR{idProduct}==\"4ee7\", MODE=\"0600\", OWNER=\"myuser\"\n</code></pre> <p>The <code>idProduct</code> and <code>idVendor</code> were deduced from <code>lsusb</code>. Then after a restart everything worked fine.</p>"}, {"location": "grapheneos/#setup-auditor", "title": "Setup Auditor", "text": "<p>Auditor provides attestation for GrapheneOS phones and the stock operating systems on a number of devices. It uses hardware security features to make sure that the firmware and operating system have not been downgraded or tampered with.</p> <p>Attestation can be done locally by pairing with another Android 8+ device or remotely using the remote attestation service. To make sure that your hardware and operating system is genuine, perform local attestation immediately after the device has been setup and prior to any internet connection.</p>"}, {"location": "grapheneos/#references", "title": "References", "text": "<ul> <li> <p>Home</p> </li> <li> <p>Articles</p> </li> <li>Features</li> </ul>"}, {"location": "graylog/", "title": "Graylog", "text": "<p>Graylog is a log management tool</p>"}, {"location": "graylog/#tips", "title": "Tips", "text": ""}, {"location": "graylog/#send-a-test-message-to-check-an-input", "title": "Send a test message to check an input", "text": "<p>The next line will send a test message to the TCP 12201 port of the graylog server, if you use UDP, add the <code>-u</code> flag to the <code>nc</code> command.</p> <pre><code>echo -e '{\"version\": \"1.1\",\"host\":\"example.org\",\"short_message\":\"Short message\",\"full_message\":\"Backtrace here\\n\\nmore stuff\",\"level\":1,\"_user_id\":9001,\"_some_info\":\"foo\",\"_some_env_var\":\"bar\"}\\0' | nc -w 1 my.graylog.server 12201\n</code></pre> <p>To see if it arrives, you can check the <code>Input</code> you're trying to access, or at a lower level, you can <code>ngrep</code> with:</p> <pre><code>ngrep -d any port 12201\n</code></pre> <p>Or if you're using UDP:</p> <pre><code>ngrep -d any '' udp port 12201\n</code></pre>"}, {"location": "graylog/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "grocy_management/", "title": "Grocy Management", "text": "<p>Buying stuff is an unpleasant activity that drains your energy and time, it's the main perpetrator of the broken capitalist system, but sadly we have to yield to survive.</p> <p>This article explores my thoughts and findings on how to optimize the use of time, money and mental load in grocy management to have enough stuff stored to live, while following the principles of ecology and sustainability. I'm no expert at all on either of these topics. I'm learning and making my mind while writing these lines.</p> <p>grocy is a web-based self-hosted groceries &amp; household management solution for your home.</p> <p>My chosen way to deploy grocy has been using Docker. The hard part comes when you do the initial load, as you have to add all the:</p> <ul> <li>User attributes.</li> <li>Product locations.</li> <li>Product groups.</li> <li>Quantity conversions.</li> <li>Products.</li> </ul>"}, {"location": "grocy_management/#tips", "title": "Tips", "text": "<p>Note</p> <p>Very recommended to use the android app</p> <ul> <li>Add first the products with less letters, so add first <code>Toothpaste</code> and then   <code>Toothpaste roommate</code>.</li> <li>Do the filling in iterations:</li> <li>Add the common products: this can be done with the ticket of the last     groceries, or manually inspecting all the elements in your home.</li> <li>Incrementally add the recipes that you use</li> <li>Add the barcodes in the products that make sense.</li> <li>Add the <code>score</code> and <code>shop</code> userfields for the products, so you can evaluate   how much you like the product and where to buy it. If you show them in the   columns, you can also filter the shopping list by shop.</li> </ul>"}, {"location": "grocy_management/#minimum-quantities", "title": "Minimum quantities", "text": "<p>The minimum quantity defines when does the product is going to be added to the shopping list, it must be enough so we have time to go to the shop to buy more, so it has to follow:</p> <pre><code>minimum quantity = max_shop_frequency * average_consumption_rate * security_factor\n</code></pre> <p>Where: * <code>max_shop_frequency</code>: is the maximum number of days between I visit the   shop where I can obtain that product. If the product can be obtained in   several shops we'll take the smallest number of days. * <code>average_consumption_rate</code>: is the average number of units consumed per day.   It can be calculated by the following equation:</p> <pre><code>average_consumption_rate = total_units_consumed / days_since_first_unit_bought\n</code></pre> <p>The calculation could be improved giving more weight to the recent consumption   against the overall trend.</p> <ul> <li><code>security_factor</code>: Is an amount to add to take into account the imprecisions   on the measures. A starting <code>security_factor</code> could be 1.2.</li> </ul> <p>But we won't have most of the required data when we start from scratch, therefore I've followed the next criteria:</p> <ul> <li>If the product is critical, I want to always have at least a spare one, so the   minimum quantity will be 2.</li> <li>I roughly evaluate the relationship between the <code>average_consumption_rate</code> and   the <code>max_shop_frequency</code>.</li> </ul> <p>Also, I usually have a recipient for the critical products, so I mark the product as consumed once I transfer it from the original recipient to my recipient. Therefore I always have a security factor. This also helps to reduce the management time. For example, for the fruit, instead of marking as consumed each time I eat a piece, I mark them as consumed when I move them from the fridge to a recipient I've got in the living room.</p>"}, {"location": "grocy_management/#parent-products", "title": "Parent products", "text": "<p>Parent products let you group different kind of products under the same roof. The idea is to set the minimum quantity in the parent product and it will inherit all the quantities of it's children.</p> <p>I've used parent products for example to set a minimum amount of red tea, while storing the different red teas in different products.</p> <p>The advantage of this approach is that you have a detailed product page for each kind of product. This allows you to have different purchase - storage ratio, price evolution, set score and description for the different kinds, set different store...</p> <p>The disadvantage is that you have to add and maintain additional products.</p> <p>So if you expect that the difference between products is relevant split them, if you don't start with one product that aggregates all, like chocolate bar for all kinds of chocolate bars, and maybe in the future refactor it to a parent and child products.</p> <p>Another good use case is if the different brands of a product sell different sizes, so the conversion from buy unit to storage unit is different. Then I'll use a parent product that specifies the minimum and the different sub products with the different conversion rate.</p>"}, {"location": "grocy_management/#on-the-units", "title": "On the units", "text": "<p>I've been uncertain on what units use on some products.</p> <p>Imagine you buy a jar of peas, should you use jar or grams? or a bottle of wine should be in bottles or milliliters?</p> <p>The rule of thumb I've been using is:</p> <ul> <li>If the product is going to be used in a recipe, use whatever measure the   recipe is going to use. For example, grams for the peas.</li> <li>If not, use whatever will cost you less management time. For example,   milliliters for the wine (so I only have to update the inventory when the   bottle is gone).</li> </ul>"}, {"location": "grocy_management/#future-ideas", "title": "Future ideas", "text": "<p>I could monitor the ratio of rotting and when a product gets below the minimum stock to optimize the units to buy above the minimum quantity so as to minimize the shopping frequency. It can be saved in the <code>max_amount</code> user field.</p> <p>To calculate it's use I can use the average shelf life, last purchased and last used specified in the product information</p>"}, {"location": "grocy_management/#todo", "title": "TODO", "text": "<ul> <li>Define the userfields I've used</li> <li>Define the workflow for :</li> <li>initial upload</li> <li>purchase</li> <li>consumption</li> <li>cooking</li> <li>How to interact with humans that don't use the system but live in the same   space</li> </ul>"}, {"location": "grocy_management/#unclassified", "title": "Unclassified", "text": "<ul> <li>When creating a child product, copy the parent buy and stock units and     conversion, also the expiration till it's solved the child creation or     duplication (search issue)</li> <li>Use of pieza de fruta to monitor the amount instead of per product</li> <li>Caja de pa\u00f1uelos solo se cuentan los que est\u00e1n encima de la nevera</li> <li>La avena he apuntado lo que implica el rellenar el bote para consumir solo     cuando lo rellene.</li> <li>Locations are going to be used when you review the inventory so make sure you     don't have to walk far</li> <li> <p>Tare weight not supported with transfer</p> </li> <li> <p>it makes no sense to ask for the Location sheet to be editable, you've got the     stock overview for that. If you want to consume do so, if you want to add     you need to enter information one by one so you can't do it in a batch.</p> </li> <li>If you want only to check if an ingredient exist but don't want to consume it     select <code>Only check if a single unit is in stock (a different quantity can     then be used above)</code>.</li> <li>Marcar bayetas como abiertas para recordarte que tienes que cambiarla</li> <li>Common userfields should go together</li> <li>Acelga o lechuga dificil de medir por raciones o piezas, tirar de gramos</li> <li>Use stock units accordingly on how you consume them. 1 ration = \u00bd lemon, and   adjust the recipes accordingly.</li> <li>For example the acelgas are saved as pieces, lettuce, as two rations per   piece, spinach bought as kg and saved as rations</li> <li>Important to specify the location, as you'll use it later for the inventory   review</li> <li>IF you don't know the rations per kilogram, use kilograms till you know it.</li> <li>Buy unit the one you are going to encounter in the supermarket, both to input   in purchase and to see the evolution of price.</li> <li>In the shops only put the ones you want to buy to, even if in others the   product is available</li> <li>Things like the spices add them to recipes without consuming stock, and once   you see you are low on the spice consume the rations</li> <li>In the things that are so light that 0.01 means a lot, change the buying unit   to the equivalent x1000, even if you have to use other unit that is not the   buying unit (species case)</li> <li>When you don't still have the complete inventory and you are cooking with   someone, annotate in a paper the recipe or at least the elements it needs and   afterwards transfer them to grocy.</li> <li>Evaluate the use of sublocations in grocy, like Freezer:Drawer 1.</li> <li>For products that are in two places, (fregadero and stock), consume the stock     one instead of consuming the other and transfering the product.</li> <li>Adapt the due days of the fresh products that don't have it.</li> <li>If you hit enter in any field it commits the product (product description,     purchase)</li> </ul>"}, {"location": "grocy_management/#issues", "title": "Issues", "text": "<ul> <li>Standard consumption location:     Change it in the products that get consumed elsewhere.</li> <li>Allow stock modifications from the location content sheet     page: Nothing to do, start     using it. He closed them as duplicate of     1,     2 and     3.</li> </ul>"}, {"location": "grocy_management/#resources", "title": "Resources", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "hard_drive_health/", "title": "Hard Drive Health", "text": "<p>Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying.</p> <p>S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology; often written as SMART) is a monitoring system included in computer hard disk drives (HDDs), solid-state drives (SSDs), and eMMC drives. Its primary function is to detect and report various indicators of drive reliability with the intent of anticipating imminent hardware failures.</p> <p>Between all the SMART attributes, some that define define the health status of the hard drive, such as:</p> <ul> <li>Reallocated Sectors Count:  Count of reallocated sectors. The raw value   represents a count of the bad sectors that have been found and remapped.   Thus, the higher the attribute value, the more sectors the drive has had to   reallocate. This value is primarily used as a metric of the life expectancy of   the drive; a drive which has had any reallocations at all is significantly   more likely to fail in the immediate months.</li> <li>Spin Retry Count: Count of retry of spin start attempts. This attribute   stores a total count of the spin start attempts to reach the fully operational   speed (under the condition that the first attempt was unsuccessful). An   increase of this attribute value is a sign of problems in the hard disk   mechanical subsystem.</li> <li>Reallocate Event Count: Count of remap operations. The raw value of this   attribute shows the total count of attempts to transfer data from reallocated   sectors to a spare area. Both successful and unsuccessful attempts are   counted.</li> <li> <p>Current Pending Sector Count: Count of \"unstable\" sectors (waiting to be     remapped, because of unrecoverable read errors). If an unstable sector is     subsequently read successfully, the sector is remapped and this value is     decreased. Read errors on a sector will not remap the sector immediately     (since the correct value cannot be read and so the value to remap is not     known, and also it might become readable later); instead, the drive     firmware remembers that the sector needs to be remapped, and will remap it     the next time it's written.</p> <p>However, some drives will not immediately remap such sectors when written; instead the drive will first attempt to write to the problem sector and if the write operation is successful then the sector will be marked good (in this case, the \"Reallocation Event Count\" (0xC4) will not be increased). This is a serious shortcoming, for if such a drive contains marginal sectors that consistently fail only after some time has passed following a successful write operation, then the drive will never remap these problem sectors. * Offline Uncorrectable Sector Count: The total count of uncorrectable errors   when reading/writing a sector. A rise in the value of this attribute indicates   defects of the disk surface and/or problems in the mechanical subsystem.</p> </li> </ul>"}, {"location": "hard_drive_health/#check-the-warranty-status", "title": "Check the warranty status", "text": "<p>If your drive is still under warranty from the manufacturer you may consider RMA\u2019ing the drive (initiating a warranty return process).</p> <ul> <li>Seagate Warranty Check</li> <li>Western Digital (WD) Warranty Check</li> <li>HGST Warranty Check</li> <li>Toshiba Warranty Check</li> </ul>"}, {"location": "hard_drive_health/#wipe-all-the-disk", "title": "Wipe all the disk", "text": "<p>Sometimes the <code>CurrentPendingSector</code> doesn't get reallocated, if you don't mind about the data in the disk, you can wipe it all with:</p> <pre><code>dd if=/dev/zero of=/dev/{{ disk_id }} bs=4096 status=progress\n</code></pre>"}, {"location": "hard_drive_health/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "hard_drive_health/#smart-error-currentpendingsector-detected-on-host", "title": "SMART error (CurrentPendingSector) detected on host", "text": "<p>As stated above, this means that at some point, the drive was unable to successfully read the data from X different sectors, and hence have flagged them for possible reallocation. The sector will be marked as reallocated if a subsequent write fails. If the write succeeds, it is removed from current pending sectors and assumed to be OK.</p> <p>Start with a long self test with <code>smartctl</code>. Assuming the disk to test is <code>/dev/sdd</code>:</p> <pre><code>smartctl -t long /dev/sdd\n</code></pre> <p>The command will respond with an estimate of how long it thinks the test will take to complete.  (But this assumes that no errors will be found!)</p> <p>To check progress use:</p> <pre><code>smartctl -A /dev/sdd | grep remaining\n# or\nsmartctl -c /dev/sdd | grep remaining\n</code></pre> <p>Don't check too often because it can abort the test with some drives. If you receive an empty output, examine the reported status with:</p> <pre><code>smartctl -l selftest /dev/sdd\n</code></pre> <p>You will see something like this:</p> <pre><code>=== START OF READ SMART DATA SECTION ===\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Extended offline    Completed: read failure       20%      1596         44724966\n</code></pre> <p>So take that 'LBA' of 44724966 and multiply by (512/4096) which is the equivalent of 'divide by 8'</p> <pre><code>44724966 / 8 = 5590620.75\n</code></pre> <p>The sector to test then is <code>5590620</code>. If it is in the middle of a file, overwritting it will corrupt the file. If you are not cool with that, check the following posts to check if that sector belongs to a file:</p> <ul> <li>Smartmontools and fixing Unreadable Disk     Sectors.</li> <li>Smartmontools Bad Block how to</li> <li>Archlinux Identify damaged files page</li> <li>Archlinux badblocks page</li> </ul> <p>If you don't care to corrupt the file, use the following command to 'zero-out' the sector:</p> <pre><code>dd if=/dev/zero of=/dev/sda conv=sync bs=4096 count=1 seek=5590620\n1+0 records in\n1+0 records out\n\nsync\n</code></pre> <p>Now retry the <code>smartctl -t short</code> (or <code>smartctl -t long</code> if <code>short</code> fails) and see if the test is able to finish the test without errors:</p> <pre><code>=== START OF READ SMART DATA SECTION ===\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Short offline       Completed without error       00%     11699         -\n# 2  Extended offline    Completed: read failure       90%     11680         65344288\n# 3  Extended offline    Completed: read failure       90%     11675         65344288\n</code></pre> <p>If reading errors remain, repeat the steps above until they don't or skip to the bad block analysis step.</p> <p><code>Current_Pending_Sector</code> should be <code>0</code> now and the drive will probably be fine. As long as <code>Reallocated_Sector_Ct</code> is zero, you should be fine. Even a few reallocated sectors seems OK, but if that count starts to increment frequently, then that is a danger sign. To regularly keep a close eye on the counters use <code>smartd</code> to schedule daily tests.</p> <p>If <code>Current_Pending_Sector</code> is still not <code>0</code>, we need to do a deeper analysis on the bad blocks.</p>"}, {"location": "hard_drive_health/#bad-block-analysis", "title": "Bad block analysis", "text": "<p>The SMART <code>long</code> test gives no guarantee to find every error. To find them, we're going to use the <code>badblocks</code> tool instead.</p> <p>There is read-only mode (default) which is the least accurate. There is the destructive write-mode (-w option) which is the most accurate but takes longer and will (obviously) destroy all data on the drive, thus making it quite useless for matching sectors up to files. There is finally the non-destructive read-write mode which is probably as accurate as the destructive mode, with the only real downside that it is probably the slowest. However, if a drive is known to be failing then read-only mode is probably still the safest.</p>"}, {"location": "hard_drive_health/#links", "title": "Links", "text": "<ul> <li>S.M.A.R.T Wikipedia article.</li> <li>linux-hardware SMART disk probes.</li> </ul>"}, {"location": "hard_drive_health/#bad-blocks", "title": "Bad blocks", "text": "<ul> <li>Smartmontools and fixing Unreadable Disk     Sectors.</li> <li>Smartmontools Bad Block how to</li> <li>Archlinux Identify damaged files page</li> <li>Archlinux badblocks page</li> <li>Hard drive geek guide on reducing the current pending sector     count.</li> <li>Hiddencode guide on how to check bad sectors</li> <li>Hiddencode guide on how to fix bad sectors</li> </ul>"}, {"location": "helm_git/", "title": "helm-git", "text": "<p>helm-git is a helm downloader plugin that provides GIT protocol support.</p> <p>This fits the following use cases:</p> <ul> <li>Need to keep charts private.</li> <li>Doesn't want to package charts before installing.</li> <li>Charts in a sub-path, or with another ref than master.</li> <li>Pull values files directly from (private) Git repository.</li> </ul>"}, {"location": "helm_git/#installation", "title": "Installation", "text": "<pre><code>helm plugin install https://github.com/aslafy-z/helm-git --version 0.11.1\n</code></pre>"}, {"location": "helm_git/#usage", "title": "Usage", "text": "<p><code>helm-git</code> will package any chart that is not so you can directly reference paths to original charts.</p> <p>Here's the Git urls format, followed by examples:</p> <pre><code>git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\ngit+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\ngit+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\n\ngit+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&amp;sparse=0\ngit+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&amp;sparse=1\ngit+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2\ngit+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&amp;sparse=0&amp;depupdate=0\n</code></pre> <p>Add your repository:</p> <pre><code>helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2\n</code></pre> <p>You can use it as any other Helm chart repository. Try:</p> <pre><code>$ helm search cert-manager\nNAME                                    CHART VERSION   APP VERSION     DESCRIPTION\ncert-manager/cert-manager               v0.6.6          v0.6.2          A Helm chart for cert-manager\n\n$ helm install cert-manager/cert-manager --version \"0.6.6\"\n</code></pre> <p>Fetching also works:</p> <pre><code>helm fetch cert-manager/cert-manager --version \"0.6.6\"\nhelm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref=v0.6.2\n</code></pre>"}, {"location": "helm_git/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "html/", "title": "HTML", "text": "<p>HTML is the standard markup language for Web pages. With HTML you can create your own Website.</p>"}, {"location": "html/#document-structure", "title": "Document structure", "text": "<p>All HTML documents must start with a document type declaration: <code>&lt;!DOCTYPE html&gt;</code>.</p> <p>The HTML document itself begins with <code>&lt;html&gt;</code> and ends with <code>&lt;/html&gt;</code>.</p> <p>The visible part of the HTML document is between <code>&lt;body&gt;</code> and <code>&lt;/body&gt;</code>.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h1&gt;My First Heading&lt;/h1&gt;\n&lt;p&gt;My first paragraph.&lt;/p&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"}, {"location": "html/#html-elements", "title": "HTML elements", "text": "<ul> <li>Headings: <code>&lt;h1&gt;</code> to <code>&lt;h6&gt;</code></li> <li>Paragraphs: <code>&lt;p&gt;This is a paragraph.&lt;/p&gt;</code>.</li> <li>Links: <code>&lt;a href=\"https://www.w3schools.com\"&gt;This is a link&lt;/a&gt;</code></li> <li>Images: <code>&lt;img src=\"w3schools.jpg\" alt=\"W3Schools.com\" width=\"104\" height=\"142\"&gt;</code></li> <li>Line breaks: <code>&lt;br&gt;</code>, <code>&lt;hr&gt;</code></li> <li>Comments: <code>&lt;!-- Write your comments here --&gt;</code></li> <li>Code: <code>&lt;code&gt; x = 5&lt;/code&gt;</code></li> </ul>"}, {"location": "html/#links", "title": "Links", "text": "<p>HTML links are hyperlinks. You can click on a link and jump to another document.</p> <p>The HTML <code>&lt;a&gt;</code> tag defines a hyperlink. It has the following syntax:</p> <pre><code>&lt;a href=\"url\"&gt;link text&lt;/a&gt;\n</code></pre> <p>The link text is the part that will be visible to the reader.</p> <p>Link attributes:</p> <ul> <li><code>href</code>: indicates the link's destination.</li> <li><code>target</code>: specifies where to open the linked document. It can have one of the following values:<ul> <li><code>_self</code>: (Default) Opens the document in the same window/tab as it was     clicked.</li> <li><code>_blank</code>: Opens the document in a new window or tab.</li> <li><code>_parent</code>: Opens the document in the parent frame.</li> <li><code>_top</code>: Opens the document in the full body of the window.</li> </ul> </li> </ul>"}, {"location": "html/#images", "title": "Images", "text": "<p>The HTML <code>&lt;img&gt;</code> tag is used to embed an image in a web page.</p> <p>Images are not technically inserted into a web page; images are linked to web pages. The <code>&lt;img&gt;</code> tag creates a holding space for the referenced image.</p> <p>The <code>&lt;img&gt;</code> tag is empty, it contains attributes only, and does not have a closing tag.</p> <p>The <code>&lt;img&gt;</code> tag has two required attributes:</p> <ul> <li><code>src</code>: Specifies the path to the image.</li> <li><code>alt</code>: Specifies an alternate text for the image shown if the user for some     reason cannot view it.</li> </ul> <pre><code>&lt;img src=\"url\" alt=\"alternatetext\"&gt;\n</code></pre> <p>Other <code>&lt;img&gt;</code> attributes are:</p> <ul> <li> <p><code>&lt;style&gt;</code>: specify the width and height of an image.</p> <pre><code>&lt;img src=\"img_1.jpg\" alt=\"img_1\" style=\"width:500px;height:600px;\"&gt;\n</code></pre> <p>Even though you could use <code>width</code> and <code>height</code>, if you use the <code>style</code> attribute you prevent style sheets to change the size of images.</p> </li> <li> <p><code>&lt;float&gt;</code>: let the image float to the right or to the left of a text.</p> <pre><code>&lt;p&gt;&lt;img src=\"smiley.gif\" alt=\"Smiley face\" style=\"float:right;width:42px;height:42px;\"&gt;\nThe image will float to the right of the text.&lt;/p&gt;\n\n&lt;p&gt;&lt;img src=\"smiley.gif\" alt=\"Smiley face\" style=\"float:left;width:42px;height:42px;\"&gt;\nThe image will float to the left of the text.&lt;/p&gt;\n</code></pre> </li> </ul> <p>If you want to use an image as a link use:</p> <pre><code> &lt;a href=\"default.asp\"&gt;\n  &lt;img src=\"smiley.gif\" alt=\"HTML tutorial\" style=\"width:42px;height:42px;\"&gt;\n&lt;/a&gt;\n</code></pre>"}, {"location": "html/#lists", "title": "Lists", "text": "<p>HTML lists allow web developers to group a set of related items in lists.</p> <ul> <li> <p>Unordered lists: starts with the <code>&lt;ul&gt;</code> tag. Each list item starts with the     <code>&lt;li&gt;</code> tag. The list items will be marked with bullets (small black circles)     by default:</p> <p><pre><code>&lt;ul&gt;\n  &lt;li&gt;Coffee&lt;/li&gt;\n  &lt;li&gt;Tea&lt;/li&gt;\n  &lt;li&gt;Milk&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> * Ordered list: Starts with the <code>&lt;ol&gt;</code> tag. Each list item starts with the <code>&lt;li&gt;</code> tag. The list items will be marked with numbers by default:</p> <pre><code>&lt;ol&gt;\n  &lt;li&gt;Coffee&lt;/li&gt;\n  &lt;li&gt;Tea&lt;/li&gt;\n  &lt;li&gt;Milk&lt;/li&gt;\n&lt;/ol&gt;\n</code></pre> </li> </ul>"}, {"location": "html/#tables", "title": "Tables", "text": "<p>HTML tables allow web developers to arrange data into rows and columns.</p> <pre><code> &lt;table&gt;\n  &lt;tr&gt;\n    &lt;th&gt;Company&lt;/th&gt;\n    &lt;th&gt;Contact&lt;/th&gt;\n    &lt;th&gt;Country&lt;/th&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Alfreds Futterkiste&lt;/td&gt;\n    &lt;td&gt;Maria Anders&lt;/td&gt;\n    &lt;td&gt;Germany&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Centro comercial Moctezuma&lt;/td&gt;\n    &lt;td&gt;Francisco Chang&lt;/td&gt;\n    &lt;td&gt;Mexico&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;th&gt;</code>: Defines the table headers</li> <li><code>&lt;tr&gt;</code>: Defines the table rows</li> <li><code>&lt;td&gt;</code>: Defines the table cells</li> </ul>"}, {"location": "html/#blocks", "title": "Blocks", "text": "<p>A block-level element always starts on a new line, and the browsers automatically add some space (a margin) before and after the element.</p> <p>A block-level element always takes up the full width available (stretches out to the left and right as far as it can).</p> <p>An inline element does not start on a new line and only takes up as much width as necessary.</p> <ul> <li> <p><code>&lt;p&gt;</code>: defines a paragraph in an HTML document.</p> </li> <li> <p><code>&lt;div&gt;</code>: defines a division or a section in an HTML document. It has     no required attributes, but style, class and id are common. When used together     with CSS, the <code>&lt;div&gt;</code> element can be used to style blocks of content:</p> <p><pre><code>&lt;div style=\"background-color:black;color:white;padding:20px;\"&gt;\n  &lt;h2&gt;London&lt;/h2&gt;\n  &lt;p&gt;London is the capital city of England. It is the most populous city in the United Kingdom, with a metropolitan area of over 13 million inhabitants.&lt;/p&gt;\n&lt;/div&gt;\n</code></pre> * <code>&lt;span&gt;</code>: Is an inline container used to mark up a part of a text, or a part of a document.</p> <p>The <code>&lt;span&gt;</code> element has no required attributes, but style, class and id are common. When used together with CSS, the <code>&lt;span&gt;</code> element can be used to style parts of the text:</p> <pre><code>&lt;p&gt;My mother has &lt;span style=\"color:blue;font-weight:bold\"&gt;blue&lt;/span&gt; eyes and my father has &lt;span style=\"color:darkolivegreen;font-weight:bold\"&gt;dark green&lt;/span&gt; eyes.&lt;/p&gt;\n</code></pre> </li> </ul>"}, {"location": "html/#classes", "title": "Classes", "text": "<p>The <code>class</code> attribute is often used to point to a class name in a style sheet. It can also be used by a JavaScript to access and manipulate elements with the specific class name.</p> <p>In the following example we have three <code>&lt;div&gt;</code> elements with a class attribute with the value of \"city\". All of the three <code>&lt;div&gt;</code> elements will be styled equally according to the <code>.city</code> style definition in the head section:</p> <pre><code> &lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;style&gt;\n.city {\n  background-color: tomato;\n  color: white;\n  border: 2px solid black;\n  margin: 20px;\n  padding: 20px;\n}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;div class=\"city\"&gt;\n  &lt;h2&gt;London&lt;/h2&gt;\n  &lt;p&gt;London is the capital of England.&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"city\"&gt;\n  &lt;h2&gt;Paris&lt;/h2&gt;\n  &lt;p&gt;Paris is the capital of France.&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"city\"&gt;\n  &lt;h2&gt;Tokyo&lt;/h2&gt;\n  &lt;p&gt;Tokyo is the capital of Japan.&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>HTML elements can belong to more than one class. To define multiple classes, separate the class names with a space, e.g. <code>&lt;div class=\"city main\"&gt;</code>. The element will be styled according to all the classes specified.</p>"}, {"location": "html/#javascript", "title": "Javascript", "text": "<p>The HTML <code>&lt;script&gt;</code> tag is used to define a client-side script (JavaScript).</p> <p>The <code>&lt;script&gt;</code> element either contains script statements, or it points to an external script file through the src attribute.</p> <p>Common uses for JavaScript are image manipulation, form validation, and dynamic changes of content.</p> <p>This JavaScript example writes \"Hello JavaScript!\" into an HTML element with <code>id=\"demo\"</code>:</p> <pre><code>&lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML = \"Hello JavaScript!\";\n&lt;/script&gt;\n</code></pre> <p>The HTML <code>&lt;noscript&gt;</code> tag defines an alternate content to be displayed to users that have disabled scripts in their browser or have a browser that doesn't support scripts:</p> <pre><code>&lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML = \"Hello JavaScript!\";\n&lt;/script&gt;\n&lt;noscript&gt;Sorry, your browser does not support JavaScript!&lt;/noscript&gt;\n</code></pre>"}, {"location": "html/#head", "title": "Head", "text": "<p>The <code>&lt;head&gt;</code> element is a container for metadata (data about data) and is placed between the <code>&lt;html&gt;</code> tag and the <code>&lt;body&gt;</code> tag.</p> <p>HTML metadata is data about the HTML document. Metadata is not displayed.</p> <p>Metadata typically define the document title, character set, styles, scripts, and other meta information.</p> <p>It contains the next sections:</p> <ul> <li> <p><code>&lt;title&gt;</code>: defines the title of the document. The title must be text-only, and     it is used to:</p> <ul> <li>define the title in the browser toolbar</li> <li>provide a title for the page when it is added to favorites</li> <li>display a title for the page in search engine-results</li> </ul> <pre><code>&lt;title&gt;A Meaningful Page Title&lt;/title&gt;\n</code></pre> </li> <li> <p><code>&lt;style&gt;</code>: define style information for a single HTML page.</p> <pre><code>&lt;style&gt;\n  body {background-color: powderblue;}\n  h1 {color: red;}\n  p {color: blue;}\n&lt;/style&gt;\n</code></pre> </li> <li> <p><code>&lt;link&gt;</code>: defines the relationship between the current document and an external resource.</p> <pre><code> &lt;link rel=\"stylesheet\" href=\"mystyle.css\"&gt;\n</code></pre> </li> <li> <p><code>&lt;meta&gt;</code>: specify the character set, page description, keywords, author of the     document, and viewport settings. It won't be displayed on the page, but are     used by browsers (how to display content or reload page), by search engines     (keywords), and other web services. For example:</p> <ul> <li>Define the character set used: <code>&lt;meta charset=\"UTF-8\"&gt;</code>.</li> <li>Define keywords for search engines: <code>&lt;meta name=\"keywords\" content=\"HTML,     CSS, JavaScript\"&gt;</code>.</li> <li>Define a description of your web page: <code>&lt;meta name=\"description\"     content=\"Free Web tutorials\"&gt;</code>.</li> <li>Define the author of a page: <code>&lt;meta name=\"author\" content=\"John Doe\"&gt;</code>.</li> <li>Refresh document every 30 seconds: <code>&lt;meta http-equiv=\"refresh\" content=\"30\"&gt;</code>.</li> <li>Setting the viewport.</li> </ul> </li> <li> <p><code>&lt;script&gt;</code>: define client-side JavaScripts.</p> <pre><code>&lt;script&gt;\n    function myFunction() {\n      document.getElementById(\"demo\").innerHTML = \"Hello JavaScript!\";\n    }\n&lt;/script&gt;\n</code></pre> </li> <li> <p><code>&lt;base&gt;</code>: specifies the base URL and/or target for all relative URLs in     a page. The <code>&lt;base&gt;</code> tag must have either an href or a target attribute     present, or both.</p> <pre><code>&lt;base href=\"https://www.w3schools.com/\" target=\"_blank\"&gt;\n</code></pre> </li> </ul>"}, {"location": "html/#favicon", "title": "Favicon", "text": "<p>A favicon image is displayed to the left of the page title in the browser tab.</p> <p>To add a favicon to your website, either save your favicon image to the root directory of your webserver, or create a folder in the root directory called images, and save your favicon image in this folder. A common name for a favicon image is \"favicon.ico\".</p> <p>Next, add a <code>&lt;link&gt;</code> element to your \"index.html\" file, after the <code>&lt;title&gt;</code> element, like this:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;My Page Title&lt;/title&gt;\n  &lt;link rel=\"icon\" type=\"image/x-icon\" href=\"/images/favicon.ico\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n</code></pre>"}, {"location": "html/#styles", "title": "Styles", "text": "<p>The HTML <code>style</code> attribute is used to add styles to an element, such as color, font, size, and more.</p> <pre><code>&lt;tagname style=\"property:value;\"&gt;\n</code></pre> <p>The property is a CSS property. The value is a CSS value.</p>"}, {"location": "html/#formatting", "title": "Formatting", "text": "<p>Formatting elements were designed to display special types of text:</p> <ul> <li><code>&lt;b&gt;</code>: Bold text.</li> <li><code>&lt;strong&gt;</code>: Important text.</li> <li><code>&lt;i&gt;</code>: Italic text.</li> <li><code>&lt;em&gt;</code>: Emphasized text.</li> <li><code>&lt;mark&gt;</code>: Marked text.</li> <li><code>&lt;small&gt;</code>: Smaller text.</li> <li><code>&lt;del&gt;</code>: Deleted text.</li> <li><code>&lt;ins&gt;</code>: Inserted text.</li> <li><code>&lt;sub&gt;</code>: Subscript text.</li> <li><code>&lt;sup&gt;</code>: Superscript text.</li> </ul>"}, {"location": "html/#layout", "title": "Layout", "text": "<p>Websites often display content in multiple columns (like a magazine or a newspaper).</p> <p>HTML has several semantic elements that define the different parts of a web page: HTML5 Semantic Elements</p> <ul> <li><code>&lt;header&gt;</code>: Defines a header for a document or a section.</li> <li><code>&lt;nav&gt;</code>: Defines a set of navigation links.</li> <li><code>&lt;section&gt;</code>: Defines a section in a document.</li> <li><code>&lt;article&gt;</code>: Defines an independent, self-contained content.</li> <li><code>&lt;aside&gt;</code>: Defines content aside from the content (like a sidebar).</li> <li><code>&lt;footer&gt;</code>: Defines a footer for a document or a section.</li> <li><code>&lt;details&gt;</code>: Defines additional details that the user can open and close on     demand.</li> <li><code>&lt;summary&gt;</code>: Defines a heading for the  element."}, {"location": "html/#layout-elements", "title": "Layout elements", "text": ""}, {"location": "html/#section", "title": "Section", "text": "<p>A section is a thematic grouping of content, typically with a heading.</p> <p>Examples of where a <code>&lt;section&gt;</code> element can be used:</p> <ul> <li>Chapters</li> <li>Introduction</li> <li>News items</li> <li>Contact information</li> </ul> <pre><code> &lt;section&gt;\n&lt;h1&gt;WWF&lt;/h1&gt;\n&lt;p&gt;The World Wide Fund for Nature (WWF) is an international organization working on issues regarding the conservation, research and restoration of the environment, formerly named the World Wildlife Fund. WWF was founded in 1961.&lt;/p&gt;\n&lt;/section&gt;\n\n&lt;section&gt;\n&lt;h1&gt;WWF's Panda symbol&lt;/h1&gt;\n&lt;p&gt;The Panda has become the symbol of WWF. The well-known panda logo of WWF originated from a panda named Chi Chi that was transferred from the Beijing Zoo to the London Zoo in the same year of the establishment of WWF.&lt;/p&gt;\n&lt;/section&gt;\n</code></pre>"}, {"location": "html/#article", "title": "article", "text": "<p>The <code>&lt;article&gt;</code> element specifies independent, self-contained content.</p> <p>An article should make sense on its own, and it should be possible to distribute it independently from the rest of the web site.</p> <p>Examples of where the <code>&lt;article&gt;</code> element can be used:</p> <ul> <li>Forum posts</li> <li>Blog posts</li> <li>User comments</li> <li>Product cards</li> <li>Newspaper articles</li> </ul> <pre><code>&lt;article&gt;\n&lt;h2&gt;Google Chrome&lt;/h2&gt;\n&lt;p&gt;Google Chrome is a web browser developed by Google, released in 2008. Chrome is the world's most popular web browser today!&lt;/p&gt;\n&lt;/article&gt;\n\n&lt;article&gt;\n&lt;h2&gt;Mozilla Firefox&lt;/h2&gt;\n&lt;p&gt;Mozilla Firefox is an open-source web browser developed by Mozilla. Firefox has been the second most popular web browser since January, 2018.&lt;/p&gt;\n&lt;/article&gt;\n\n&lt;article&gt;\n&lt;h2&gt;Microsoft Edge&lt;/h2&gt;\n&lt;p&gt;Microsoft Edge is a web browser developed by Microsoft, released in 2015. Microsoft Edge replaced Internet Explorer.&lt;/p&gt;\n&lt;/article&gt;\n</code></pre>"}, {"location": "html/#header", "title": "header", "text": "<p>The <code>&lt;header&gt;</code> element represents a container for introductory content or a set of navigational links.</p> <p>A <code>&lt;header&gt;</code> element typically contains:</p> <ul> <li>one or more heading elements (<code>&lt;h1&gt;</code> - <code>&lt;h6&gt;</code>)</li> <li>logo or icon</li> <li>authorship information</li> </ul> <pre><code> &lt;article&gt;\n  &lt;header&gt;\n    &lt;h1&gt;What Does WWF Do?&lt;/h1&gt;\n    &lt;p&gt;WWF's mission:&lt;/p&gt;\n  &lt;/header&gt;\n  &lt;p&gt;WWF's mission is to stop the degradation of our planet's natural environment,\n  and build a future in which humans live in harmony with nature.&lt;/p&gt;\n&lt;/article&gt;\n</code></pre>"}, {"location": "html/#footer", "title": "footer", "text": "<p>The <code>&lt;footer&gt;</code> element defines a footer for a document or section.</p> <p>A <code>&lt;footer&gt;</code> element typically contains:</p> <ul> <li>authorship information</li> <li>copyright information</li> <li>contact information</li> <li>sitemap</li> <li>back to top links</li> <li>related documents</li> </ul> <pre><code> &lt;footer&gt;\n  &lt;p&gt;Author: Hege Refsnes&lt;/p&gt;\n  &lt;p&gt;&lt;a href=\"mailto:hege@example.com\"&gt;hege@example.com&lt;/a&gt;&lt;/p&gt;\n&lt;/footer&gt;\n</code></pre>"}, {"location": "html/#layout-techniques", "title": "Layout Techniques", "text": "<p>There are four different techniques to create multicolumn layouts. Each technique has its pros and cons:</p> <ul> <li>CSS framework</li> <li>CSS float property</li> <li>CSS flexbox</li> <li>CSS grid</li> </ul>"}, {"location": "html/#frameworks", "title": "Frameworks", "text": "<p>If you want to create your layout fast, you can use a CSS framework, like W3.CSS or Bootstrap.</p>"}, {"location": "html/#float-layout", "title": "Float layout", "text": "<p>It is common to do entire web layouts using the CSS <code>float</code> property. Float is easy to learn - you just need to remember how the <code>float</code> and <code>clear</code> properties work.</p> <p>Disadvantages: Floating elements are tied to the document flow, which may harm the flexibility.</p>"}, {"location": "html/#flexbox-layout", "title": "Flexbox layout", "text": "<p>Use of flexbox ensures that elements behave predictably when the page layout must accommodate different screen sizes and different display devices.</p>"}, {"location": "html/#grid-layout", "title": "Grid layout", "text": "<p>The CSS Grid Layout Module offers a grid-based layout system, with rows and columns, making it easier to design web pages without having to use floats and positioning.</p>"}, {"location": "html/#responsive", "title": "Responsive", "text": "<p>Responsive web design is about creating web pages that look good on all devices.</p> <p>A responsive web design will automatically adjust for different screen sizes and viewports.</p>"}, {"location": "html/#setting-the-viewport", "title": "Setting the viewport", "text": "<p>To create a responsive website, add the following <code>&lt;meta&gt;</code> tag to all your web pages:</p> <p><pre><code>&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n</code></pre> This gives the browser instructions on how to control the page's dimensions and scaling.</p> <p>The <code>width=device-width</code> part sets the width of the page to follow the screen-width of the device (which will vary depending on the device).</p> <p>The <code>initial-scale=1.0</code> part sets the initial zoom level when the page is first loaded by the browser.</p>"}, {"location": "html/#responsive-images", "title": "Responsive images", "text": "<p>Using the <code>max-width</code> property: If the CSS <code>max-width</code> property is set to <code>100%</code>, the image will be responsive and scale up and down, but never scale up to be larger than its original size:</p> <pre><code>&lt;img src=\"img_girl.jpg\" style=\"max-width:100%;height:auto;\"&gt;\n</code></pre>"}, {"location": "html/#responsive-text-size", "title": "Responsive text size", "text": "<p>The text size can be set with a \"vw\" unit, which means the \"viewport width\".</p> <p>That way the text size will follow the size of the browser window:</p> <pre><code>&lt;h1 style=\"font-size:10vw\"&gt;Hello World&lt;/h1&gt;\n</code></pre> <p>Viewport is the browser window size. <code>1vw = 1%</code> of viewport width. If the viewport is 50cm wide, 1vw is 0.5cm.</p>"}, {"location": "html/#media-queries", "title": "Media queries", "text": "<p>In addition to resize text and images, it is also common to use media queries in responsive web pages.</p> <p>With media queries you can define completely different styles for different browser sizes.</p> <p>The next example will make the three div elements display horizontally on large screens and stacked vertically on small screens:</p> <pre><code> &lt;style&gt;\n.left, .right {\n  float: left;\n  width: 20%; /* The width is 20%, by default */\n}\n\n.main {\n  float: left;\n  width: 60%; /* The width is 60%, by default */\n}\n\n/* Use a media query to add a breakpoint at 800px: */\n@media screen and (max-width: 800px) {\n  .left, .main, .right {\n    width: 100%; /* The width is 100%, when the viewport is 800px or smaller */\n  }\n}\n&lt;/style&gt;\n</code></pre>"}, {"location": "html/#code-style", "title": "Code Style", "text": "<ul> <li>Always declare the document type as the first line in your document.     <pre><code>&lt;!DOCTYPE html&gt;\n</code></pre></li> <li>Use lowercase element names:     <pre><code>&lt;body&gt;\n&lt;p&gt;This is a paragraph.&lt;/p&gt;\n&lt;/body&gt;\n</code></pre></li> <li>Close all HTML elements.</li> <li>Use lowercase attribute names</li> <li>Always quote attribute values</li> <li>Always Specify alt, width, and height for Images.</li> <li>Don't add spaces between equal signs: <code>&lt;link rel=\"stylesheet\"     href=\"styles.css\"&gt;</code></li> <li>Avoid Long Code Lines</li> <li>Do not add blank lines, spaces, or indentations without a reason.</li> <li>Use two spaces for indentation instead of tab</li> <li>Never Skip the <code>&lt;title&gt;</code> Element</li> <li>Always add the <code>&lt;html&gt;</code>, <code>&lt;head&gt;</code> and <code>&lt;body&gt;</code> tags.</li> <li> <p>Always include the <code>lang</code> attribute inside the <code>&lt;html&gt;</code> tag</p> <p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-us\"&gt;\n&lt;/html&gt;\n</code></pre> * Set the character encoding: <code>&lt;meta charset=\"UTF-8\"&gt;</code> * Set the viewport.</p> </li> </ul>"}, {"location": "html/#tips", "title": "Tips", "text": ""}, {"location": "html/#html-beautifier", "title": "HTML beautifier", "text": "<p>If you encounter html code that it's not well indented  you can use html beautify.</p>"}, {"location": "html/#references", "title": "References", "text": "<ul> <li>W3 tutorial</li> </ul>"}, {"location": "husboard/", "title": "Hushboard", "text": "<p>Hushboard is an utility that mutes your microphone while you\u2019re typing.</p>"}, {"location": "husboard/#installation", "title": "Installation", "text": "<p>They recommend using the Snap Store package but you can also install it manually as follows:</p> <pre><code>sudo apt install libgirepository1.0-dev libcairo2-dev\nmkvirtualenv hushboard\ngit clone https://github.com/stuartlangridge/hushboard\ncd hushboard\npip install pycairo PyGObject six xlib\npip install .\ndeactivate\n</code></pre>"}, {"location": "husboard/#running-the-application", "title": "Running the application", "text": "<p>You can run it manually as follows</p> <pre><code>workon hushboard\npython -m hushboard\ndeactivate\n</code></pre> <p>Or if you use i3wm, create the following script.</p> <pre><code>#!/usr/bin/env bash\n\nsource {WORKON_PATH}/hushboard/bin/activate\npython -m hushboard\ndeactivate\n</code></pre> <p>You should replace <code>{WORKON_PATH}</code> with your virtual environments path. Then add this line to your <code>i3wm</code> configuration file to start it automatically.</p> <pre><code>exec --no-startup-id ~/scripts/hushboard.sh\n</code></pre>"}, {"location": "husboard/#reference", "title": "Reference", "text": "<ul> <li>M0wer Husboard article</li> </ul>"}, {"location": "i3wm/", "title": "i3", "text": "<p>i3 is a tiling window manager.</p>"}, {"location": "i3wm/#layout-saving", "title": "Layout saving", "text": "<p>Layout saving/restoring allows you to load a JSON layout file so that you can have a base layout to start working with after powering on your computer.</p> <p>First of all arrange the windows in the workspace, then you can save the layout of either a single workspace or an entire output:</p> <pre><code>i3-save-tree --workspace \"1: terminal\" &gt; ~/.i3/workspace-1.json\n</code></pre> <p>You need to open the created file and remove the comments that match the desired windows under the <code>swallows</code> keys, so transform the next snippet:</p> <pre><code>    ...\n    \"swallows\": [\n        {\n        //  \"class\": \"^URxvt$\",\n        //  \"instance\": \"^irssi$\"\n        }\n    ]\n    ...\n</code></pre> <p>Into:</p> <pre><code>    ...\n    \"swallows\": [\n        {\n            \"class\": \"^URxvt$\",\n            \"instance\": \"^irssi$\"\n        }\n    ]\n    ...\n</code></pre> <p>Once is ready close all the windows of the workspace you want to restore (moving them away is not enough!).</p> <p>Then on a terminal you can restore the layout with:</p> <pre><code>i3-msg 'workspace \"1: terminal\"; append_layout ~/.i3/workspace-1.json'\n</code></pre> <p>It's important that you don't use a relative path</p> <p>Even if you're in <code>~/.i3/</code> you have to use <code>i3-msg append_layout ~/.i3/workspace-1.json</code>.</p> <p>This command will create some fake windows (called placeholders) with the layout you had before, <code>i3</code> will then wait for you to create the windows that match the selection criteria. Once they are, it will put them in their respective placeholders.</p> <p>If you wish to create the layouts at startup you can add the next snippet to your i3 config.</p> <pre><code>exec --no-startup-id \"i3-msg 'workspace \\\"1: terminal\\\"; append_layout ~/.i3/workspace-1.json'\"\n</code></pre>"}, {"location": "i3wm/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "ics/", "title": "ics", "text": "<p>ics is a pythonic iCalendar library. Its goals are to read and write ics data in a developer-friendly way.</p>"}, {"location": "ics/#installation", "title": "Installation", "text": "<p>Install using pip:</p> <pre><code>pip install ics\n</code></pre>"}, {"location": "ics/#usage", "title": "Usage", "text": "<p><code>ics</code> will delete all data that it doesn't understand. Maybe it's better for your case to build a parse for ics.</p>"}, {"location": "ics/#import-a-calendar-from-a-file", "title": "Import a calendar from a file", "text": "<pre><code>file = '/tmp/event.ics'\n\nfrom ics import Calendar\n\nwith open(file, 'r') as fd:\n    calendar = Calendar(fd.read())\n\n# &lt;Calendar with 118 events and 0 todo&gt;\ncalendar.events\n\n# {&lt;Event 'Visite de \"Fab Bike\"' begin:2016-06-21T15:00:00+00:00 end:2016-06-21T17:00:00+00:00&gt;,\n# &lt;Event 'Le lundi de l'embarqu\u00e9: Adventure in Espressif Non OS SDK edition' begin:2018-02-19T17:00:00+00:00 end:2018-02-19T22:00:00+00:00&gt;,\n#  ...}\nevent = list(calendar.timeline)[0]\n</code></pre>"}, {"location": "ics/#export-a-calendar-to-a-file", "title": "Export a Calendar to a file", "text": "<pre><code>with open('my.ics', 'w') as f:\n    f.writelines(calendar.serialize_iter())\n# And it's done !\n\n# iCalendar-formatted data is also available in a string\ncalendar.serialize()\n# 'BEGIN:VCALENDAR\\nPRODID:...\n</code></pre>"}, {"location": "ics/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "instant_messages_management/", "title": "Instant messages management", "text": "<p>Instant messaging in all it's forms is becoming the main communication channel.</p> <p>As any other input system, if not used wisely, it can be a sink of productivity.</p>"}, {"location": "instant_messages_management/#analyze-how-often-you-need-to-check-it", "title": "Analyze how often you need to check it", "text": "<p>Follow the interruption analysis to discover how often you need to check it and if you need the notifications or fine grain them to the sources that have higher priority. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.</p>"}, {"location": "instant_messages_management/#workflow", "title": "Workflow", "text": "<p>I interact with messaging applications in two ways:</p> <ul> <li>To read the new items and answer questions.</li> <li>To start a conversation.</li> </ul> <p>The passively reading for new items works perfectly with the interruption management processes. Each time you decide to check for new messages, follow the inbox processing guidelines to extract the information to the appropriate system (task manager, calendar or knowledge manager). If you answer someone or if you start a new conversation, assume that any work done in the next 5 to 10 minutes will probably be interrupted, so choose small or mindless tasks. If the person doesn't answer in that time, start a new pomodoro and go back when the next interruption event comes.</p>"}, {"location": "instant_messages_management/#use-calls-for-non-short-conversations", "title": "Use calls for non short conversations", "text": "<p>Chats are good for short conversations that don't require long or quick responses. Even though people may have forgotten it, they are an asynchronous communication channel.</p> <p>They're not suited for long conversations though as:</p> <ul> <li>Typing on a keyboard (or a mobile <code>\u1559(\u21c0\u2038\u21bc\u2036)\u1557</code>) is slower than talking directly.</li> <li>It's difficult to transmit the conversation tone by message, and each reader     can interpret it differently, leading to misunderstandings.</li> <li>If the conversation topic is complex, graphical aids such as screen sharing or     doodling can make the conversation more efficient.</li> <li>Unless everyone involved is fully focused on the conversation, the delays     between messages can be high, and all that time, the attendees need to     manage the interruptions.</li> <li>If you fully focus on the conversation, you're loosing your time while you     wait for the other to answer.</li> </ul> <p>For all these reasons, whenever a conversation looks not to be short or trivial, arrange a quick call or video call.</p>"}, {"location": "instant_messages_management/#at-work-or-collectives-use-group-rooms-over-direct-messages", "title": "At work or collectives, use group rooms over direct messages", "text": "<p>Asking for help through direct messages should be avoided whenever possible, instead of interrupting one person, it's better to ask in the group rooms because:</p> <ul> <li>More people are reading, so you'll probably get answered sooner.</li> <li>Knowledge is spread throughout the group instead of isolated on specific     people. Even if I don't answer a question, I read what others have     said thus learning in the process.</li> <li>The responsibility of answering is shared between the group members, making     it easier to define the interruptions role.</li> </ul>"}, {"location": "instant_messages_management/#use-threads-or-replies-if-the-client-allows-it", "title": "Use threads or replies if the client allows it", "text": "<p>Threads are a feature that allows people to have parallel conversations in the same room in a way that the messages aren't mixed. This makes it easier to maintain the focus and follow past messages. It also allows users that are not interested, to silence the thread, so they won't get application or/and desktop notifications on that particular topic.</p> <p>Replies can be used when the conversation is not lengthy enough to open a thread. They give the benefit of giving context to the user you're replying to.</p>"}, {"location": "instant_messages_management/#use-chats-to-transport-information-not-to-store-it", "title": "Use chats to transport information, not to store it", "text": "<p>Chat applications were envisioned as a protocol for person A to send information to person B. The fact that the message providers allow users to have almost no limit on their message history has driven people to use them as a knowledge repository. This approach has many problems:</p> <ul> <li>As most people don't use end to end encryption (OMEMO/OTR/Signal), the data of     their messages is available for the service provider to read. This is     a privacy violation that should be avoided. Most providers don't allow you     to set a message limit, so you'd have to delete them manually.</li> <li>Searching information in the chats is a nightmare. There are more     efficient knowledge repositories to store your information.</li> </ul>"}, {"location": "instant_messages_management/#use-key-bindings", "title": "Use key bindings", "text": "<p>Using the mouse to interact with the chat client graphical interfaces is not efficient, try to learn the key bindings and use them as much as possible.</p>"}, {"location": "instant_messages_management/#environment-setup", "title": "Environment setup", "text": ""}, {"location": "instant_messages_management/#account-management", "title": "Account management", "text": "<p>It's common to have more than one account or application to check. There are many instant messaging solutions, such as XMPP, Signal, IRC, Telegram, Slack, Whatssap or Facebook. It would be ideal to have a client that could act as a bridge to all the solutions, but at least I don't know it, so you're forced to install the different applications to interact with them.</p> <p>The obvious suggestion would be to reduce the number of platforms in use, but we all know that it's asking too much as it will probably isolate you from specific people.</p> <p>Once you have the minimum clients chosen, put them all on the same workspace, for example an i3 window manager workspace, and only check them following the workflow rules.</p>"}, {"location": "instant_messages_management/#isolate-your-work-and-personal-environments", "title": "Isolate your work and personal environments", "text": "<p>Make sure that you set your environment so that you can't check your personal chats when you're working and the other way around. For example, you could configure different instances of the chat clients and only open the ones that you need to. Or you could avoid configuring the work clients in your personal phone.</p> <p>For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities.</p> <p>For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird. Once you choose one, try to master it.</p>"}, {"location": "instant_messages_management/#fine-grain-configure-the-notifications", "title": "Fine grain configure the notifications", "text": "<p>Modern client applications allow you to define the notifications at room or people level. I usually:</p> <ul> <li>Use notifications on all messages on high priority channels. For example the     infrastructure monitorization one. Agree with your team to     write as less as possible.</li> <li>Use notifications when mentioned on group rooms: Don't get notified on any     message unless they add your name on it.</li> <li>Use notifications on direct messages: Decide which people are important enough     to activate the notifications.</li> </ul> <p>Sometimes the client applications don't give enough granularity, or you would like to show notifications based on more complex conditions, that's why I created the seed project to improve the notification management in Linux.</p>"}, {"location": "interruption_management/", "title": "Interruption Management", "text": "<p>Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions.</p> <p>We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning.</p> <p>Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels.</p>"}, {"location": "interruption_management/#interruption-analysis", "title": "Interruption analysis", "text": "<p>The interruption analysis is the main input to do interruption management. With it you consider what are the sources of the interruptions, and for each of them you classify the different source events in categories evaluating:</p> <ul> <li>How many interruption events does the source or category create.</li> <li>How many of the events require an action, and if it can be automated.</li> <li>How many hold information that don't need any action, and what do you want to     do with that information.</li> <li>How many could be automatically filtered out.</li> <li>What priority do the events have, and if it's the same for all events.</li> <li>How long can the associated action be delayed.</li> </ul> <p>Once you have that list, think if you can reduce it. Can you merge or directly remove one of the sources? The less channels to check, the better.</p> <p>Then think which of them you have no control over and think of ways to regain it. If you decide when to address the interruptions, your mind will have less load and will perform better when you're actually working.</p> <p>The ultimate goal of the analysis is to safely define the maximum amount of time you can spend without looking at the channels. Checking them continuously makes no sense, you're breaking your workflow for no good reason, as most times there is nothing new, and if there is, you feel the urge to act upon them, even though they could wait.</p> <p>In some teams, the situation doesn't allow you not to check them frequently. In those cases you can define the interruption manager role. A figure that is rotated by the team's members so that only one human needs to be monitoring the interruption channels, while the rest of them are able to work continuously on their tasks.</p> <p>If you want to see the analysis in action, check my work analysis or my personal one.</p>"}, {"location": "interruption_management/#workflow", "title": "Workflow", "text": "<p>Once you have all the interruption sources identified, classified, and defined the checking periodicity, you need to decide how to handle them.</p>"}, {"location": "interruption_management/#define-your-interruption-events", "title": "Define your interruption events", "text": "<p>To minimize the times you interrupt your workflow, aggregate the different sources and schedule when are you want to check them. For example, if the analysis gave the next sources:</p> <ul> <li>Source A: check each 4 hours.</li> <li>Source B: check each 5 hours.</li> <li>Source C: check each 20 minutes.</li> </ul> <p>You can schedule the next interruption events:</p> <ul> <li>Check sources A, B and C: when you start working, before lunch and before the     end of the day.</li> <li>Check C: after each Pomodoro     iteration.</li> </ul>"}, {"location": "interruption_management/#process-the-interruption-event-information", "title": "Process the interruption event information", "text": "<p>When an interruption event arrives, process sequentially each source following the inbox emptying guidelines.</p>"}, {"location": "issues/", "title": "Issue tracking", "text": "<p>I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track.</p>"}, {"location": "issues/#pydantic-errors", "title": "Pydantic errors", "text": "<ul> <li>No name 'BaseModel' in module 'pydantic'     (no-name-in-module),     you can find a patch in the pydantic article,     the pydantic developers took that as a solution as it lays in pylint's     roof, once that last issue is     solved try to find a better way to improve the patch solution.</li> </ul>"}, {"location": "issues/#vim-workflow-improvements", "title": "Vim workflow improvements", "text": "<p>Manually formatting paragraphs is an unproductive pain in the ass, Vim-pencil looks promising but there are still some usability issues that need to be fixed first:</p> <ul> <li>Wrong list management: #93     linked to #31 and     #95.</li> <li>Disable wrap of document     headers (less important).</li> </ul>"}, {"location": "issues/#gitea-improvements", "title": "Gitea improvements", "text": "<ul> <li>Replying discussion comments redirects to mail pull request     page: Notify the people     that it's fixed.</li> </ul>"}, {"location": "issues/#gitea-kanban-board-improvements", "title": "Gitea Kanban board improvements", "text": "<ul> <li>Remove the Default issue template:     #14383. When it's solved     apply it in the work's issue tracker.</li> </ul>"}, {"location": "issues/#docker-monitorization", "title": "Docker monitorization", "text": "<ul> <li>Integrate diun in the CI pipelines when they support prometheus     metrics. Update the     docker article too.</li> </ul>"}, {"location": "issues/#gadgetbridge-improvements", "title": "Gadgetbridge improvements", "text": "<ul> <li>Smart alarm     support: Use     it whenever it's available.</li> <li>GET Sp02 real time data, or at least export     it: See how     to use this data once it's available.</li> <li>export heart rate for activities without a GPX     track: See if     I can export the heart rate for post processing. Maybe it's covered     here.</li> <li>Add UI and logic for more complex database import, export and     merging:     Monitor to see if there are new ways or improvements of exporting data.</li> <li>Blog's RSS is not     working: Add     it to the feed reader once it does, and remove the warning from the     gadgetbridge article</li> <li>Integrate with home     assistant:     Check if the integration with kalliope is easy.</li> <li>Issues with zoom, swipe, interact with     graphs:     enable back disable swipe between tabs in the chart settings.</li> <li>PAI     implementation:     Check it once it's ready.</li> <li>Calendar synchronization     issue, could     be related with notifications work after     restart: try     it when it's solved</li> <li>Change snooze time     span: Change     the timespan from 10 to 5 minutes.</li> </ul>"}, {"location": "issues/#ombi-improvements", "title": "Ombi improvements", "text": "<ul> <li>Ebook     requests:     Configure it in the service, notify the people and start using it.</li> <li>Add working links to the details     pages:     nothing to do, just start using it.</li> <li>Allow search by     genre: Notify     the people and start using it.</li> </ul>"}, {"location": "javascript_snippets/", "title": "javascript snippets", "text": ""}, {"location": "javascript_snippets/#set-variable-if-its-undefined", "title": "Set variable if it's undefined", "text": "<pre><code>var x = (x === undefined) ? your_default_value : x;\n</code></pre>"}, {"location": "javascript_snippets/#concatenate-two-arrays", "title": "Concatenate two arrays", "text": "<pre><code>const arr1 = [\"Cecilie\", \"Lone\"];\nconst arr2 = [\"Emil\", \"Tobias\", \"Linus\"];\nconst children = arr1.concat(arr2);\n</code></pre> <p>To join more arrays you can use:</p> <pre><code>const arr1 = [\"Cecilie\", \"Lone\"];\nconst arr2 = [\"Emil\", \"Tobias\", \"Linus\"];\nconst arr3 = [\"Robin\"];\nconst children = arr1.concat(arr2,arr3);\n</code></pre>"}, {"location": "javascript_snippets/#check-if-a-variable-is-not-undefined", "title": "Check if a variable is not undefined", "text": "<pre><code>if(typeof lastname !== \"undefined\")\n{\n  alert(\"Hi. Variable is defined.\");\n}\n</code></pre>"}, {"location": "javascript_snippets/#select-a-substring", "title": "Select a substring", "text": "<pre><code>'long string'.substring(startIndex, endIndex)\n</code></pre>"}, {"location": "javascript_snippets/#round-a-number", "title": "Round a number", "text": "<pre><code>Math.round(2.5)\n</code></pre>"}, {"location": "javascript_snippets/#remove-focus-from-element", "title": "Remove focus from element", "text": "<pre><code>document.activeElement.blur();\n</code></pre>"}, {"location": "jellyfin/", "title": "Jellyfin", "text": "<p>Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it.</p>"}, {"location": "jellyfin/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "jellyfin/#wrong-image-covers", "title": "Wrong image covers", "text": "<p>Remove all the <code>jpg</code> files of the directory and then fetch again the data from your favourite media management software.</p>"}, {"location": "jellyfin/#green-bars-in-the-reproduction", "title": "Green bars in the reproduction", "text": "<p>It's related to some hardware transcoding issue related to some video codecs, the solution is to either get a file with other codec, or convert it yourself without the hardware transcoding with:</p> <pre><code>ffmpeg -i input.avi -c:v libx264 out.mp4\n</code></pre>"}, {"location": "jellyfin/#stuck-at-login-page", "title": "Stuck at login page", "text": "<p>Sometimes Jellyfin gets stuck at the login screen when trying to log in with an endlessly spinning loading wheel. It looks like it's already fixed, so first try to update to the latest version. If the error remains, follow the next steps:</p> <p>To fix it run the next snippet:</p> <pre><code>systemctl stop jellyfin.service\nmv /var/lib/jellyfin/data/jellyfin.db{,.bak}\nsystemctl start jellyfin.service\n# Go to JF URL, get asked to log in even though\n# there are no Users in the JF DB now\nsystemctl stop jellyfin.service\nmv /var/lib/jellyfin/data/jellyfin.db{.bak,}\nsystemctl start jellyfin.service\n</code></pre> <p>If you use jfa-go for the invites, you may need to regenerate all the user profiles, so that the problem is not introduced again.</p>"}, {"location": "jellyfin/#issues", "title": "Issues", "text": "<ul> <li> <p>Subtitles get delayed from the video on some devices:     1,     2,     3. There is     a feature     request for a fix. Once it's solved notify the users     once it's solved.</p> </li> <li> <p>Trailers not     working:     No solution until it's fixed</p> </li> <li> <p>Unnecessary transcoding:     nothing to do</p> </li> <li>Local social     features:     test it and see how to share rating between users.</li> <li>Skip     intro/outro/credits:     try it.</li> <li>Music star rating:     try it and plan to migrate everything to Jellyfin.</li> <li>Remove pagination/use lazy     loading:     try it.</li> <li>Support     2FA:     try it.</li> <li>Mysql server     backend:     implement it to add robustness.</li> <li>Watched history:     try it.</li> <li>A richer ePub     reader:     migrate from Polar and add jellyfin to the awesome selfhosted list.</li> <li>Prometheus     exporter:     monitor it.</li> <li>Easy Import/Export Jellyfin     settings:     add to the backup process.</li> <li>Temporary direct file sharing     links:     try it.</li> <li>Remember subtitle and audio track choice between     episodes:     try it.</li> <li>IMBD Rating and Rotten Tomatoes Audiance Rating and Fresh rating on Movies and TV Shows:     try the new ratings.</li> <li>Trailers     Plugin:     Once it's merged to the core, remove the plugin.</li> <li>Jellyfin for apple     tv: tell     the people that use the shitty device.</li> </ul>"}, {"location": "jellyfin/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li>Blog(RSS)</li> </ul>"}, {"location": "kag/", "title": "Kag", "text": "<p>King Arthur Gold, also known as KAG, is a free Medieval Build n'Kill Multiplayer Game with Destructible Environments.</p> <p>Construct freeform forts as a medieval Builder, fight in sword duels as a Knight or snipe with your bow as an Archer. KAG blends the cooperative aspects of Lost Vikings, mashes them with the full destructibility of Worms and the visual style and action of Metal Slug, brought to you by the creators of Soldat.</p>"}, {"location": "kag/#guides", "title": "Guides", "text": ""}, {"location": "kag/#archer-guides", "title": "Archer guides", "text": "<ul> <li>Turtlebutt and Bunnie</li> <li>Coroz and RedOTheWisp</li> </ul>"}, {"location": "kag/#builder-guides", "title": "Builder guides", "text": "<ul> <li>Turtlebutt and Bunnie</li> </ul>"}, {"location": "khal/", "title": "Khal", "text": "<p><code>khal</code> is a standards based Python CLI (console) calendar program, able to synchronize with CalDAV servers through <code>vdirsyncer</code>.</p> <p>Features:</p> <ul> <li>Can read and write events/icalendars to vdir, so <code>vdirsyncer</code>   can be used to synchronize calendars with a variety of other programs, for   example CalDAV servers.</li> <li>Fast and easy way to add new events</li> <li><code>ikhal</code> (interactive <code>khal</code>) lets you browse and edit calendars and events.</li> </ul> <p>Limitations:</p> <ul> <li>It's not easy to get an idea of what you need to do in the week. At least not   as comfortable as a graphical interface.</li> <li> <p>Editing events with <code>ikhal</code> is a little bit cumbersome.</p> </li> <li> <p>Only rudimentary support for creating and editing recursion rules.</p> </li> <li>You cannot edit the timezones of events.</li> </ul>"}, {"location": "khal/#installation", "title": "Installation", "text": "<p>Although it's available in the major package managers, you can get a more bleeding edge version with <code>pip</code>.</p> <pre><code>pipx install khal\n</code></pre> <p>If you don't have <code>pipx</code> you can use <code>pip</code>.</p>"}, {"location": "khal/#configuration", "title": "Configuration", "text": "<p><code>khal</code> reads configuration files in the ini syntax. If you do not have a configuration file yet, running <code>khal configure</code> will launch a small, interactive tool that should help you with initial configuration of khal.</p> <p><code>khal</code> is looking for configuration files in the following places and order:</p> <ul> <li><code>$XDG_CONFIG_HOME/khal/config</code>: (on most systems this is   <code>~/.config/khal/config</code>),</li> <li><code>~/.khal/khal.conf</code> (deprecated)</li> <li>A <code>khal.conf</code> file in the current directory (deprecated).</li> </ul> <p>Alternatively you can specify which configuration file to use with <code>-c path/to/config</code> at runtime.</p>"}, {"location": "khal/#the-calendars-section", "title": "The calendars section", "text": "<p>The <code>[calendars]</code> section is mandatory and must contain at least one subsection. Every subsection must have a unique name (enclosed by two square brackets). Each subsection needs exactly one path setting, everything else is optional. Here is a small example:</p> <pre><code>[calendars]\n\n  [[home]]\n    path = ~/.calendars/home/\n    color = dark green\n    priority = 20\n\n  [[work]]\n    path = ~/.calendars/work/\n    readonly = True\n</code></pre> <p>Some properties are:</p> <ul> <li><code>path</code>: The path to an existing directory where this calendar is saved as a   vdir.</li> <li><code>color</code>: <code>khal</code> will use this color for coloring this calendar\u2019s event. The   following color names are supported: <code>black</code>, <code>white</code>, <code>brown</code>, <code>yellow</code>,   <code>dark gray</code>, <code>dark green</code>, <code>dark blue</code>, <code>light gray</code>, <code>light green</code>,   <code>light   blue</code>, <code>dark magenta</code>, <code>dark cyan</code>, <code>dark red</code>, <code>light magenta</code>,   <code>light   cyan</code>, <code>light red</code>.</li> <li><code>priority</code>: When coloring days, the color will be determined based on the   calendar with the highest priority. If the priorities are equal, then the   \u201cmultiple\u201d color will be used.</li> <li><code>readonly</code>: Setting this to True, will keep <code>khal</code> from making any changes to   this calendar.</li> </ul>"}, {"location": "khal/#the-default-section", "title": "The default section", "text": "<p>Some of this configurations do not affect <code>ikhal</code>.</p> <ul> <li><code>default_calendar</code>: The calendar to use if none is specified for some   operation (e.g. if adding a new event). If this is not set, such operations   require an explicit value.</li> <li><code>default_dayevent_duration</code>: Define the default duration for an event   (<code>khal   new</code> only). <code>1h</code> by default.</li> <li><code>default_event_duration</code>: Define the default duration for a day-long event   (<code>khal  new</code> only). <code>1d</code> by default.</li> <li><code>highlight_event_days</code>: If true, <code>khal</code> will highlight days with events.   Options for highlighting are in   highlight_days   section.</li> </ul>"}, {"location": "khal/#the-key-bindings-section", "title": "The key bindings section", "text": "<p>Key bindings for <code>ikhal</code> are set here. You can bind more than one key (combination) to a command by supplying a comma-separated list of keys. For binding key combinations concatenate them keys (with a space in between), for example <code>ctrl n</code>.</p> Action Default Description down down, j Move the cursor down (in the calendar browser). up up, k Move the cursor up (in the calendar browser). left left, h, backspace Move the cursor left (in the calendar browser). right right, l, space Move the cursor right (in the calendar browser). view enter Show details or edit (if details are already shown) the currently selected event. save meta enter Save the currently edited event and leave the event editor. quit q, Q Quit. new n Create a new event on the selected date. delete d Delete the currently selected event. search / Open a text field to start a search for events. mark v Go into highlight (visual) mode to choose a date range. other o In highlight mode go to the other end of the highlighted date range. today t Focus the calendar browser on today. duplicate p Duplicate the currently selected event. export e Export event as a .ics file. log L Show logged messages. external_edit meta E Edit the currently selected events\u2019 raw .ics file with $EDITOR <p>Use the <code>external_edit</code> with caution, the icalendar library we use doesn't do a lot of validation, it silently disregards most invalid data.</p>"}, {"location": "khal/#syncing", "title": "Syncing", "text": "<p>To get <code>khal</code> working with CalDAV you will first need to setup <code>vdirsyncer</code>. After each start <code>khal</code> will automatically check if anything has changed and automatically update its caching db (this may take some time after the initial sync, especially for large calendar collections). Therefore, you might want to execute <code>khal</code> automatically after syncing with <code>vdirsyncer</code> (for example via <code>cron</code>).</p>"}, {"location": "khal/#usage", "title": "Usage", "text": "<p><code>khal</code> offers a set of commands, most importantly:</p> <ul> <li><code>list</code>: Shows all events scheduled for a given date (or datetime) range, with   custom formatting.</li> <li><code>calendar</code>: Shows a calendar (similar to cal(1)) and list.</li> <li><code>new</code>: Allows for adding new events.</li> <li><code>search</code>: Search for events matching a search string and print them.</li> <li><code>at</code>: shows all events scheduled for a given datetime.</li> <li><code>edit</code>: An   interactive command for editing and deleting events using a search string.</li> <li><code>interactive</code>: Invokes the interactive version of <code>khal</code>, can also be invoked   by calling <code>ikhal</code>.</li> <li><code>printcalendars</code>:</li> <li><code>printformats</code></li> </ul>"}, {"location": "khal/#new", "title": "new", "text": "<pre><code>khal new [-a CALENDAR] [OPTIONS] [START [END | DELTA] [TIMEZONE] SUMMARY\n[:: DESCRIPTION]]\n</code></pre> <p>Where <code>start</code> and <code>end</code> are either datetimes, times, or keywords and times in the formats defined in the config file.</p> <p>If no calendar is given via <code>-a</code>, the default calendar is used.</p> <p>For example:</p> <pre><code>khal new 18:00 Awesome Event\n</code></pre> <p>Adds a new event starting today at 18:00 with summary <code>Awesome event</code> (lasting for the default time of one hour) to the default calendar.</p> <pre><code>khal new tomorrow 16:30 Coffee Break\n</code></pre> <p>Adds a new event tomorrow at 16:30.</p> <pre><code>khal new 25.10. 18:00 24:00 Another Event :: with Alice and Bob\n</code></pre> <p>Adds a new event on 25<sup>th</sup> of October lasting from 18:00 to 24:00 with an additional description.</p> <pre><code>khal new -a work 26.07. Great Event -g meeting -r weekly\n</code></pre> <p>Adds a new all day event on 26<sup>th</sup> of July to the calendar work in the meeting category, which recurs every week.</p>"}, {"location": "khal/#interactive", "title": "Interactive", "text": "<p>When the calendar on the left is in focus, you can:</p> <ul> <li> <p>Move through the calendar (default keybindings are the arrow keys, space and   backspace, those keybindings are configurable in the config file).</p> </li> <li> <p>Focus on the right column by pressing <code>tab</code> or <code>enter</code>.</p> </li> <li> <p>Focus on the current date, default keybinding <code>t</code> as in today.</p> </li> <li> <p>Marking a date range, default keybinding <code>v</code>, as in visual, think visual mode   in Vim, pressing <code>esc</code> escapes this visual mode.</p> </li> </ul> <p>If in visual mode, you can select the other end of the currently marked range,   default keybinding <code>o</code> as in other (again as in Vim).</p> <ul> <li> <p>Create a new event on the currently focused day (or date range if a range is   selected), default keybinding <code>n</code>.</p> </li> <li> <p>Search for events, default keybinding <code>/</code>, a pop-up will ask for your search   term.</p> </li> </ul> <p>When an event list is in focus, you can:</p> <ul> <li>View an event\u2019s details with pressing enter (or tab) and edit it with pressing   enter (or tab) again (if [view] <code>event_view_always_visible</code> is set to   <code>True</code>, the event in focus will always be shown in detail).</li> <li>Toggle an event\u2019s deletion status, default keybinding <code>d</code>, events marked for   deletion will appear with a <code>D</code> in front and will be deleted when <code>khal</code>   exits.</li> <li>Duplicate the selected event, default keybinding <code>p</code></li> <li>Export the selected event, default keybinding <code>e</code>.</li> </ul> <p>In the event editor, you can:</p> <ul> <li>Jump to the next (previous) selectable element with pressing <code>tab</code> (shift+tab)</li> <li>Quick save, default keybinding <code>meta+enter</code> (meta will probably be alt).</li> <li>Use some common editing short cuts in most text fields (ctrl+w deletes word   before cursor, ctrl+u (ctrl+k) deletes till the beginning (end) of the line,   ctrl+a (ctrl+e) will jump to the beginning (end) of the line.</li> <li>In the date and time fields you can increment and decrement the number under   the cursor with <code>ctrl+a</code> and <code>ctrl+x</code> (time in 15 minute steps)</li> <li>In the date fields you can access a miniature calendar by pressing enter.</li> <li>Activate actions by pressing enter on text enclosed by angled brackets, e.g.   \\&lt; Save &gt; (sometimes this might open a pop up).</li> </ul> <p>Pressing <code>esc</code> will cancel the current action and/or take you back to the previously shown pane (i.e. what you see when you open ikhal), if you are at the start pane, ikhal will quit on pressing esc again.</p>"}, {"location": "khal/#tricks", "title": "Tricks", "text": ""}, {"location": "khal/#edit-the-events-in-a-more-pleasant-way", "title": "Edit the events in a more pleasant way", "text": "<p>The <code>ikhal</code> event editor is not comfortable for me. I usually only change the title or the start date and in the default interface you need to press many keystrokes to make it happen.</p> <p>A patch solution is to pass a custom script on the <code>EDITOR</code> environmental variable. Assuming you have <code>questionary</code> and <code>ics</code> installed you can save the next snippet into an <code>edit_event</code> file in your <code>PATH</code>:</p> <pre><code>#!/usr/bin/python3\n\n\"\"\"Edit an ics calendar event.\"\"\"\n\nimport sys\n\nimport questionary\nfrom ics import Calendar\n\n# Load the event\nfile = sys.argv[1]\nwith open(file, \"r\") as fd:\n    calendar = Calendar(fd.read())\nevent = list(calendar.timeline)[0]\n\n# Modify the event\nevent.name = questionary.text(\"Title: \", default=event.name).ask()\nstart = questionary.text(\n    \"Start: \",\n    default=f\"{str(event.begin.hour).zfill(2)}:{str(event.begin.minute).zfill(2)}\",\n).ask()\nevent.begin = event.begin.replace(\n    hour=int(start.split(\":\")[0]), minute=int(start.split(\":\")[1])\n)\n\n# Save the event\nwith open(file, \"w\") as fd:\n    fd.writelines(calendar.serialize_iter())\n</code></pre> <p>Now if you open <code>ikhal</code> as <code>EDITOR=edit_event ikhal</code>, whenever you edit one event you'll get a better interface. Add to your <code>.zshrc</code> or <code>.bashrc</code>:</p> <pre><code>alias ikhal='EDITOR=edit_event ikhal'\n</code></pre> <p>The default keybinding for the edition is not very comfortable either, add the next snippet on your config:</p> <pre><code>[keybindings]\nexternal_edit = e\nexport = meta e\n</code></pre>"}, {"location": "khal/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "kitty/", "title": "kitty", "text": "<p>kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me.</p>"}, {"location": "kitty/#installation", "title": "Installation", "text": "<p>Although it's in the official repos, the version of Debian is quite old, instead you can install it for the current user with:</p> <pre><code>curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin\n</code></pre> <p>You'll need to add the next alias too to your <code>.zshrc</code> or <code>.bashrc</code></p> <pre><code>alias kitty=\"~/.local/kitty.app/bin/kitty\"\n</code></pre>"}, {"location": "kitty/#configuration", "title": "Configuration", "text": "<p>It's configuration is a simple, human editable, single file for easy reproducibility stored at <code>~/.config/kitty/kitty.conf</code></p>"}, {"location": "kitty/#print-images-in-the-terminal", "title": "Print images in the terminal", "text": "<p>Create an alias in your <code>.zshrc</code>:</p> <pre><code>alias icat=\"kitty +kitten icat\"\n</code></pre> <p></p>"}, {"location": "kitty/#colors", "title": "Colors", "text": "<p>The themes kitten allows you to easily change color themes, from a collection of almost two hundred pre-built themes available at kitty-themes. To use it run:</p> <pre><code>kitty +kitten themes\n</code></pre> <p>The kitten allows you to pick a theme, with live previews of the colors. You can choose between light and dark themes and search by theme name by just typing a few characters from the name.</p> <p>If you want to tweak some colors once you select a theme, you can use terminal sexy.</p>"}, {"location": "kitty/#make-the-background-transparent", "title": "Make the background transparent", "text": "<p>File: <code>~/.config/kitty/kitty.conf</code></p> <pre><code>background_opacity 0.85\n</code></pre> <p>A number between 0 and 1, where 1 is opaque and 0 is fully transparent. This will only work if supported by the OS (for instance, when using a compositor under X11).</p> <p>If you're using i3wm you need to configure compton</p> <p>Install it with <code>sudo apt-get install compton</code>, and configure i3 to start it in the background adding <code>exec --no-startup-id compton</code> to your i3 config.</p>"}, {"location": "kitty/#terminal-bell", "title": "Terminal bell", "text": "<p>I hate the auditive terminal bell, disable it with:</p> <pre><code>enable_audio_bell no\n</code></pre>"}, {"location": "kitty/#movement", "title": "Movement", "text": "<p>By default the movement is not vim friendly because if you use the same keystrokes, they will be captured by kitty and not forwarded to the application. The closest I got is:</p> <pre><code># Movement\n\nmap ctrl+shift+k scroll_line_up\nmap ctrl+shift+j scroll_line_down\nmap ctrl+shift+u scroll_page_up\nmap ctrl+shift+d scroll_page_down\n</code></pre> <p>If you need more fine grained movement, use the scrollback buffer.</p>"}, {"location": "kitty/#the-scrollback-buffer", "title": "The scrollback buffer", "text": "<p><code>kitty</code> supports scrolling back to view history, just like most terminals. You can use either keyboard shortcuts or the mouse scroll wheel to do so. However, kitty has an extra, neat feature. Sometimes you need to explore the scrollback buffer in more detail, maybe search for some text or refer to it side-by-side while typing in a follow-up command. kitty allows you to do this by pressing the <code>ctrl+shift+h</code> key-combination, which will open the scrollback buffer in your favorite pager program (which is less by default). Colors and text formatting are preserved. You can explore the scrollback buffer comfortably within the pager.</p> <p>To use <code>nvim</code> as the pager follow this discussion, the latest working snippet was:</p> <pre><code># Scrollback buffer\n# https://sw.kovidgoyal.net/kitty/overview/#the-scrollback-buffer\n# `bash -c '...'` Run everything in a shell taking the scrollback content on stdin\n# `-u NORC` Load plugins but not initialization files\n# `-c \"map q :qa!&lt;CR&gt;\"` Close with `q` key\n# `-c \"autocmd TermOpen * normal G\"` On opening of the embedded terminal go to last line\n# `-c \"terminal cat /proc/$$/fd/0 -\"` Open the embedded terminal and read stdin of the shell\n# `-c \"set clipboard+=unnamedplus\"` Always use clipboard to yank/put instead of having to specify +\nscrollback_pager bash -c 'nvim &lt;/dev/null -u NORC -c \"map q :qa!&lt;CR&gt;\" -c \"autocmd TermOpen * normal G\" -c \"terminal cat /proc/$$/fd/0 -\" -c \"set clipboard+=unnamedplus\" -c \"call cursor(CURSOR_LINE, CURSOR_COLUMN)\"'\n</code></pre> <p>To make the history scrollback infinite add the next lines:</p> <pre><code>scrollback_lines -1\nscrollback_pager_history_size 0\n</code></pre>"}, {"location": "kitty/#clipboard-management", "title": "Clipboard management", "text": "<pre><code># Clipboard\nmap ctrl+v        paste_from_clipboard\n</code></pre>"}, {"location": "kitty/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "kitty/#scrollback-when-ssh-into-a-machine-doesnt-work", "title": "Scrollback when ssh into a machine doesn't work", "text": "<p>This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server:</p> <pre><code>kitty +kitten ssh myserver\n</code></pre> <p>This ssh kitten takes all the same command line arguments as ssh, you can alias it to ssh in your shell\u2019s rc files to avoid having to type it each time:</p> <pre><code>alias ssh=\"kitty +kitten ssh\"\n</code></pre>"}, {"location": "kitty/#reasons-to-migrate-from-urxvt-to-kitty", "title": "Reasons to migrate from urxvt to kitty", "text": "<ul> <li>It doesn't fuck up your terminal colors.</li> <li>You can use peek to record your screen.</li> <li>Easier to extend.</li> </ul>"}, {"location": "kitty/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "krew/", "title": "Krew", "text": "<p>Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew.</p>"}, {"location": "krew/#installation", "title": "Installation", "text": "<ol> <li> <p>Run this command to download and install krew:</p> <pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> </li> <li> <p>Add the <code>$HOME/.krew/bin</code> directory to your PATH environment variable. To do    this, update your <code>.bashrc</code> or <code>.zshrc</code> file and append the following line:</p> </li> </ol> <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre> <ol> <li> <p>Restart your shell.</p> </li> <li> <p>Run <code>kubectl krew</code> to check the installation.</p> </li> </ol>"}, {"location": "krew/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "ksniff/", "title": "Ksniff", "text": "<p>Ksniff is a Kubectl plugin to ease sniffing on kubernetes pods using tcpdump and wireshark.</p>"}, {"location": "ksniff/#installation", "title": "Installation", "text": "<p>Recommended installation is done via krew</p> <pre><code>kubectl krew install sniff\n</code></pre> <p>For manual installation, download the latest release package, unzip it and use the attached makefile:</p> <pre><code>unzip ksniff.zip\nmake install\n</code></pre> <p>(I tried doing it manually and it failed for me).</p>"}, {"location": "ksniff/#usage", "title": "Usage", "text": "<pre><code>kubectl sniff &lt;POD_NAME&gt; [-n &lt;NAMESPACE_NAME&gt;] [-c &lt;CONTAINER_NAME&gt;] [-i &lt;INTERFACE_NAME&gt;] [-f &lt;CAPTURE_FILTER&gt;] [-o OUTPUT_FILE] [-l LOCAL_TCPDUMP_FILE] [-r REMOTE_TCPDUMP_FILE]\n\nPOD_NAME: Required. the name of the kubernetes pod to start capture it's traffic.\nNAMESPACE_NAME: Optional. Namespace name. used to specify the target namespace to operate on.\nCONTAINER_NAME: Optional. If omitted, the first container in the pod will be chosen.\nINTERFACE_NAME: Optional. Pod Interface to capture from. If omitted, all Pod interfaces will be captured.\nCAPTURE_FILTER: Optional. specify a specific tcpdump capture filter. If omitted no filter will be used.\nOUTPUT_FILE: Optional. if specified, ksniff will redirect tcpdump output to local file instead of wireshark. Use '-' for stdout.\nLOCAL_TCPDUMP_FILE: Optional. if specified, ksniff will use this path as the local path of the static tcpdump binary.\nREMOTE_TCPDUMP_FILE: Optional. if specified, ksniff will use the specified path as the remote path to upload static tcpdump to.\n</code></pre> <p>You'll need to remove the pods manually once you've finished analyzing the traffic.</p>"}, {"location": "ksniff/#issues", "title": "Issues", "text": ""}, {"location": "ksniff/#wtap_encap-0", "title": "<code>WTAP_ENCAP = 0</code>", "text": "<p>Upgrade your wireshark to a version greater or equal to <code>3.3.0</code>.</p>"}, {"location": "ksniff/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "kubernetes_debugging/", "title": "Kubernetes Debugging", "text": ""}, {"location": "kubernetes_debugging/#network-debugging", "title": "Network debugging", "text": "<p>Sometimes you need to monitor the network traffic that goes between pods to solve an issue. There are different ways to see it:</p> <ul> <li>Using Mizu</li> <li>Running tcpdump against a running container</li> <li>Using ksniff</li> <li>Using ephemeral debug containers</li> </ul> <p>Of all the solutions, the cleaner and easier is to use Mizu.</p>"}, {"location": "kubernetes_debugging/#running-tcpdump-against-a-running-container", "title": "Running tcpdump against a running container", "text": "<p>If the pod you want to analyze has root permissions (bad idea) you'll be able to install <code>tcpdump</code> (<code>apt-get install tcpdump</code>) and pipe it into <code>wireshark</code> on your local machine.</p> <pre><code>kubectl exec my-app-pod -- tcpdump -i eth0 -w - | wireshark -k -i -\n</code></pre> <p>There's some issues with this, though:</p> <ul> <li>You have to <code>kubectl exec</code> and install arbitrary software from the internet on     a running Pod. This is fine for internet-connected dev environments, but     probably not something you'd want to do (or be able to do) in production.</li> <li>If this app had been using a minimal <code>distroless</code> base image or was built with     a <code>buildpack</code> you won't be able to install <code>tcpdump</code>.</li> </ul>"}, {"location": "kubernetes_debugging/#using-ephemeral-debug-containers", "title": "Using ephemeral debug containers", "text": "<p>Kubernetes 1.16 has a new Ephemeral Containers feature that is perfect for our use case. With Ephemeral Containers, we can ask for a new temporary container with the image of our choosing to run inside an existing Pod. This means we can keep the main images for our applications lightweight and then bolt on a heavy image with all of our favorite debug tools when necessary.</p> <pre><code>kubectl debug -it pod-to-debug-id --image=nicolaka/netshoot --target=pod-to-debug -- tcpdump -i eth0 -w - | wireshark -k -i\n</code></pre> <p>Where <code>nicolaka/netshoot</code> is an optimized network troubleshooting docker.</p> <p>There's some issues with this too, for example, as of Kubernetes 1.21 Ephemeral containers are not enabled by default, so chances are you won't have access to them yet in your environment.</p>"}, {"location": "lazy_loading/", "title": "Lazy evaluation", "text": "<p>Lazy loading is an programming implementation paradigm which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations.</p> <p>Lazy evaluation is the preferred implementation when the operation is expensive, requiring either extensive processing time or memory. For example, in Python, one of the best-known techniques involving lazy evaluation is generators. Instead of creating whole sequences for the iteration, which can consume lots of memory, generators lazily evaluate the current need and yield one element at a time when requested.</p> <p>Other example are attributes that take long to compute:</p> <pre><code>class Person:\n    def __init__(self, name, occupation):\n        self.name = name\n        self.occupation = occupation\n        self.relatives = self._get_all_relatives()\n\n    def _get_all_relatives():\n        ...\n        # This is an expensive operation\n</code></pre> <p>This approach may cause initialization to take unnecessarily long, especially when you don't always need to access <code>Person.relatives</code>.</p> <p>A better strategy would be to get relatives when it's needed.</p> <pre><code>class Person:\n    def __init__(self, name, occupation):\n        self.name = name\n        self.occupation = occupation\n        self._relatives = None\n\n    @property\n    def relatives(self):\n        if self._relatives is None:\n            self._relatives = ... # Get all relatives\n        return self._relatives\n</code></pre> <p>In this case, the list of relatives is computed the first time <code>Person.relatives</code> is accessed. After that, it's stored in <code>Person._relatives</code> to prevent repeated evaluations.</p> <p>A perhaps more Pythonic approach would be to use a decorator that makes a property lazy-evaluated.</p> <pre><code>def lazy_property(fn):\n    '''Decorator that makes a property lazy-evaluated.\n    '''\n    attr_name = '_lazy_' + fn.__name__\n\n    @property\n    def _lazy_property(self):\n        if not hasattr(self, attr_name):\n            setattr(self, attr_name, fn(self))\n        return getattr(self, attr_name)\n    return _lazy_property\n\nclass Person:\n    def __init__(self, name, occupation):\n        self.name = name\n        self.occupation = occupation\n\n    @lazy_property\n    def relatives(self):\n        # Get all relatives\n        relatives = ...\n        return relatives\n</code></pre> <p>This removes a lot of boilerplate, especially when an object has many lazily-evaluated properties.</p> <p>Another approach is to use the getattr special method.</p>"}, {"location": "lazy_loading/#references", "title": "References", "text": "<ul> <li>Steven Loria article on Lazy Properties</li> <li>Yong Cui article on Lazy attributes</li> </ul>"}, {"location": "libreelec/", "title": "LibreElec", "text": "<p>LibreElec is the lightweight distribution to run Kodi</p> <p>The root filesystem is mounted as readonly.</p>"}, {"location": "libreelec/#mount-directories-with-sshfs", "title": "Mount directories with sshfs", "text": "<ul> <li>Install the network-tool LibreElec addon.</li> <li>Configure the ssh credentials</li> <li>Add the following service file:     <code>/storage/.config/system.d/storage-media.mount</code></li> </ul> <pre><code>[Unit]\nDescription=remote external drive share\nRequires=multi-user.target network-online.service\nAfter=multi-user.target network-online.service\nBefore=kodi.service\n\n[Mount]\nWhat=/storage/.kodi/addons/virtual.network-tools/bin/sshfs#{{ user }}@{{ host }}:{{ source_path }}\nWhere=/storage/media\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"}, {"location": "life_management/", "title": "Life Management", "text": "<p>I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals.</p>"}, {"location": "linux_snippets/", "title": "Linux snippets", "text": ""}, {"location": "linux_snippets/#df-and-du-showing-different-results", "title": "df and du showing different results", "text": "<p>Sometimes on a linux machine you will notice that both <code>df</code> command (display free disk space) and <code>du</code> command (display disk usage statistics) report different output. Usually, <code>df</code> will output a bigger disk usage than <code>du</code>.</p> <p>The <code>du</code> command estimates file space usage, and the <code>df</code> command shows file system disk space usage.</p> <p>There are many reasons why this could be happening:</p>"}, {"location": "linux_snippets/#disk-mounted-over-data", "title": "Disk mounted over data", "text": "<p>If you mount a disk on a directory that already holds data, then when you run <code>du</code> that data won't show, but <code>df</code> knows it's there.</p> <p>To troubleshoot this, umount one by one of your disks, and do an <code>ls</code> to see if there's any remaining data in the mount point.</p>"}, {"location": "linux_snippets/#used-deleted-files", "title": "Used deleted files", "text": "<p>When a file is deleted under Unix/Linux, the disk space occupied by the file will not be released immediately in some cases. The result of the command <code>du</code> doesn\u2019t include the size of the deleting file. But the impact of the command <code>df</code> for the deleting file\u2019s size due to its disk space is not released immediately. Hence, after deleting the file, the results of <code>df</code> and <code>du</code> are different until the disk space is freed.</p> <p>Open file descriptor is main causes of such wrong information. For example, if a file called <code>/tmp/application.log</code> is open by a third-party application OR by a user and the same file is deleted, both <code>df</code> and <code>du</code> report different outputs. You can use the <code>lsof</code> command to verify this:</p> <pre><code>lsof | grep tmp\n</code></pre> <p>To fix it:</p> <ul> <li>Use the <code>lsof</code> command as discussed above to find a deleted file opened by   other users and apps. See how to list all users in the system for more info.</li> <li>Then, close those apps and log out of those Linux and Unix users.</li> <li>As a sysadmin you restart any process or <code>kill</code> the process under Linux and   Unix that did not release the deleted file.</li> <li>Flush the filesystem using the <code>sync</code> command that synchronizes cached writes   to persistent disk storage.</li> <li>If everything else fails, try restarting the system using the <code>reboot</code> command   or <code>shutdown</code> command.</li> </ul>"}, {"location": "linux_snippets/#scan-a-physical-page-in-linux", "title": "Scan a physical page in Linux", "text": "<p>Install <code>xsane</code> and run it.</p>"}, {"location": "linux_snippets/#git-checkout-to-main-with-master-as-a-fallback", "title": "Git checkout to main with master as a fallback", "text": "<p>I usually use the alias <code>gcm</code> to change to the main branch of the repository, given the change from main to master now I have some repos that use one or the other, but I still want <code>gcm</code> to go to the correct one. The solution is to use:</p> <pre><code>alias gcm='git checkout \"$(git symbolic-ref refs/remotes/origin/HEAD | cut -d'/' -f4)\"'\n</code></pre>"}, {"location": "linux_snippets/#create-qr-code", "title": "Create QR code", "text": "<pre><code>qrencode -o qrcode.png 'Hello World!'\n</code></pre>"}, {"location": "linux_snippets/#trim-silences-of-sound-files", "title": "Trim silences of sound files", "text": "<p>To trim all silence longer than 2 seconds down to only 2 seconds long.</p> <pre><code>sox in.wav out6.wav silence -l 1 0.1 1% -1 2.0 1%\n</code></pre> <p>Note that SoX does nothing to bits of silence shorter than 2 seconds.</p> <p>If you encounter the <code>sox FAIL formats: no handler for file extension 'mp3'</code> error you'll need to install the <code>libsox-fmt-all</code> package.</p>"}, {"location": "linux_snippets/#adjust-the-replay-gain-of-many-sound-files", "title": "Adjust the replay gain of many sound files", "text": "<pre><code>sudo apt-get install python-rgain\nreplaygain -f *.mp3\n</code></pre>"}, {"location": "linux_snippets/#check-vulnerabilities-in-nodejs-applications", "title": "Check vulnerabilities in Node.js applications", "text": "<p>With <code>yarn audit</code> you'll see the vulnerabilities, with <code>yarn outdated</code> you can see the packages that you need to update.</p>"}, {"location": "linux_snippets/#check-vulnerabilities-in-rails-dependencies", "title": "Check vulnerabilities in rails dependencies", "text": "<pre><code>gem install bundler-audit\ncd project_with_gem_lock\nbundler-audit\n</code></pre>"}, {"location": "linux_snippets/#create-basic-auth-header", "title": "Create Basic Auth header", "text": "<pre><code>$ echo -n user:password | base64\ndXNlcjpwYXNzd29yZA==\n</code></pre> <p>Without the <code>-n</code> it won't work well.</p>"}, {"location": "linux_snippets/#install-one-package-from-debian-unstable", "title": "Install one package from Debian unstable", "text": "<ul> <li>Add the <code>unstable</code> repository to your <code>/etc/apt/sources.list</code></li> </ul> <pre><code># Unstable\ndeb http://deb.debian.org/debian/ unstable main contrib non-free\ndeb-src http://deb.debian.org/debian/ unstable main contrib non-free\n</code></pre> <ul> <li>Configure <code>apt</code> to only use <code>unstable</code> when specified</li> </ul> <p>!!! note \"File: <code>/etc/apt/preferences</code>\" ``` Package: * Pin: release a=stable Pin-Priority: 700</p> <pre><code>Package: *\nPin: release  a=testing\nPin-Priority: 600\n\nPackage: *\nPin: release a=unstable\nPin-Priority: 100\n```\n</code></pre> <ul> <li>Update the package data with <code>apt-get update</code>.</li> <li>See that the new versions are available with   <code>apt-cache policy   &lt;package_name&gt;</code></li> <li>To install a package from unstable you can run   <code>apt-get install -t unstable   &lt;package_name&gt;</code>.</li> </ul>"}, {"location": "linux_snippets/#fix-the-following-packages-have-been-kept-back", "title": "Fix the following packages have been kept back", "text": "<pre><code>sudo apt-get --with-new-pkgs upgrade\n</code></pre>"}, {"location": "linux_snippets/#monitor-outgoing-traffic", "title": "Monitor outgoing traffic", "text": ""}, {"location": "linux_snippets/#easy-and-quick-way-watch-lsof", "title": "Easy and quick way watch &amp; lsof", "text": "<p>You can simply use a combination of <code>watch</code> &amp; <code>lsof</code> command in Linux to get an idea of outgoing traffic on specific ports. Here is an example of outgoing traffic on ports <code>80</code> and <code>443</code>.</p> <pre><code>$ watch -n1 lsof -i TCP:80,443\n</code></pre> <p>Here is a sample output.</p> <pre><code>dropbox    2280 saml   23u  IPv4 56015285      0t0  TCP www.example.local:56003-&gt;snt-re3-6c.sjc.dropbox.com:http (ESTABLISHED)\nthunderbi  2306 saml   60u  IPv4 56093767      0t0  TCP www.example.local:34788-&gt;ord08s09-in-f20.1e100.net:https (ESTABLISHED)\nmono       2322 saml   15u  IPv4 56012349      0t0  TCP www.example.local:54018-&gt;204-62-14-135.static.6sync.net:https (ESTABLISHED)\nchrome    4068 saml  175u  IPv4 56021419      0t0  TCP www.example.local:42182-&gt;stackoverflow.com:http (ESTABLISHED)\n</code></pre> <p>You'll miss the short lived connections though.</p>"}, {"location": "linux_snippets/#fine-grained-with-tcpdump", "title": "Fine grained with tcpdump", "text": "<p>You can also use <code>tcpdump</code> command to capture all raw packets, on all interfaces, on all ports, and write them to file.</p> <pre><code>sudo tcpdump -tttt -i any -w /tmp/http.log\n</code></pre> <p>Or you can limit it to a specific port adding the arguments <code>port 443 or 80</code>. The <code>-tttt</code> flag is used to capture the packets with a human readable timestamp.</p> <p>To read the recorded information, run the <code>tcpdump</code> command with <code>-A</code> option. It will print ASCII text in recorded packets, that you can browse using page up/down keys.</p> <pre><code>tcpdump -A -r /tmp/http.log | less\n</code></pre> <p>However, <code>tcpdump</code> cannot decrypt information, so you cannot view information about HTTPS requests in it.</p>"}, {"location": "linux_snippets/#clean-up-system-space", "title": "Clean up system space", "text": ""}, {"location": "linux_snippets/#clean-package-data", "title": "Clean package data", "text": "<p>There is a couple of things to do when we want to free space in a no-brainer way. First, we want to remove those deb packages that get cached every time we do <code>apt-get install</code>.</p> <pre><code>apt-get clean\n</code></pre> <p>Also, the system might keep packages that were downloaded as dependencies but are not needed anymore. We can get rid of them with</p> <pre><code>apt-get autoremove\n</code></pre> <p>Remove data of unpurged packages.</p> <pre><code>sudo apt-get purge $(dpkg -l | grep '^rc' | awk '{print $2}')\n</code></pre> <p>If we want things tidy, we must know that whenever we <code>apt-get remove</code> a package, the configuration will be kept in case we want to install it again. In most cases we want to use <code>apt-get purge</code>. To clean those configurations from removed packages, we can use</p> <pre><code>dpkg --list | grep \"^rc\" | cut -d \" \" -f 3 | xargs --no-run-if-empty sudo dpkg --purge\n</code></pre> <p>So far we have not uninstalled anything. If now we want to inspect what packages are consuming the most space, we can type</p> <pre><code>dpkg-query -Wf '${Installed-Size}\\t${Package}\\n' | sort -n\n</code></pre>"}, {"location": "linux_snippets/#clean-snap-data", "title": "Clean snap data", "text": "<p>If you're using <code>snap</code> you can clean space by:</p> <ul> <li> <p>Reduce the number of versions kept of a package with   <code>snap set system refresh.retain=2</code></p> </li> <li> <p>Remove the old versions with <code>clean_snap.sh</code></p> </li> </ul> <pre><code>#!/bin/bash\n#Removes old revisions of snaps\n#CLOSE ALL SNAPS BEFORE RUNNING THIS\nset -eu\nLANG=en_US.UTF-8 snap list --all | awk '/disabled/{print $1, $3}' |\nwhile read snapname revision; do\n    snap remove \"$snapname\" --revision=\"$revision\"\ndone\n</code></pre>"}, {"location": "linux_snippets/#clean-journalctl-data", "title": "Clean journalctl data", "text": "<ul> <li>Check how much space it's using: <code>journalctl --disk-usage</code></li> <li>Rotate the logs: <code>journalctl --rotate</code></li> </ul> <p>Then you have three ways to reduce the data:</p> <ol> <li>Clear journal log older than X days: <code>journalctl --vacuum-time=2d</code></li> <li>Restrict logs to a certain size: <code>journalctl --vacuum-size=100M</code></li> <li>Restrict number of log files: <code>journactl --vacuum-files=5</code>.</li> </ol> <p>The operations above will affect the logs you have right now, but it won't solve the problem in the future. To let <code>journalctl</code> know the space you want to use open the <code>/etc/systemd/journald.conf</code> file and set the <code>SystemMaxUse</code> to the amount you want (for example <code>1000M</code> for a gigabyte). Once edited restart the service with <code>sudo systemctl restart systemd-journald</code>.</p>"}, {"location": "linux_snippets/#clean-up-docker-data", "title": "Clean up docker data", "text": "<p>To remove unused <code>docker</code> data you can run <code>docker system prune -a</code>. This will remove:</p> <ul> <li>All stopped containers</li> <li>All networks not used by at least one container</li> <li>All images without at least one container associated to them</li> <li>All build cache</li> </ul> <p>Sometimes that's not enough, and your <code>/var/lib/docker</code> directory still weights more than it should. In those cases:</p> <ul> <li>Stop the <code>docker</code> service.</li> <li>Remove or move the data to another directory</li> <li>Start the <code>docker</code> service.</li> </ul> <p>In order not to loose your persisted data, you need to configure your dockers to mount the data from a directory that's not within <code>/var/lib/docker</code>.</p>"}, {"location": "linux_snippets/#set-up-docker-logs-rotation", "title": "Set up docker logs rotation", "text": "<p>By default, the stdout and stderr of the container are written in a JSON file located in <code>/var/lib/docker/containers/[container-id]/[container-id]-json.log</code>. If you leave it unattended, it can take up a large amount of disk space.</p> <p>If this JSON log file takes up a significant amount of the disk, we can purge it using the next command.</p> <pre><code>truncate -s 0 &lt;logfile&gt;\n</code></pre> <p>We could setup a cronjob to purge these JSON log files regularly. But for the long term, it would be better to setup log rotation. This can be done by adding the following values in <code>/etc/docker/daemon.json</code>.</p> <pre><code>{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"10\"\n  }\n}\n</code></pre>"}, {"location": "linux_snippets/#clean-old-kernels", "title": "Clean old kernels", "text": "<p>!!! warning \"I don't recommend using this step, rely on <code>apt-get autoremove</code>, it' safer\"</p> <p>The full command is</p> <pre><code>dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d\"-\"` | grep -e [0-9] | grep -E \"(image|headers)\" | xargs sudo apt-get -y purge\n</code></pre> <p>To test what packages will it remove use:</p> <pre><code>dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d\"-\"` | grep -e [0-9] | grep -e \"(image|headers)\" | xargs sudo apt-get --dry-run remove\n</code></pre> <p>Remember that your running kernel can be obtained by <code>uname -r</code>.</p>"}, {"location": "linux_snippets/#replace-a-string-with-sed-recursively", "title": "Replace a string with sed recursively", "text": "<pre><code>find . -type f -exec sed -i 's/foo/bar/g' {} +\n</code></pre>"}, {"location": "linux_snippets/#bypass-client-ssl-certificate-with-cli-tool", "title": "Bypass client SSL certificate with cli tool", "text": "<p>Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature.</p> <p>To solve it, we can use a transparent proxy that does the exchange for us.</p> <ul> <li>Export your certificate: If you have a <code>p12</code> certificate, you first need to   extract the key, crt and the ca from the certificate into the <code>site.pem</code>.</li> </ul> <pre><code>openssl pkcs12 -in certificate.p12 -out site.key.pem -nocerts -nodes # It asks for the p12 password\nopenssl pkcs12 -in certificate.p12 -out site.crt.pem -clcerts -nokeys\nopenssl pkcs12 -cacerts -nokeys -in certificate.p12 -out site-ca-cert.ca\n\ncat site.key.pem site.crt.pem site-ca-cert.ca &gt; site.pem\n</code></pre> <ul> <li>Build the proxy ca: Then we merge the site and the client ca's into the   <code>site-ca-file.cert</code> file:</li> </ul> <pre><code>openssl s_client -connect www.site.org:443 2&gt;/dev/null  | openssl x509 -text &gt; site-ca-file.cert\ncat site-ca-cert.ca &gt;&gt; web-ca-file.cert\n</code></pre> <ul> <li>Change your hosts file to redirect all requests to the proxy.</li> </ul> <pre><code># vim /etc/hosts\n[...]\n0.0.0.0 www.site.org\n</code></pre> <ul> <li>Run the proxy</li> </ul> <pre><code>docker run --rm \\\n    -v $(pwd):/certs/ \\\n    -p 3001:3001 \\\n    -it ghostunnel/ghostunnel \\\n    client \\\n    --listen 0.0.0.0:3001 \\\n    --target www.site.org:443 \\\n    --keystore /certs/site.pem \\\n    --cacert /certs/site-ca-file.cert \\\n    --unsafe-listen\n</code></pre> <ul> <li>Run the command line tool using the http protocol on the port 3001:</li> </ul> <pre><code>wpscan  --url http://www.site.org:3001/ --disable-tls-checks\n</code></pre> <p>Remember to clean up your env afterwards.</p>"}, {"location": "linux_snippets/#allocate-space-for-a-virtual-filesystem", "title": "Allocate space for a virtual filesystem", "text": "<p>Also useful to simulate big files</p> <pre><code>fallocate -l 20G /path/to/file\n</code></pre>"}, {"location": "linux_snippets/#identify-what-a-string-or-file-contains", "title": "Identify what a string or file contains", "text": "<p>Identify anything. <code>pyWhat</code> easily lets you identify emails, IP addresses, and more. Feed it a .pcap file or some text and it'll tell you what it is.</p>"}, {"location": "linux_snippets/#split-a-file-into-many-with-equal-number-of-lines", "title": "Split a file into many with equal number of lines", "text": "<p>You could do something like this:</p> <pre><code>split -l 200000 filename\n</code></pre> <p>Which will create files each with 200000 lines named <code>xaa</code>, <code>xab</code>, <code>xac</code>, ...</p>"}, {"location": "linux_snippets/#check-if-an-rsync-command-has-gone-well", "title": "Check if an rsync command has gone well", "text": "<p>Sometimes after you do an <code>rsync</code> between two directories of different devices (an usb and your hard drive for example), the sizes of the directories don't match. I've seen a difference of a 30% less on the destination. <code>du</code>, <code>ncdu</code> and <code>and</code> have a long story of reporting wrong sizes with advanced filesystems (ZFS, VxFS or compressing filesystems), these do a lot of things to reduce the disk usage (deduplication, compression, extents, files with holes...) which may lead to the difference in space.</p> <p>To check if everything went alright run <code>diff -r --brief source/ dest/</code>, and check that there is no output.</p>"}, {"location": "linux_snippets/#list-all-process-swap-usage", "title": "List all process swap usage", "text": "<pre><code>for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file; done | sort -k 2 -n -r | less\n</code></pre>"}, {"location": "maison/", "title": "Maison", "text": "<p>Maison is a Python library to read configuration settings from configuration files using <code>pydantic</code> behind the scenes.</p> <p>It's useful to parse TOML config files.</p> <p>Note: \"If you want to use YAML for your config files use <code>goodconf</code> instead.\"</p>"}, {"location": "maison/#installation", "title": "Installation", "text": "<pre><code>pip install maison\n</code></pre>"}, {"location": "maison/#usage", "title": "Usage", "text": "<pre><code>from maison import ProjectConfig\n\nconfig = ProjectConfig(project_name=\"acme\")\nfoo_option = config.get_option(\"foo\")\n\nprint(foo_option)\n</code></pre>"}, {"location": "maison/#read-from-file", "title": "Read from file", "text": "<p>By default, <code>maison</code> will look for a <code>pyproject.toml</code> file. If you prefer to look elsewhere, provide a <code>source_files</code> list to <code>ProjectConfig</code> and <code>maison</code> will select the first source file it finds from the list.</p> <pre><code>from maison import ProjectConfig\n\nconfig = ProjectConfig(project_name=\"acme\", source_files=[\"acme.ini\", \"pyproject.toml\"])\n</code></pre>"}, {"location": "maison/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "map_management/", "title": "Map management", "text": "<p>As a privacy minded human, I try to avoid using proprietary software and services as much as possible. Map management is not an exception. Google maps is monopolizing mapping and routing, but there are better alternatives out there.</p> <p>For navigating on the go, I strongly recommend OSMand+, for browsing maps in the browser, use OpenStreetMaps or CyclOSM if you want to move by bike.</p> <p>To plan routes, you can use brouter.de, it works perfectly for bikes. For hiking is awesome too, it shows you a lot of data needed to plan your tracks (check the settings on the right). If you want to invest a little more time, you can even set your personalize profiles, so that the routing algorithm prioritizes the routes to your desires. It's based on brouter and both can be self-hosted, although brouter does not yet use Docker.</p>"}, {"location": "mbsync/", "title": "mbsync", "text": "<p>mbsync is a command line application which synchronizes mailboxes; currently Maildir and IMAP4 mailboxes are supported. New messages, message deletions and flag changes can be propagated both ways; the operation set can be selected in a fine-grained manner.</p>"}, {"location": "mbsync/#installation", "title": "Installation", "text": "<pre><code>apt-get install isync\n</code></pre>"}, {"location": "mbsync/#configuration", "title": "Configuration", "text": "<p>Assuming that you want to sync the mails of <code>example@examplehost.com</code> and that you have your password stored in <code>pass</code> under <code>mail/example</code>.</p> <p>File: ~/.mbsyncrc</p> <pre><code>IMAPAccount example\nHost examplehost.com\nUser \"example@examplehost.com\"\nPassCmd \"/usr/bin/pass mail/example\"\n\nIMAPStore example-remote\nAccount example\nUseNamespace no\n\nMaildirStore example-local\nPath ~/mail/example/\nInbox ~/mail/example/Inbox\n\nChannel example\nMaster :example-remote:\nSlave :example-local:\nCreate Both\nPatterns *\nSyncState *\nCopyArrivalDate yes\nSync Pull\n</code></pre> <p>You need to manually create the directories where you store the emails.</p> <pre><code>mkdir -p ~/mail/example\n</code></pre>"}, {"location": "mbsync/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "mdformat/", "title": "MDFormat", "text": "<p>MDFormat is an opinionated Markdown formatter that can be used to enforce a consistent style in Markdown files. Mdformat is a Unix-style command-line tool as well as a Python library.</p> <p>The features/opinions of the formatter include:</p> <ul> <li>Consistent indentation and whitespace across the board</li> <li>Always use ATX style headings</li> <li>Move all link references to the bottom of the document (sorted by label)</li> <li>Reformat indented code blocks as fenced code blocks</li> <li>Use 1. as the ordered list marker if possible, also for noninitial list items.</li> </ul> <p>It's based on the <code>markdown-it-py</code> Markdown parser, which is a Python implementation of <code>markdown-it</code>.</p>"}, {"location": "mdformat/#installation", "title": "Installation", "text": "<p>By default it uses CommonMark support:</p> <pre><code>pip install mdformat\n</code></pre> <p>This won't support task lists, if you want them use the github flavoured parser instead:</p> <pre><code>pip install mdformat-gfm\n</code></pre> <p>You may want to also install some interesting plugins:</p> <ul> <li><code>mdformat-beautysh</code>: format   <code>bash</code> and <code>sh</code> code blocks.</li> <li><code>mdformat-black</code>: format <code>python</code>   code blocks.</li> <li><code>mdformat-config</code>: format <code>json</code>,   <code>toml</code> and <code>yaml</code> code blocks.</li> <li><code>mdformat-web</code>: format <code>javascript</code>,   <code>css</code>, <code>html</code> and <code>xml</code> code blocks.</li> <li><code>mdformat-tables</code>: Adds   support for Github Flavored Markdown style tables.</li> <li><code>mdformat-frontmatter</code>:   Adds support for the yaml header with metadata of the file.</li> </ul> <p>To install them with <code>pipx</code> you can run:</p> <pre><code>pipx install --include-deps mdformat-gfm\npipx inject mdformat-gfm mdformat-beautysh mdformat-black mdformat-config \\\n    mdformat-web mdformat-tables mdformat-frontmatter\n</code></pre>"}, {"location": "mdformat/#desires", "title": "Desires", "text": "<p>These are the functionalities I miss when writing markdown that can be currently fixed with <code>mdformat</code>:</p> <ul> <li>Long lines are wrapped.</li> <li>Long lines in lists are wrapped and the indentation is respected.</li> <li>Add correct blank lines between sections.</li> </ul> <p>I haven't found yet a way to achieve:</p> <ul> <li>Links are sent to the bottom of the document.</li> <li>Do   typographic replacements</li> <li>End paragraphs with a dot.</li> </ul>"}, {"location": "mdformat/#developing-mdformat-plugins", "title": "Developing mdformat plugins", "text": "<p>There are two kinds of plugins:</p> <ul> <li>Formatters: They change the output of the text. For example   <code>mdformatormat-black</code>.</li> <li>Parsers: They are extensions to the base CommonMark parser.</li> </ul> <p>You can see some plugin examples here.</p>"}, {"location": "mdformat/#issues", "title": "Issues", "text": "<ul> <li>It doesn't yet   support admonitions</li> <li>You can't   ignore some files,   nor   some part of the file</li> </ul>"}, {"location": "mdformat/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> </ul>"}, {"location": "meditation/", "title": "Meditation", "text": "<p>Meditation is a practice where an individual uses a technique,such as mindfulness, or focusing the mind on a particular object, thought, or activity, to train attention and awareness, and achieve a mentally clear and emotionally calm and stable state.</p> <p>Meditation may reduce stress, anxiety, depression, and pain, and enhance peace, perception, self-concept, and well-being.</p>"}, {"location": "meditation/#types-of-meditation", "title": "Types of meditation", "text": "<p>Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality.</p> <p>There are nine popular types of meditation practice:</p> <ul> <li> <p>Mindfulness meditation: You pay attention to your thoughts as they pass     through your mind. You don't judge the thoughts or become involved with     them. You simply observe and take note of any patterns.</p> <p>This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings.</p> <p>This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone.</p> </li> <li> <p>Focused meditation: Involves concentration using any of the five senses.</p> <p>For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention.</p> <p>Try counting mala beads, listening to a gong, or staring at a candle flame.</p> <p>This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first.</p> <p>If your mind does wander, it\u2019s important to come back to the practice and refocus.</p> <p>As the name suggests, this practice is ideal for anyone who requires additional focus in their life.</p> </li> <li> <p>Movement meditation: It\u2019s an active form of meditation where the movement     guides you. It can be achieved through yoga, martial arts or by walking     through the woods, gardening, qigong, and other gentle forms of motion.</p> <p>Movement meditation is good for people who find peace in action and prefer to let their minds wander.</p> </li> <li> <p>Mantra meditation: Uses a repetitive sound to clear the mind. It can be     a word, phrase, or sound, such as the popular \u201cOm.\u201d</p> <p>It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness.</p> <p>Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition.</p> </li> <li> <p>Transcendental Meditation: It is more customizable than mantra meditation,     using a mantra or series of words that are specific to each practitioner.</p> <p>This practice is for those who like structure and are serious about maintaining a meditation practice.</p> </li> <li> <p>Progressive relaxation: Also known as body scan meditation, it's a practice     aimed at reducing tension in the body and promoting relaxation.</p> <p>Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body.</p> <p>In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension.</p> <p>This form of meditation is often used to relieve stress and unwind before bedtime.</p> </li> <li> <p>Loving-kindness meditation: is used to strengthen feelings of compassion,     kindness, and acceptance toward oneself and others.</p> <p>It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings.</p> <p>Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation: Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images.</p> <p>With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible.</p> <p>Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation.</p> <p>Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace.</p> </li> <li> <p>Spiritual meditation: Spiritual meditation is used in Eastern religions,     such as Hinduism and Daoism, and in Christian faith..</p> <p>It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe.</p> </li> </ul>"}, {"location": "meditation/#how-to-get-started", "title": "How to get started", "text": "<p>The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there.</p>"}, {"location": "meditation/#references", "title": "References", "text": "<ul> <li>healthline article on types of meditation</li> <li>NonCompete video on meditation for anti-capitalists</li> </ul>"}, {"location": "meditation/#to-review", "title": "To review", "text": "<ul> <li>https://wiki.nikitavoloboev.xyz/mindfulness/meditation</li> <li>https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep</li> <li>https://threader.app/thread/1261481222359801856</li> <li>https://quietkit.com/box-breathing/</li> <li>https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8</li> <li>https://www.mindful.org/how-to-meditate/</li> </ul>"}, {"location": "meditation/#books", "title": "Books", "text": "<ul> <li>The Mind Illuminated: A Complete Meditation Guide Integrating Buddhist Wisdom and Brain Science by Culadasa (John Yates)</li> </ul>"}, {"location": "mentoring/", "title": "Mentoring", "text": "<p>Mentoring is a process for the informal transmission of knowledge, social capital, and the psychosocial support perceived by the recipient as relevant to work, career, or professional development; mentoring entails informal communication, usually face-to-face and during a sustained period of time, between a person who is perceived to have greater relevant knowledge, wisdom, or experience (the mentor) and a person who is perceived to have less (the apprentice).</p>"}, {"location": "mentoring/#obstacles", "title": "Obstacles", "text": ""}, {"location": "mentoring/#apprentice-obstacles", "title": "Apprentice obstacles", "text": "<p>The most common obstacles I've found apprentices have in their early steps of learning are:</p> <ul> <li>Not knowing where to start.</li> <li>Not having a clear roadmap.</li> <li>Having wrong expectations.</li> <li>Feeling overwhelmed by big tasks.</li> <li>Not knowing how to break a big task in small actionable steps.</li> <li>Given a requirement, design possible solutions and choose the best one.</li> <li>Feeling insecure about themselves.</li> <li>Suffering from the impostor syndrome</li> </ul> <p>A mentor can greatly help the apprentice overcome them.</p>"}, {"location": "mentoring/#mentor-obstacles", "title": "Mentor obstacles", "text": "<p>The most common obstacles I've found as a mentor are:</p> <ul> <li>Use concepts that the apprentice doesn't yet understand.</li> <li>Try to impose my way of doing things.</li> <li>Try to impose the best solution or practices even though they are out of reach     of the apprentice yet.</li> </ul>"}, {"location": "mentoring/#mentorship-principles", "title": "Mentorship principles", "text": "<p>People involved in a mentorship experience a strong personal relationship, in order to make it pleasant and healthy it must be based on the next principles:</p> <ul> <li>Care</li> <li>Equality</li> <li>Transparency</li> </ul>"}, {"location": "mentoring/#care", "title": "Care", "text": "<p>As in any relationship, care must be one of the main focuses of both parties, by care I mean:</p> <ul> <li>Actively read the other person mood and state and adjust your behaviours     accordingly.</li> <li>Ask for the other person's well being, keep track of the events of their     lives, and ask them how they went afterwards.</li> <li>Know your weak spots, have an improvement plan, and make them visible when they     arise.</li> <li>Actively search for ways to make their life easier and more pleasant.</li> <li>Respect the other person's time, don't be late.</li> </ul> <p>Men must put special interest in this point as we're usually not taught on caring for others.</p>"}, {"location": "mentoring/#equality", "title": "Equality", "text": "<p>There's a high risk of having unhealthy power dynamics where the mentor is taken as in a higher position than the apprentice because he has more knowledge in the specific field of study. The reality is that there is a lot more involved in the experience than the transmission of knowledge of the field from mentor to apprentice.</p> <p>Only if you see yourself as equals you can build the best experience.</p>"}, {"location": "mentoring/#transparency", "title": "Transparency", "text": ""}, {"location": "mentoring/#shared-roles", "title": "Shared roles", "text": "<ul> <li>Actively defend and follow the mentorship     principles.</li> <li>Review and improve the mentoring workflows.</li> </ul>"}, {"location": "mentoring/#mentor-roles", "title": "Mentor roles", "text": "<p>The mentor can help through the next ways:</p> <ul> <li>Roadmap definition and maintenance</li> <li>Task management</li> <li>Overcome the mentor's obstacles</li> </ul>"}, {"location": "mentoring/#roadmap-definition-and-maintenance", "title": "Roadmap definition and maintenance", "text": ""}, {"location": "mentoring/#get-to-know-each-other", "title": "Get to know each other", "text": "<p>First of all we need to know what are the underlying goals of the apprentice in order to sketch the best roadmap.</p>"}, {"location": "mentoring/#define-and-maintain-a-roadmap", "title": "Define and maintain a roadmap", "text": "<p>It's very important that the apprentice has a clear idea of In order to</p> <p>Beginner Junior</p>"}, {"location": "mentoring/#overcome-the-mentor-obstacles", "title": "Overcome the mentor obstacles", "text": "<ul> <li>Be attentive of the apprentice reactions</li> </ul>"}, {"location": "mentoring/#task-management", "title": "Task management", "text": ""}, {"location": "mentoring/#apprentice-roles", "title": "Apprentice roles", "text": "<ul> <li>Try it's best to follow the agreed roadmap and tasks.</li> <li>Analyze themselves with respect to the mentoring workflows.</li> <li>Overcome the apprentice's obstacles.     *</li> </ul>"}, {"location": "mermaidjs/", "title": "MermaidJS", "text": "<p>MermaidJS is a Javascript library that lets you create diagrams using text and code.</p> <p>It can render the next diagram types:</p> <ul> <li>Flowchart</li> <li>Sequence.</li> <li>Gantt</li> <li>Class</li> <li>Git graph</li> <li>Entity Relationship</li> <li>User journey</li> </ul>"}, {"location": "mermaidjs/#installation", "title": "Installation", "text": "<p>Installing it requires node, I've only used it in mkdocs, which is easier to install and use.</p>"}, {"location": "mermaidjs/#usage", "title": "Usage", "text": ""}, {"location": "mermaidjs/#flowchart", "title": "Flowchart", "text": "<p>It can have two orientations top to bottom (<code>TB</code>) or left to right (<code>LR</code>).</p> <pre><code>graph TD\n    Start --&gt; Stop\n</code></pre> <p>By default the text shown is the same as the id, if you need a big text it's recommended to use the <code>id1[This is the text in the box]</code> syntax so it's easy to reference the node in the relationships.</p> <p>To link nodes, use <code>--&gt;</code> or <code>---</code>. If you cant to add text to the link use <code>A-- text --&gt;B</code></p>"}, {"location": "mermaidjs/#adding-links", "title": "Adding links", "text": "<p>You can add <code>click</code> events to the diagrams:</p> <pre><code>graph LR;\n    A--&gt;B;\n    B--&gt;C;\n    C--&gt;D;\n    click A callback \"Tooltip for a callback\"\n    click B \"http://www.github.com\" \"This is a tooltip for a link\"\n    click A call callback() \"Tooltip for a callback\"\n    click B href \"http://www.github.com\" \"This is a tooltip for a link\"\n</code></pre> <p>By default the links are opened in the same browser tab/window. It is possible to change this by adding a link target to the click definition (<code>_self</code>, <code>_blank</code>, <code>_parent</code>, or <code>_top</code>).</p> <pre><code>graph LR;\n    A--&gt;B;\n    B--&gt;C;\n    C--&gt;D;\n    D--&gt;E;\n    click A \"http://www.github.com\" _blank\n</code></pre>"}, {"location": "mermaidjs/#node-styling", "title": "Node styling", "text": "<p>You can define the style for each node with:</p> <pre><code>graph LR\n    id1(Start)--&gt;id2(Stop)\n    style id1 fill:#f9f,stroke:#333,stroke-width:4px\n    style id2 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\n</code></pre> <p>Or if you're going to use the same style for multiple nodes, you can define classes:</p> <pre><code>graph LR\n    A:::someclass --&gt; B\n    classDef someclass fill:#f96;\n</code></pre>"}, {"location": "mermaidjs/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "mizu/", "title": "Mizu", "text": "<p>Mizu is an API Traffic Viewer for Kubernetes, think <code>TCPDump</code> and Chrome Dev Tools combined.</p>"}, {"location": "mizu/#installation", "title": "Installation", "text": "<pre><code>curl -Lo mizu \\\nhttps://github.com/up9inc/mizu/releases/latest/download/mizu_linux_amd64 \\\n&amp;&amp; chmod 755 mizu\n</code></pre>"}, {"location": "mizu/#usage", "title": "Usage", "text": "<p>At the core of Mizu functionality is the pod tap</p> <pre><code>mizu tap &lt;podname&gt;\n</code></pre> <p>To view traffic of several pods, identified by a regular expression:</p> <pre><code>mizu tap \"(catalo*|front-end*)\"\n</code></pre> <p>After tapping your pods, Mizu will tell you that \"Web interface is now available at <code>https://localhost:8899/</code>. Visit the link from Mizu to view traffic in the Mizu UI.</p>"}, {"location": "mizu/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "money_management/", "title": "Money Management", "text": "<p>Money management is the act of analyzing where you spend your money on with the least amount of mental load.</p> <p>Some years ago I started using the double entry counting method with beancount.</p>"}, {"location": "money_management/#system-inputs", "title": "System inputs", "text": "<p>I have two types of financial transactions to track:</p> <ul> <li> <p>The credit/debit card movements: Easy to track as usually the banks support     exporting them as CSV, and beancount have specific bank     importers.</p> </li> <li> <p>The cash movements: Harder to track as you need to keep them manually. This     has been my biggest source of errors, once I understood how to correctly use     beancount.</p> <p>In the latest iteration, I'm using the cone Android app to keep track of these expenses.</p> </li> </ul>"}, {"location": "money_management/#workflow", "title": "Workflow", "text": ""}, {"location": "money_management/#beancount-ledger-organization", "title": "Beancount ledger organization", "text": "<p>My beancount project directory tree is:</p> <pre><code>.\n\u251c\u2500\u2500 .git\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 2011\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 09.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 10.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 11.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 12.book\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 year.book\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 2020\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 01.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 02.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 11.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 12.book\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 year.book\n\u251c\u2500\u2500 ledger.book\n\u251c\u2500\u2500 .closed.accounts.book\n\u251c\u2500\u2500 roadmap.md\n\u2514\u2500\u2500 to.process\n    \u251c\u2500\u2500 cone.book\n    \u251c\u2500\u2500 bank1.csv\n    \u2514\u2500\u2500 bank2.csv\n</code></pre> <p>Where:</p> <ul> <li><code>.git</code>: I keep everything in a git repository to have a version controlled ledger.</li> <li>Each year has it's own directory with:<ul> <li>A <code>book</code> file per month, check below it's contents.</li> <li> <p>A <code>year.book</code> file with just include statements:</p> <p><pre><code>include \"01.book\"\ninclude \"02.book\"\ninclude \"03.book\"\ninclude \"04.book\"\ninclude \"05.book\"\ninclude \"06.book\"\ninclude \"07.book\"\ninclude \"08.book\"\ninclude \"09.book\"\n# include \"10.book\"\n# include \"11.book\"\n# include \"12.book\"\n</code></pre> * <code>ledger.book</code>: The beancount entry point where the accounts are defined. * <code>.closed.accounts.book</code>: To store the account closing statements. * <code>roadmap.md</code>: To store the financial plan for the semester/year/life. * <code>to.process</code>: To store the raw data from external sources.</p> </li> </ul> </li> </ul>"}, {"location": "money_management/#the-main-ledger", "title": "The main ledger", "text": "<p>The <code>ledger.book</code> file contains the beancount configuration, with the opening of accounts and inclusion of the monthly books. I like to split it in sections.</p> TL;DR: The full <code>ledger.book</code> <pre><code># Options\noption \"title\" \"Lyz Lair Ledge\"\noption \"operating_currency\" \"EUR\"\n\n# Events\n2016-12-19 event \"employer\" \"XXX\"\n\n# Eternal accounts\n\n# Assets\n\n2010-05-17 open Assets:Cash EUR\n2021-01-25 open Assets:Cash:Coins EUR\n2021-01-25 open Assets:Cash:Paper EUR\n2018-09-10 open Assets:Cashbox EUR\n2019-01-11 open Assets:Cashbox:Coins EUR\n2019-01-11 open Assets:Cashbox:Paper EUR\n2018-04-01 open Assets:Savings EUR\n2019-08-01 open Assets:Savings:CashFlowRefiller EUR\n2019-08-01 open Assets:Savings:UnexpectedExpenses EUR\n2019-08-01 open Assets:Savings:Home EUR\n2018-04-01 open Assets:CashFlowCard EUR\n2018-04-01 open Assets:CashDeposit EUR\n\n# Debts\n\n2016-01-01 open Assets:Debt:Person1 EUR\n2016-01-01 open Assets:Debt:Person2 EUR\n\n# Income\n\n2016-12-01 open Income:Employer1 EUR\n2019-05-21 open Income:Employer2 EUR\n2010-05-17 open Income:State EUR\n2019-01-01 open Income:Gifts EUR\n\n# Equity\n\n2010-05-17 open Equity:Opening-Balances\n2010-05-17 open Equity:Errors\n2010-05-17 open Equity:Forgiven\n\n# Expenses\n\n2010-01-01 open Expenses:Bills EUR\n2013-01-01 open Expenses:Bills:Gas EUR\n2010-01-01 open Expenses:Bills:Phone EUR\n2019-01-01 open Expenses:Bills:Light EUR\n2010-01-01 open Expenses:Bills:Rent EUR\n2010-01-01 open Expenses:Bills:PublicTransport EUR\n2017-01-01 open Expenses:Bills:Subscriptions EUR\n2016-01-01 open Expenses:Bills:Union EUR\n2010-01-01 open Expenses:Books EUR\n2010-12-01 open Expenses:Car EUR\n2010-12-01 open Expenses:Car:Fuel EUR\n2010-12-01 open Expenses:Car:Insurance EUR\n2010-12-01 open Expenses:Car:Repair EUR\n2010-12-01 open Expenses:Car:Taxes EUR\n2010-12-01 open Expenses:Car:Tickets EUR\n2010-01-01 open Expenses:Clothes EUR\n2018-11-01 open Expenses:Donations EUR\n2010-05-17 open Expenses:Financial EUR\n2010-01-01 open Expenses:Games EUR\n2010-01-01 open Expenses:Games:Steam EUR\n2010-01-01 open Expenses:Games:HumbleBundle EUR\n2019-06-01 open Expenses:Games:GOG EUR\n2020-06-01 open Expenses:Games:Itchio EUR\n2010-01-01 open Expenses:Gifts EUR\n2010-01-01 open Expenses:Gifts:Person1 EUR\n2010-01-01 open Expenses:Gifts:Person2 EUR\n2010-01-01 open Expenses:Gifts:Mine EUR\n2010-01-01 open Expenses:Groceries EUR\n2018-11-01 open Expenses:Groceries:Extras EUR\n2020-01-01 open Expenses:Groceries:Supermarket EUR\n2020-01-01 open Expenses:Groceries:Prepared EUR\n2020-01-01 open Expenses:Groceries:GreenGrocery EUR\n2010-01-01 open Expenses:Hardware EUR\n2010-01-01 open Expenses:Home EUR\n2010-01-01 open Expenses:Home:WashingMachine EUR\n2010-01-01 open Expenses:Home:DishWasher EUR\n2010-01-01 open Expenses:Home:Fridge EUR\n2020-06-01 open Expenses:Legal EUR\n2010-01-01 open Expenses:Medicines EUR\n2010-01-01 open Expenses:Social EUR\n2010-01-01 open Expenses:Social:Eat EUR\n2010-01-01 open Expenses:Social:Drink EUR\n2019-06-01 open Expenses:Taxes:Tax1 EUR\n2016-01-01 open Expenses:Taxes:Tax2 EUR\n2010-05-17 open Expenses:Trips EUR\n2010-05-17 open Expenses:Trips:Accommodation EUR\n2010-05-17 open Expenses:Trips:Drink EUR\n2010-05-17 open Expenses:Trips:Food EUR\n2010-05-17 open Expenses:Trips:Tickets EUR\n2010-05-17 open Expenses:Trips:Transport EUR\n2019-05-20 open Expenses:Work EUR\n2019-05-20 open Expenses:Work:Phone EUR\n2019-05-20 open Expenses:Work:Hardware EUR\n2019-05-20 open Expenses:Work:Trips EUR\n2019-05-20 open Expenses:Work:Trips:Accommodation EUR\n2019-05-20 open Expenses:Work:Trips:Drink EUR\n2019-05-20 open Expenses:Work:Trips:Food EUR\n2019-05-20 open Expenses:Work:Trips:Tickets EUR\n2019-05-20 open Expenses:Work:Trips:Transport EUR\n\n## Initialization\n\n2010-05-17 pad Assets:Cash Equity:Opening-Balances\n2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances\n\n# Transfers\n\ninclude \".closed.accounts.book\"\ninclude \"2011/year.book\"\ninclude \"2012/year.book\"\ninclude \"2013/year.book\"\ninclude \"2014/year.book\"\ninclude \"2015/year.book\"\ninclude \"2016/year.book\"\ninclude \"2017/year.book\"\ninclude \"2018/year.book\"\ninclude \"2019/year.book\"\ninclude \"2020/year.book\"\n</code></pre>"}, {"location": "money_management/#assets", "title": "Assets", "text": "<p>Asset accounts represent something you have.</p> <pre><code># Assets\n\n2010-05-17 open Assets:Cash EUR\n2021-01-25 open Assets:Cash:Coins EUR\n2021-01-25 open Assets:Cash:Paper EUR\n2018-09-10 open Assets:Cashbox EUR\n2019-01-11 open Assets:Cashbox:Coins EUR\n2019-01-11 open Assets:Cashbox:Paper EUR\n2018-04-01 open Assets:Savings EUR\n2019-08-01 open Assets:Savings:CashFlowRefiller EUR\n2019-08-01 open Assets:Savings:UnexpectedExpenses EUR\n2019-08-01 open Assets:Savings:Home EUR\n2018-04-01 open Assets:CashFlowCard EUR\n2018-04-01 open Assets:CashDeposit EUR\n</code></pre> <p>Being a privacy minded person, I try to pay everything by cash. To track it, I've created the following asset accounts:</p> <ul> <li><code>Assets:Cash:Paper</code>: Paper money in my wallet. I like to have a 5, 10 and 20     euro bills as it gives the best flexibility without carrying too much money.</li> <li><code>Assets:Cash:Coins</code>: Coins in my wallet.</li> <li><code>Assets:Cashbox:Paper</code>: Paper money stored at home. I fill it up monthly to     my average monthly expenses, so I reduce the trips to the ATM to the     minimum. Once this account is below 100 EUR, I add the mental task to refill     it.</li> <li><code>Assets:Cashbox:Coins</code>: Coins stored at home. I keep it at 10 EUR in     coins of 2 EUR, so it's quick to count at the same time as it's able to     cover most of the things you need to buy with coins.</li> <li><code>Assets:Cashbox:SmallCoins</code>: If my coins wallet is starting to get heavy,     I extract the coins smaller than 50 cents into a container with copper     coins.</li> <li><code>Assets:CashDeposit</code>: You never know when the bank system is going to fuck     you, so it's always good to have some cash under the mattress.</li> </ul> <p>Having this level of granularity and doing weekly balances of each of those accounts has helped me understand the flaws in my processes that lead to the cash accounting errors.</p> <p>As most humans living in the first world, I'm forced to have at least one bank account. For security reasons I have two:</p> <ul> <li><code>Assets:CashFlowCard</code>: The bank account with an associated debit card. Here is     from where I make my expenses, such as home rental, supplies payment, ATM     money withdrawal. As it is exposed to all the payment platforms, I assume     that it will come a time when a vulnerability is found in one of them, so     I keep the least amount of money I can. As with the <code>Cashbox</code> I monthly     refill it with the expected expenses amount plus a safety amount.</li> <li> <p><code>Assets:Savings</code>: The bank account where I store my savings. I have it     subdivided in three sections:</p> <ul> <li><code>Assets:Savings:CashFlowRefiller</code>: Here I store the average monthly     expenses for the following two months.</li> <li><code>Assets:Savings:UnexpectedExpenses</code>: Deposit for unexpected expenses such     as car or domestic appliances repairs.</li> <li><code>Assets:Savings:Home</code>: Deposit for the initial payment or a house.</li> </ul> </li> </ul>"}, {"location": "money_management/#debts", "title": "Debts", "text": "<p>Debts can be tracked either as an asset or as a liability. If you expect them to owe you more often (you lend a friend some money), model it as an asset, if you're going to owe them (you borrow from the bank to buy a house), model it as a liability.</p> <pre><code># Debts\n\n2016-01-01 open Assets:Debt:Person1 EUR\n2016-01-01 open Assets:Debt:Person2 EUR\n</code></pre>"}, {"location": "money_management/#income", "title": "Income", "text": "<p>Income accounts represent where you get the money from.</p> <pre><code># Income\n\n2016-12-01 open Income:Employer1 EUR\n2019-05-21 open Income:Employer2 EUR\n2010-05-17 open Income:State EUR\n2019-01-01 open Income:Gifts EUR\n</code></pre>"}, {"location": "money_management/#equity", "title": "Equity", "text": "<p>I use equity accounts to make adjustments.</p> <pre><code># Equity\n\n2010-05-17 open Equity:Opening-Balances\n2010-05-17 open Equity:Errors\n2010-05-17 open Equity:Forgiven\n</code></pre> <ul> <li><code>Equity:Opening-Balances</code>: Used to set the initial balance of an account.</li> <li><code>Equity:Errors</code>: Used with the <code>pad</code> statements to track the errors in the     accounting.</li> <li><code>Equity:Forgiven</code>: Used in the transactions to forgive someone's debts.</li> </ul>"}, {"location": "money_management/#expenses", "title": "Expenses", "text": "<p>Expense accounts model where you expend the money on.</p> <pre><code># Expenses\n\n2010-01-01 open Expenses:Bills EUR\n2013-01-01 open Expenses:Bills:Gas EUR\n2010-01-01 open Expenses:Bills:Phone EUR\n2019-01-01 open Expenses:Bills:Light EUR\n2010-01-01 open Expenses:Bills:Rent EUR\n2010-01-01 open Expenses:Bills:PublicTransport EUR\n2017-01-01 open Expenses:Bills:Subscriptions EUR\n2016-01-01 open Expenses:Bills:Union EUR\n2010-01-01 open Expenses:Books EUR\n2010-12-01 open Expenses:Car EUR\n2010-12-01 open Expenses:Car:Fuel EUR\n2010-12-01 open Expenses:Car:Insurance EUR\n2010-12-01 open Expenses:Car:Repair EUR\n2010-12-01 open Expenses:Car:Taxes EUR\n2010-12-01 open Expenses:Car:Tickets EUR\n2010-01-01 open Expenses:Clothes EUR\n2018-11-01 open Expenses:Donations EUR\n2010-05-17 open Expenses:Financial EUR\n2010-01-01 open Expenses:Games EUR\n2010-01-01 open Expenses:Games:Steam EUR\n2010-01-01 open Expenses:Games:HumbleBundle EUR\n2019-06-01 open Expenses:Games:GOG EUR\n2020-06-01 open Expenses:Games:Itchio EUR\n2010-01-01 open Expenses:Gifts EUR\n2010-01-01 open Expenses:Gifts:Person1 EUR\n2010-01-01 open Expenses:Gifts:Person2 EUR\n2010-01-01 open Expenses:Gifts:Mine EUR\n2010-01-01 open Expenses:Groceries EUR\n2018-11-01 open Expenses:Groceries:Extras EUR\n2020-01-01 open Expenses:Groceries:Supermarket EUR\n2020-01-01 open Expenses:Groceries:Prepared EUR\n2020-01-01 open Expenses:Groceries:GreenGrocery EUR\n2010-01-01 open Expenses:Hardware EUR\n2010-01-01 open Expenses:Home EUR\n2010-01-01 open Expenses:Home:WashingMachine EUR\n2010-01-01 open Expenses:Home:DishWasher EUR\n2010-01-01 open Expenses:Home:Fridge EUR\n2020-06-01 open Expenses:Legal EUR\n2010-01-01 open Expenses:Medicines EUR\n2010-01-01 open Expenses:Social EUR\n2010-01-01 open Expenses:Social:Eat EUR\n2010-01-01 open Expenses:Social:Drink EUR\n2019-06-01 open Expenses:Taxes:Tax1 EUR\n2016-01-01 open Expenses:Taxes:Tax2 EUR\n2010-05-17 open Expenses:Trips EUR\n2010-05-17 open Expenses:Trips:Accommodation EUR\n2010-05-17 open Expenses:Trips:Drink EUR\n2010-05-17 open Expenses:Trips:Food EUR\n2010-05-17 open Expenses:Trips:Tickets EUR\n2010-05-17 open Expenses:Trips:Transport EUR\n2019-05-20 open Expenses:Work EUR\n2019-05-20 open Expenses:Work:Phone EUR\n2019-05-20 open Expenses:Work:Hardware EUR\n2019-05-20 open Expenses:Work:Trips EUR\n2019-05-20 open Expenses:Work:Trips:Accommodation EUR\n2019-05-20 open Expenses:Work:Trips:Drink EUR\n2019-05-20 open Expenses:Work:Trips:Food EUR\n2019-05-20 open Expenses:Work:Trips:Tickets EUR\n2019-05-20 open Expenses:Work:Trips:Transport EUR\n</code></pre> <p>I decided to split my expenses in:</p> <ul> <li><code>Expenses:Bills</code>: All the periodic bills I pay<ul> <li><code>Expenses:Bills:Gas</code>:</li> <li><code>Expenses:Bills:Phone</code>:</li> <li><code>Expenses:Bills:Light</code>:</li> <li><code>Expenses:Bills:Rent</code>:</li> <li><code>Expenses:Bills:PublicTransport</code>:</li> <li><code>Expenses:Bills:Subscriptions</code>: Newspaper, magazine, web service     subscriptions.</li> <li><code>Expenses:Bills:Union</code>:</li> </ul> </li> <li><code>Expenses:Books</code>:</li> <li><code>Expenses:Car</code>:<ul> <li><code>Expenses:Car:Fuel</code>:</li> <li><code>Expenses:Car:Insurance</code>:</li> <li><code>Expenses:Car:Repair</code>:</li> <li><code>Expenses:Car:Taxes</code>:</li> <li><code>Expenses:Car:Tickets</code>:</li> </ul> </li> <li><code>Expenses:Clothes</code>:</li> <li><code>Expenses:Donations</code>:</li> <li><code>Expenses:Financial</code>: Expenses related to financial operations or account     maintenance.</li> <li><code>Expenses:Games</code>:<ul> <li><code>Expenses:Games:Steam</code>:</li> <li><code>Expenses:Games:HumbleBundle</code>:</li> <li><code>Expenses:Games:GOG</code>:</li> <li><code>Expenses:Games:Itchio</code>:</li> </ul> </li> <li><code>Expenses:Gifts</code>:<ul> <li><code>Expenses:Gifts:Person1</code>:</li> <li><code>Expenses:Gifts:Person2</code>:</li> <li><code>Expenses:Gifts:Mine</code>:</li> </ul> </li> <li><code>Expenses:Groceries</code>:<ul> <li><code>Expenses:Groceries:Extras</code>:</li> <li><code>Expenses:Groceries:Supermarket</code>:</li> <li><code>Expenses:Groceries:Prepared</code>:</li> <li><code>Expenses:Groceries:GreenGrocery</code>:</li> </ul> </li> <li><code>Expenses:Hardware</code>:</li> <li><code>Expenses:Home</code>:<ul> <li><code>Expenses:Home:WashingMachine</code>:</li> <li><code>Expenses:Home:DishWasher</code>:</li> <li><code>Expenses:Home:Fridge</code>:</li> </ul> </li> <li><code>Expenses:Legal</code>:</li> <li><code>Expenses:Medicines</code>:</li> <li><code>Expenses:Social</code>:<ul> <li><code>Expenses:Social:Eat</code>:</li> <li><code>Expenses:Social:Drink</code>:</li> </ul> </li> <li><code>Expenses:Taxes</code>:<ul> <li><code>Expenses:Taxes:Tax1</code>:</li> <li><code>Expenses:Taxes:Tax2</code>:</li> </ul> </li> <li><code>Expenses:Trips</code>:<ul> <li><code>Expenses:Trips:Accommodation</code>:</li> <li><code>Expenses:Trips:Drink</code>:</li> <li><code>Expenses:Trips:Food</code>:</li> <li><code>Expenses:Trips:Tickets</code>:</li> <li><code>Expenses:Trips:Transport</code>:</li> </ul> </li> <li><code>Expenses:Work</code>:<ul> <li><code>Expenses:Work:Phone</code>:</li> <li><code>Expenses:Work:Hardware</code>:</li> <li><code>Expenses:Work:Trips</code>:</li> <li><code>Expenses:Work:Trips:Accommodation</code>:</li> <li><code>Expenses:Work:Trips:Drink</code>:</li> <li><code>Expenses:Work:Trips:Food</code>:</li> <li><code>Expenses:Work:Trips:Tickets</code>:</li> <li><code>Expenses:Work:Trips:Transport</code>:</li> </ul> </li> </ul>"}, {"location": "money_management/#initialization-of-accounts", "title": "Initialization of accounts", "text": "<pre><code># Initialization\n\n2010-05-17 pad Assets:Cash Equity:Opening-Balances\n2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances\n</code></pre>"}, {"location": "money_management/#transfer-includes", "title": "Transfer includes", "text": "<p>I reference each year's <code>year.book</code> and the <code>.closed.accounts.book</code>.</p> <pre><code># Transfers\n\ninclude \".closed.accounts.book\"\ninclude \"2011/year.book\"\n...\ninclude \"2020/year.book\"\n</code></pre>"}, {"location": "money_management/#the-monthly-book", "title": "The monthly book", "text": "<p>Each month has a file with this structure:</p> <pre><code># Cash transfers\n\n# CashFlowCard\n\n# Savings\n\n# Balances taken at 2020-12-06T19:32\n\n## Active accounts\n\n2020-12-06 balance Assets:Cashbox:Paper  EUR\n2020-12-06 balance Assets:Cashbox:Coins  EUR\n2020-12-06 balance Assets:Cash:Paper  EUR\n2020-12-06 balance Assets:Cash:Coins  EUR\n\n2020-12-06 balance Assets:CashFlowCard  EUR\n2020-12-06 balance Assets:Savings  EUR\n\n## Deposits\n\n2020-12-06 balance Assets:Savings:CashFlowRefiller XXX EUR\n2020-12-06 balance Assets:Savings:UnexpectedExpenses XXX EUR\n2020-12-06 balance Assets:CashDeposit XXX EUR\n\n## Debts\n\n2020-12-06 balance Assets:Debt:Person1 XXX EUR\n\n## Equity\n\n# 2020-12-05 pad Assets:Cash Equity:Errors\n# 2020-12-05 pad Assets:Cashbox Equity:Errors\n\n# Weekly balances\n\n## Measure done on 2020-09-04T17:10\n2020-09-04 balance Assets:Cash:Coins XXX EUR\n2020-09-04 balance Assets:Cash:Paper XXX EUR\n2020-09-04 balance Assets:Cashbox:Coins XXX EUR\n2020-09-04 balance Assets:Cashbox:Paper XXX EUR\n</code></pre> <p>Where each section stores:</p> <ul> <li> <p><code>Cash transfers</code>: The transactions done by cash, extracted from the     Android <code>cone</code> application.</p> </li> <li> <p><code>CashFlowCard</code>: Bank account extracts transformed from the csv to postings     with <code>bean-extract</code>.</p> </li> <li> <p><code>Savings</code>: Bank account extracts transformed from the csv to postings     with <code>bean-extract</code>.</p> </li> <li> <p><code>Monthly balances</code>: I try to review the accounts once each month. This section is subdivided in:</p> <ul> <li><code>Active accounts</code>: The accounts whose value changes monthly.</li> <li><code>Deposits</code>: The accounts that don't change much each month.</li> <li><code>Debts</code>: The balance of debt accounts.</li> <li><code>Equity</code>: The <code>pad</code> statements to track the errors in the monthly account.</li> </ul> </li> <li> <p><code>Weekly balances</code>: As doing the monthly review is long, but it doesn't give me     the enough information to not mess up the cash transactions, I do a weekly     balance of those accounts.</p> </li> </ul>"}, {"location": "monitoring_comparison/", "title": "Monitoring Comparison", "text": "<p>As with any technology, when you want to adopt it, you first need to analyze your options. In this article we're going to compare the two most popular solutions at the moment, Nagios and Prometheus. Zabbix is similar in architecture and features to Nagios, so for the first iteration we're going to skip it.</p> <p>TL;DR: Prometheus is better, but it needs more effort.</p> <p>Nagios is suitable for basic monitoring of small and/or static systems where blackbox probing is sufficient.</p> <p>If you want to do whitebox monitoring, or have a dynamic or cloud based environment, then Prometheus is a good choice.</p>"}, {"location": "monitoring_comparison/#nagios", "title": "Nagios", "text": "<p>Nagios is an industry leader in IT infrastructure monitoring. It has four different products to choose from:</p> <ul> <li> <p>Nagios XI: Is an enterprise-ready server and network monitoring system that     supplies data to track app or network infrastructure health, performance,     availability, of the components, protocols, and services. It has     a user-friendly interface that allows UI configuration, customized     visualizations, and alert preferences.</p> </li> <li> <p>Nagios Log Server: It's used for log management and analysis of user     scenarios. It has the ability to correlate logged events across different     services and servers in real time, which helps with the investigation of     incidents and the performance of root cause analysis.</p> <p>Because Nagios Log Server\u2019s design is specifically for network security and audits, it lets users generate alerts for suspicious operations and commands. Log Server retains historical data from all events, supplying organizations with everything they need to pass a security audit.</p> </li> <li> <p>Nagios Network Analyzer: It's a tool for collecting and displaying either     metrics or extra information about an application network. It identifies     which IPs are communicating with the application servers and what requests     they\u2019re sending. The Network Analyzer maintains a record of all server     traffic, including who connected a specific server, to a specific port and     the specific request.</p> <p>This helps plan out server and network capacity, plus understand various kinds of security breaches likes unauthorized access, data leaks, DDoS, and viruses or malwares on servers.</p> </li> <li> <p>Nagios Fusion: is a compilation of the three tools Nagios offers. It provides     a complete solution that assists businesses in satisfying any and all of     their monitoring requirements. Its design is for scalability and for     visibility of the application and all of its dependencies.</p> </li> </ul>"}, {"location": "monitoring_comparison/#prometheus", "title": "Prometheus", "text": "<p>Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy.</p> <p>At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets.</p> <p>The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications.</p> <p>There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented.</p>"}, {"location": "monitoring_comparison/#comparison", "title": "Comparison", "text": "<p>For each dimension we'll check how each solution meets the criteria. An aggregation of all the results can be found in the summary.</p>"}, {"location": "monitoring_comparison/#open-source", "title": "Open source", "text": "<p>Only the Nagios Core is open sourced, it provides basic monitoring but it's enhanced by community contributions. It's also the base of the rest solutions, which are proprietary.</p> <p>Prometheus is completely open source under the Apache 2.0 license.</p>"}, {"location": "monitoring_comparison/#community", "title": "Community", "text": "<p>In Nagios, only the Nagios Core is an open-source tool. The rest are proprietary, so there is no community behind them.</p> <p>Community contributions to Nagios are gathered in the Nagios Exchange, it's hard to get other activity statistics than the overall number of contributions, but there are more than 850 addons, 4.5k plugins and 300 documentation contributions.</p> <p>Overall metrics (2021-02-22):</p> Metric Nagios Core Prometheus Stars 932 35.4k Forks 341 5.7k Watch 121 1.2k Commits 3.4k 8.5k Open Issues 195 290 Closed Issues 455 3.5k Open PR 9 116 Closed PR 155 4.5k <p>Last month metrics (2021-02-22):</p> Metric Nagios Core Prometheus Active PR 1 80 Active Issues 3 64 Commits 0 74 Authors 0 35 <p>We can see that Prometheus in comparison with Nagios Core is:</p> <ul> <li>More popular in terms of community contributions.</li> <li>More maintained.</li> <li>Growing more.</li> <li>Development is more distributed.</li> <li>Manages the issues collaboratively.</li> </ul> <p>This comparison is biased though, because Nagios comes from a time where GitHub and Git (and Youtube!) did not exist, and the communities formed around different sites.</p> <p>Also, given that Nagios has almost 20 years of existence, and that it forked from a previous monitoring project (NetSaint), the low number contributions indicate a stable and mature product, whereas the high numbers for Prometheus are indicators of a young, still in development product.</p> <p>Keep in mind that this comparison only analyzes the core, it doesn't take into account the metrics of the community contributions, as it is not easy to aggregate their statistics.</p> <p>Which makes Prometheus one of the biggest open-source projects in existence. It actually has hundreds of contributors maintaining it. The tool continues to be up-to-date to contemporary and popular apps, extending its list of exporters and responding to requests.</p> <p>On 16 January 2014, Nagios Enterprises redirected the nagios-plugins.org domain to a web server controlled by Nagios Enterprises without explicitly notifying the Nagios Plugins community team the consequences of their actions. Nagios Enterprises replaced the nagios-plugins team with a group of new, different members. The community team members who were replaced continued their work under the name Monitoring Plugins along with a new website with the new domain of monitoring-plugins.org. Which is a nasty move against the community.</p>"}, {"location": "monitoring_comparison/#configuration-and-usage", "title": "Configuration and usage", "text": "<p>Neither solution is easy to configure, you need to invest time in them.</p> <p>Nagios is easier to use for non technical users though.</p>"}, {"location": "monitoring_comparison/#visualizations", "title": "Visualizations", "text": "<p>The graphs and dashboards Prometheus provides don't meet today's needs. As a result, users resort to other visualization tools to display metrics collected by Prometheus, often Grafana.</p> <p>Nagios comes with a set of dashboards that fit the requirements of monitoring networks and infrastructure components. Yet, it still lacks graphs for more applicative-related issues.</p> <p>Personally I find Grafana dashboards more beautiful and easier to change. It also has a massive community behind providing customizable dashboards for free.</p>"}, {"location": "monitoring_comparison/#installation", "title": "Installation", "text": "<p>Nagios comes as a downloadable bundle with dedicated packages for every product with Windows or Linux distributions. After downloading and installing the tool, a set of first-time configurations is required. Once you\u2019ve installed the Nagios agents, data should start streaming into Nagios and its generic dashboards.</p> <p>Prometheus deployment is done through Docker containers that can spin up on every machine type, or through pre-compiled or self-compiled binaries.</p> <p>There are community maintained ansible roles for both solutions, doing a quick search I've found a Prometheus one that it's more maintained.</p> <p>For Kubernetes installation, I've only found helm charts for Prometheus.</p>"}, {"location": "monitoring_comparison/#kubernetes-integration", "title": "Kubernetes integration", "text": "<p>Prometheus, as Kubernetes are leading projects of the Cloud Native Computing Foundation, which is a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution.</p> <p>Prometheus has native support to be run in and to monitor Kubernetes clusters. Although Nagios can monitor Kubernetes, it's not meant to be run inside it.</p>"}, {"location": "monitoring_comparison/#documentation", "title": "Documentation", "text": "<p>I haven't used much the Nagios documentation, but I can tell you that even though it's improving Prometheus' is not very complete, and you find yourself often looking at issues and stackoverflow.</p>"}, {"location": "monitoring_comparison/#integrations", "title": "Integrations", "text": "<p>Official Prometheus\u2019 integrations are practically boundless. The long list of existing exporters combined with the user\u2019s ability to write new exporters allows integration with any tool, and PromQL allows users to query Prometheus data from any visualization tool that supports it.</p> <p>Nagios has a very limited list of official integrations. Most of them are operating systems which use the agents to monitor other network components. Others include MongoDB, Oracle, Selenium, and VMware. Once again, the community comes to rescue us with their contributions, keep in mind that you'll need to dive into the exchange for special monitoring needs.</p>"}, {"location": "monitoring_comparison/#alerts", "title": "Alerts", "text": "<p>Prometheus offers Alertmanager, a simple service that allows users to set thresholds and push alerts when breaches occur.</p> <p>Nagios uses a variety of media channels for alerts, including email, SMS, and audio alerts. Because its integration with the operating system is swift, Nagios even knows to generate a WinPopup message with the alert details.</p> <p>On a side note, there is an alert Nagios plugin that alerts for Prometheus query results.</p> <p>As Nagios doesn't support labels for the metrics, so there is no grouping, routing or deduplication of alerts as Prometheus do. Also the silence of alerts is done individually on each alert, while in Prometheus it's done using labels, which is more powerful.</p>"}, {"location": "monitoring_comparison/#advanced-monitorization", "title": "Advanced monitorization", "text": "<p>Nagios alerting is based on the return codes of scripts, Prometheus on the other hand alerts based on metrics, this fact together with the easy and powerful query language PromQL allows the user to make much more rich alerts that better represent the state of the system to monitor.</p> <p>In Nagios there is no concept of making queries to the gathered data.</p>"}, {"location": "monitoring_comparison/#data-storage", "title": "Data storage", "text": "<p>Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation.</p> <p>Prometheus has a defined amount of data that's available (for example 30 days), to be able to store more you need to use Thanos, the prometheus long term storage solution.</p>"}, {"location": "monitoring_comparison/#high-availability", "title": "High availability", "text": "<p>Nagios servers are standalone, they are not meant to collaborate with other instances, so to achieve high availability you need to do it the old way, with multiple independent instances with a loadbalancer upfront.</p> <p>Prometheus can have different servers running collaboratively, monitoring between themselves. So you get high availability for free without any special configuration.</p>"}, {"location": "monitoring_comparison/#dynamic-infrastructure", "title": "Dynamic infrastructure", "text": "<p>In the past, infrastructure had a low rate of change, it was strange that you needed to add something to the monitorization system. Nowadays, with cloud infrastructures and kubernetes, instances are spawned and killed continuously.</p> <p>In Nagios, you need to manually configure each new service following the push architecture. In prometheus, thanks to the pull architecture and service discovery, new services are added and dead one removed automatically.</p>"}, {"location": "monitoring_comparison/#custom-script-execution", "title": "Custom script execution", "text": "<p>Nagios alerting is based on the return codes of scripts, therefore it's straightforward to create an alert based on a custom script.</p> <p>If you need to monitor something in Prometheus, and nobody has done it before, the development costs of an ad-hoc solutions are incredibly high, compared to Nagios. You'd need either to:</p> <ul> <li> <p>Use the script_exporter with     your script.  I've seen their repo, and the last commit is from March, and     they don't have a helm chart to install     it. I've searched     other alternative exporters, but this one seems to be the best for this     approach.</p> <p>The advantages of this approach is that you don't need to create and maintain a new prometheus exporter.</p> <p>The disadvantages though are that you'd have to:</p> <ul> <li>Manually install the required exporter resources in the cluster until a helm chart     exists.</li> <li>Create the helm charts yourself if they don't develop it.</li> <li> <p>Integrate your tool inside the script_exporter docker through one of these     ways:</p> <ul> <li>Changing the exporter Docker image to add it. Which would mean a Docker image     to maintain.</li> <li>Mounting the binary through a volume inside kubernetes. Which would mean     defining a way on how to upload it and assume the high availability penalty     that a stateful kubernetes service entail with the cluster configuration right     now.<ul> <li>If it's not already in your stack, it would mean adding a new exporter to maintain and a new development team to depend on.</li> </ul> </li> </ul> </li> </ul> <p>Alternatively you can use the script exporter binary in a baremetal or virtualized server instead of using a docker, that way you wouldn't need to maintain the different dockers for the different solutions, but you'd need a \"dedicated\" server for this purpose.</p> </li> <li> <p>Create your own exporter. You'd need to create a docker that exposes the command line functionality through a <code>metrics</code> endpoint. You wouldn't depend on a third party development team and would be able to use your script. On the other side it has the following disadvantages:</p> <ul> <li>We would need to create and maintain a new prometheus exporter. That would mean     creating and maintaining the Docker with the command line tool and a simple http     server that exposes the <code>/metrics</code> endpoint, that will run the command whenever the     Prometheus server accesses this endpoint.</li> <li>We add a new exporter to maintain but we develop it ourselves, so we don't depend on     third party developers.</li> </ul> </li> <li> <p>Use other exporters to do the check. For example, if you can deduce the     critical API call that will decide if the script fails or succeeds, you     could use the blackbox exporter to monitor it instead. The advantages of     this solution are:</p> <ul> <li>We don't add new infrastructure to develop or maintain.</li> <li>We don't depend on third party development teams.</li> </ul> <p>And the disadvantage is that if the logic changes, we would need to update how we do the check.</p> </li> </ul>"}, {"location": "monitoring_comparison/#network-monitorization", "title": "Network monitorization", "text": "<p>Both can use the Simple Network Management Protocol (SNMP) to communicate with network switches or other components by using SNMP protocol to query their status.</p> <p>Not being an expert on the topic, knowing it's been one of the core focus of Nagios in the past years and as I've not been able to find good comparison between both, I'm going to suppose that even though both support network monitoring, Nagios does a better job.</p>"}, {"location": "monitoring_comparison/#summary", "title": "Summary", "text": "Metric Nagios Prometheus Open Source \u2713* \u2713\u2713 Community \u2713 \u2713\u2713 Configuration and usage \u2713 x Visualizations \u2713 \u2713\u2713 Ansible Role \u2713 \u2713\u2713 Helm chart x \u2713 Kubernetes x \u2713 Documentation \u2713 x Integrations \u2713 \u2713\u2713 Alerts \u2713 \u2713\u2713 Advanced monitoring x \u2713 Custom script execution \u2713\u2713 \u2713 Data storage x \u2713 Dynamic infrastructure x \u2713 High availability \u2713 \u2713\u2713 Network Monitoring \u2713 \u2713 <p>* Only Nagios Core and the community contributions are open sourced.</p> <p>Where each symbol means:</p> <ul> <li>x: Doesn't meet the criteria.</li> <li>\u2713: Meets the criteria.</li> <li>\u2713\u2713: Meets the criteria and it's better than the other solution.</li> <li>?: I'm not sure.</li> </ul> <p>Nagios is the reference of the old-school monitoring solutions, suitable for basic monitoring of small, static and/or old-school systems where blackbox probing is sufficient.</p> <p>Prometheus is the reference of the new-wave monitoring solutions, suitable for more advanced monitoring of dynamic, new-wave systems (web applications, cloud, containers or Kubernetes) where whitebox monitoring is desired.</p>"}, {"location": "monitoring_comparison/#references", "title": "References", "text": "<ul> <li>Logz io post on Prometheus vs Nagios</li> </ul>"}, {"location": "mopidy/", "title": "Mopidy", "text": "<p>Mopidy is an extensible music server written in Python.</p> <p>The key features are:</p> <ul> <li>Plays music from many sources: local disk, Spotify, SoundCloud, Google Play Music, and more.</li> <li>Can be used as a server: Out of the box, Mopidy is an HTTP server. If you     install the Mopidy-MPD     extension, it becomes an MPD server too. Given that MPD is a popular, old,     and robust solution, you can benefit of the many solutions that exist out     there for MPD.</li> <li>Edit the playlist from any phone, tablet, or computer using a variety of MPD     and web clients.</li> <li>It supports Beets as a library     source.</li> <li>Is hackable: The awesome documentation, being Python based, the extension     system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your     projects.</li> </ul>"}, {"location": "mopidy/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Home</li> </ul>"}, {"location": "mopidy/#developer-info", "title": "Developer info", "text": "<ul> <li>API reference</li> <li>Write your own extension</li> </ul>"}, {"location": "music_management/", "title": "Music management", "text": "<p>Music management is the set of systems and processes to get and categorize songs so it's easy to browse and discover new content. It involves the next actions:</p> <ul> <li>Automatically index and download metadata of new songs.</li> <li>Notify the user when a new song is added.</li> <li>Monitor the songs of an artist, and get them once they are released.</li> <li>A nice interface to browse the existent library, with the possibility of     filtering by author, genre, years, tags or release types.</li> <li>An interface to listen to the music</li> <li>An interface to rate and review library items.</li> <li>An interface to discover new content based on the ratings and item metadata.</li> </ul>"}, {"location": "music_management/#components", "title": "Components", "text": "<p>I've got a music collection built from mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I have  a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation.</p> <p>playlist_generator helped me with the last point, based on the metadata gathered with mep, but it's still not enough.</p> <p>So I'm in my way of migrate all the library to beets, and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata.</p> <p>Once it's implemented, I'll migrate all the metadata to the new system.</p>"}, {"location": "music_management/#lidarr", "title": "Lidarr", "text": "<p>I'm also using Lidarr to manage what content is missing.</p> <p>Both Lidarr and <code>beets</code> get their from MusicBrainz. This means that sometimes some artist may lack a release, if they do, please contribute to MusicBrainz and add the information. Be patient, Lidarr may take some time to fetch the information, as it probably is not available straight away from the API.</p> <p>One awesome feature of Lidarr is that you can select the type of releases you want for each artist. They are defined in <code>Settings/Profiles/Metadata Profiles</code>. To be able to fine grain your settings, you first need to understand what do , Primary Types, Secondary Types and Release Status means.</p> <p>If you want to set the missing picture of an artist, you need to add it at fanart.tv. It's a process that needs the moderators approval, so don't expect it to be automatic. They are really kind when you don't do things right, but still, check their upload guidelines before you contribute.</p>"}, {"location": "musicbrainz/", "title": "MusicBrainz", "text": "<p>MusicBrainz is an open music encyclopedia that collects music metadata and makes it available to the public.</p> <p>MusicBrainz aims to be:</p> <ul> <li>The ultimate source of music information by allowing anyone to contribute and     releasing the data under open licenses.</li> <li>The universal lingua franca for music by providing a reliable and unambiguous     form of music identification, enabling both people and machines to have     meaningful conversations about music.</li> </ul> <p>Like Wikipedia, MusicBrainz is maintained by a global community of users and we want everyone \u2014 including you \u2014 to participate and contribute.</p>"}, {"location": "musicbrainz/#contributing", "title": "Contributing", "text": "<p>Creating an account is free and easy. To be able to add new releases easier, I've seen that there are some UserScript importers, they suggest to use the ViolentMonkey addon and install the desired plugins.</p> <p>With the Discogs one, if you don't see the <code>Import into MB</code> button is because you can't import a Master release, you have to click on a specific release. If that doesn't work for you, check these issues (1 and 2). It works without authentication.</p> <p>All the data is fetched for you except for the album cover, which you have to manually add.</p> <p>Make sure that you fill up the Release Status otherwise it won't show up in Lidarr.</p> <p>Then be patient, sometimes you need to wait hours before the changes are propagated to Lidarr.</p>"}, {"location": "musicbrainz/#filling-up-releases", "title": "Filling up releases", "text": "<p>Some notes on the release fields</p>"}, {"location": "musicbrainz/#primary-types", "title": "Primary types", "text": "<ul> <li> <p>Album: Perhaps better defined as a \"Long Play\" (LP) release, generally     consists of previously unreleased material (unless this type is combined     with secondary types which change that, such as \"Compilation\").</p> </li> <li> <p>Single: A single typically has one main song and possibly         a handful of additional tracks or remixes of the main track; the single         is usually named after its main song; the single is primarily released         to get radio play and to promote release sales.</p> </li> <li> <p>EP: An EP is a so-called \"Extended Play\" release and often contains the     letters EP in the title. Generally an EP will be shorter than a full length     release (an LP or \"Long Play\"), usually less than four tracks, and the     tracks are usually exclusive to the EP, in other words the tracks don't come     from a previously issued release.  EP is fairly difficult to define; usually     it should only be assumed that a release is an EP if the artist defines it     as such.</p> </li> <li> <p>Broadcast: An episodic release that was originally broadcast via radio,     television, or the Internet, including podcasts.</p> </li> <li> <p>Other: Any release that does not fit or can't decisively be placed in any of     the categories above.</p> </li> </ul>"}, {"location": "musicbrainz/#secondary-types", "title": "Secondary types", "text": "<ul> <li> <p>Compilation: A compilation, for the purposes of the MusicBrainz database,     covers the following types of releases:</p> <ul> <li>A collection of recordings from various old sources (not necessarily     released) combined together. For example a \"best of\", retrospective or     rarities type release.</li> <li>A various artists song collection, usually based on a general theme     (\"Songs for Lovers\"), a particular time period (\"Hits of 1998\"), or some     other kind of grouping (\"Songs From the Movies\", the \"Caf\u00e9 del Mar\"     series, etc).</li> </ul> <p>The MusicBrainz project does not generally consider the following to be compilations:</p> <ul> <li>A reissue of an album, even if it includes bonus tracks.</li> <li>A tribute release containing covers of music by another artist.</li> <li>A classical release containing new recordings of works by a classical artist.</li> <li>A split release containing new music by several artists</li> </ul> <p>Compilation should be used in addition to, not instead of, other types: for example, a various artists soundtrack using pre-released music should be marked as both a soundtrack and a compilation. As a general rule, always select every secondary type that applies.</p> </li> <li> <p>Soundtrack: A soundtrack is the musical score to a movie, TV series, stage     show, video game, or other medium. Video game CDs with audio tracks should     be classified as soundtracks because the musical properties of the CDs are     more interesting to MusicBrainz than their data properties.</p> </li> <li> <p>Spokenword: Non-music spoken word releases.</p> </li> <li> <p>Interview: An interview release contains an interview, generally with an     artist.</p> </li> <li> <p>Audiobook: An audiobook is a book read by a narrator without music.</p> </li> <li> <p>Audio drama: An audio drama is an audio-only performance of a play (often,     but not always, meant for radio). Unlike audiobooks, it usually has multiple     performers rather than a main narrator.</p> </li> <li> <p>Live: A release that was recorded live.</p> </li> <li> <p>Remix: A release that primarily contains remixed material.</p> </li> <li> <p>DJ-mix: A DJ-mix is a sequence of several recordings played one after the     other, each one modified so that they blend together into a continuous flow     of music. A DJ mix release requires that the recordings be modified in some     manner, and the DJ who does this modification is usually (although not     always) credited in a fairly prominent way.</p> </li> <li> <p>Mixtape/Street: Promotional in nature (but not necessarily free), mixtapes     and street albums are often released by artists to promote new artists, or     upcoming studio albums by prominent artists. They are also sometimes used to     keep fans' attention between studio releases and are most common in rap     &amp; hip hop genres. They are often not sanctioned by the artist's label, may     lack proper sample or song clearances and vary widely in production and     recording quality. While mixtapes are generally DJ-mixed, they are distinct     from commercial DJ mixes (which are usually deemed compilations) and are     defined by having a significant proportion of new material, including     original production or original vocals over top of other artists'     instrumentals. They are distinct from demos in that they are designed for     release directly to the public and fans; not to labels.</p> </li> </ul>"}, {"location": "musicbrainz/#release-status", "title": "Release status", "text": "<p>Status describes how \"official\" a release is. Possible values are:</p> <ul> <li>Official: Any release officially sanctioned by the artist and/or their     record company. Most releases will fit into this category.</li> <li>Promotional: A give-away release or a release intended to promote an     upcoming official release (e.g. pre-release versions, releases included     with a magazine, versions supplied to radio DJs for air-play).</li> <li>Bootleg: An unofficial/underground release that was not sanctioned by     the artist and/or the record company. This includes unofficial live     recordings and pirated releases.</li> <li>Pseudo-release: An alternate version of a release where the titles have     been changed. These don't correspond to any real release and should be     linked to the original release using the transl(iter)ation     relationship.</li> </ul>"}, {"location": "musicbrainz/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "nas/", "title": "NAS", "text": "<p>Network-attached storage or NAS, is a computer data storage server connected to a computer network providing data access to many other devices. Basically a computer where you can attach many hard drives.</p> <p>A quick search revealed two kind of solutions:</p> <ul> <li>Plug and play.</li> <li>Do It Yourself.</li> </ul> <p>The first ones are servers that some companies like Synology or Qnap sell with their own software. They're meant for users that want to have something that works and does not give too much work at the expense of being restricted to what the provider gives you. The second ones are similar servers but you need to build it from scratch with existent tools like Linux or ZFS. I personally feel a bit constrained and vendor locked by the first. For example they are not user-repairable, so if a part breaks after warranty, you have to replace the whole server. Or you may use without knowing their proprietary storage format, that may mean that it's difficult to recover the data once you want to move away from their solution It makes sense then to trade time and dedication for freedom.</p>"}, {"location": "nas/#software", "title": "Software", "text": ""}, {"location": "nas/#truenas", "title": "TrueNAS", "text": "<p>TrueNAS (formerly known as FreeNAS) is one of the most popular solution operating systems for storage servers. It\u2019s open-source, and it\u2019s been around for almost 20 years, so it seemed like a reliable choice. As you can use it on any hardware, therefore removing the vendor locking, and it's open source. They have different solutions, being the Core the most basic. They also have Truenas Scale with which you could even build distributed systems. A trusted friend prevented me from going in this direction as he felt that GlusterFS over ZFS was not a good idea.</p> <p>TrueNAS core looked good but I still felt a bit constrained and out of control, so I discarded it.</p>"}, {"location": "nas/#unraid", "title": "Unraid", "text": "<p>Unraid is a proprietary Linux-based operating system designed to run on home media server setups that operates as a network-attached storage device, application server, and virtualization host. I've heard that it can do wonders like tweaking the speed of disks based on the need. It's not \"that expensive\" but still it's a proprietary so nope.</p>"}, {"location": "nas/#debian-with-zfs", "title": "Debian with ZFS", "text": "<p>This solution gives you the most freedom and if you're used to use Linux like me, is the one where you may feel most at home.</p> <p>The idea is to use a normal Debian and configure it to use ZFS to manage the storage.</p>"}, {"location": "nas/#hardware", "title": "Hardware", "text": "<p>Depending the amount of data you need to hold and how do you expect it to grow you need to find the solution that suits your needs. After looking to many I've decided to make my own from scratch.</p> <p>Warning: If you pursue the beautiful and hard path of building one yourself, don't just buy the components online, there are thousands of things that can go wrong that will make you loose money. Instead go to your local hardware store and try to build the server with them. Even if it's a little bit more expensive you'll save energy and peace of mind.</p>"}, {"location": "nas/#disks", "title": "Disks", "text": "<p>ZFS tutorials suggest buying everything all at once for the final stage of your server. Knowing the disadvantages it entails, I'll start with a solution for 16TB that supports to easily expand in the future. I'll start with 5 disks of 8TB, make sure you read the ZFS section on storage planning to understand why. The analysis of choosing the disks to hold data gave two IronWolf Pro and two Exos 7E8 for a total amount of 1062$. I'll add another IronWolf Pro as a cold spare.</p>"}, {"location": "nas/#ram", "title": "RAM", "text": "<p>Most ZFS resources suggest using ECC RAM. The provider gives me two options:</p> <ul> <li>Kingston Server Premier DDR4 3200MHz 16GB CL22</li> <li>Kingston Server Premier DDR4 2666MHz 16GB CL19</li> </ul> <p>I'll go with two modules of 3200MHz CL22 because it has a smaller RAM latency.</p> <p>Which was a bummer, as it turned out that my motherboard doesn't support Registered ECC ram, but only Unregistered ECC ram.</p>"}, {"location": "nas/#motherboard", "title": "Motherboard", "text": "<p>After reading these reviews(1, 2) I've come to the decision to purchase the ASRock X570M Pro4 because, It supports:</p> <ul> <li>8 x SATA3 disks</li> <li>2 x M.2 disks</li> <li>4 x DDR4 RAM slots with speeds up to 4200+ and ECC support</li> <li>1 x AMD AM4 Socket Ryzen\u2122 2000, 3000, 4000 G-Series, 5000 and 5000 G-Series   Desktop Processors</li> <li>Supports NVMe SSD as boot disks</li> <li>Micro ATX Form Factor.</li> </ul> <p>And it gives me room enough to grow:</p> <ul> <li>It supports PCI 4.0 for the M.2 which is said to be capable of perform twice   the speed compared to previous 3<sup>rd</sup> generation. the chosen M2 are of 3<sup>rd</sup>   generation, so if I need more speed I can change them.</li> <li>I'm only going to use 2 slots of RAM giving me 32GB, but I could grow 32 more   easily.</li> </ul>"}, {"location": "nas/#cpu", "title": "CPU", "text": "<p>After doing some basic research I've chosen the Ryzen 7 5700x.</p>"}, {"location": "nas/#cpu-cooler", "title": "CPU cooler", "text": "<p>After doing some basic research I've chosen the Dark Rock 4 but just because the Enermax ETS-T50 AXE Silent Edition doesn't fit my case :(.</p>"}, {"location": "nas/#graphic-card", "title": "Graphic card", "text": "<p>As it's going to be a server and I don't need it for transcoding or gaming, I'll start without a graphic card.</p>"}, {"location": "nas/#server-case", "title": "Server Case", "text": "<p>Computer cases are boxes that hold all your computer components together.</p> <p>I'm ruling out the next ones:</p> <ul> <li>Fractal Design R6:   More expensive than the Node 804 and it doesn't have hot swappable disks.</li> <li>Silverstone Technology SST-CS381: Even though it's gorgeous it's too   expensive.</li> <li>Silverstone DS380: It only supports Mini-ITX which I don't have.</li> </ul> <p>The remaining are:</p> Model Fractal Node 804 Silverstone CS380 Form factor Micro - ATX Mid tower Motherboard Micro ATX Micro ATX Drive bays 8 x 3.5\", 2 x 2.5\" 8 x 3.5\", 2 x 5.25\" Hot-swap No yes Expansion Slots 5 7 CPU cooler height 160mm 146 mm PSU compatibility ATX ATX Fans Front: 4, Top: 4, Rear 3 Side: 2, Rear: 1 Price 115 184 Size 34 x 31 x 39 cm 35 x 28 x 21 cm <p>I like the Fractal Node 804 better and it's cheaper.</p>"}, {"location": "nas/#power-supply-unit", "title": "Power supply unit", "text": "<p>Using PCPartPicker I've seen that with 4 disks it consumes approximately 264W, when I have the 8 disks, it will consume up to 344W, if I want to increase the ram then it will reach 373W. So in theory I can go with a 400W power supply unit.</p> <p>You need to make sure that it has enough wires to connect to all the disks. Although that usually is not a problem as there are adapters:</p> <ul> <li>Molex to sata</li> <li>Sata to sata</li> </ul> <p>After an analysis on the different power supply units, I've decided to go with Be Quiet! Straight Power 11 450W Gold</p>"}, {"location": "nas/#wires", "title": "Wires", "text": "<p>Usually disks come without sata wires, so you have to buy them</p>"}, {"location": "nas/#hardware-conclusion", "title": "Hardware conclusion", "text": "Piece Purpose Number Total price ($) Seagate IronWolf Pro (8TB) Data disk 3 Seagate Exos 7E8 (8TB) Data disk 2 438 WD Red SN700 (1TB) M.2 disks 2 246 Kingston Server DDR4 (16GB) ECC RAM 2 187 AsRock X570M Pro4 Motherboard 1 225 Ryzen 7 5700x CPU 1 274 Fractal Node 804 Case 1 137 Dark Rock 4 CPU Cooler 1 Be Quiet! Straight Power 11 PSU 1 99 Sata wires Sata 3 6.5 No graphic card Graphic card 0 0 CPU thermal paste thermal paste 0 0"}, {"location": "nas/#references", "title": "References", "text": "<ul> <li>mtlynch NAS building guide</li> <li>NAS master building guide</li> </ul>"}, {"location": "networkx/", "title": "NetworkX", "text": "<p>NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.</p>"}, {"location": "networkx/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Home</li> </ul>"}, {"location": "news_management/", "title": "News Management", "text": "<p>The information world of today is overwhelming. It can reach a point that you just want to disconnect so as to avoid the continuous bombardment, but that leads to loosing connection with what's happening in the world. Without knowing what's going on it's impossible to act to shape it better.</p> <p>The problem to solve is to:</p> <ul> <li>Keep updated on the important articles for you.</li> <li>Don't invest too much time on it.</li> <li>Don't loose time reading articles you're not interested on.</li> </ul>"}, {"location": "news_management/#workflow", "title": "Workflow", "text": "<p>I've found three information types to explore:</p> <ul> <li>Written content: articles, blogs, newspapers...</li> <li>Listened content: mainly podcasts.</li> <li>Viewed content: youtube, twich channels.</li> </ul> <p>Each has it's advantages and disadvantages. I like the written content as it lets me decide the pace of information ingestion, it's compatible with incremental reading, and it's the best medium to learn by making annotations and summaries, it requires your full attention though. Listened content is best to keep updated while you do brainless tasks such as cooking or cleaning, but it makes difficult to save references or ideas. Viewed content is as attention demanding as reading unless you don't care about the visual content and take it as a podcast.</p>"}, {"location": "news_management/#written-content", "title": "Written content", "text": "<p>To process the written content I use an RSS reader (Feeder) to gather all written content in one place. I skim through the elements without reading them, and I send the ones that catch my attention to wallabag for later reading. Then I go to wallabag and read the elements that feels more attractive at that moment.</p> <p>Before starting to read, I define the amount of time I want to spend on getting updated. Half of that time goes to skimming through, and the other to deep reading the selected content. You'll probably won't be able to process either the whole content on your RSS reader nor the selected content, that's why a recommender system would be awesome.</p> <p>Finding the reading devices is very important. I prefer to browse it on a tablet as it's much more pleasant than a mobile or a computer, an e-reader would be better, although wallabag is supported on some e-readers, I haven't tried it yet. I can't wait for the PineNote to be released.</p> <p>The moments I've found suitable for reading content are while eating breakfast or dinner when I'm alone.</p>"}, {"location": "news_management/#listened-content", "title": "Listened content", "text": "<p>I've selected a small number of podcasts that I listen with AntennaPod while cooking or cleaning, instead of listening directly from the mobile, I use a bluetooth loudspeaker that I carry everywhere I go (at home! use headphones when you are outside. people with loudspeakers on the public transport or streets are hateful), if there is a reference I want to save, I write it down in the mobile inbox and process it later with pynbox.</p>"}, {"location": "news_management/#the-perfect-software-solution", "title": "The perfect software solution", "text": "<p>My current workflow could be improved by software, currently the key features I'd want are:</p> <ul> <li>One place for all sources: It's useless to go to <code>n</code> different websites to see     if there is new information. RSS has been with us for too long to fall on     that.</li> <li>The user has control of it's data: The user should be able to decide which     information is private and which one is public. Only the people it trusts     will have access to it's private data.</li> <li>There must be a filter of the incoming elements: It doesn't matter how well     you choose your sources, there's always going to be content that is not     interesting for you. So there needs to be a powerful filtering system.</li> </ul>"}, {"location": "news_management/#content-filtering", "title": "Content filtering", "text": "<p>Filtering content is a subsection of the recommender systems, of all the basic models, the ones that apply are:</p> <ul> <li>Collaborative     filtering: Where the     data of many users is used to filter out the relevant items.</li> <li>Content based     filtering: Where     the data of the user on past items is used to filter new elements.</li> </ul>"}, {"location": "news_management/#collaborative-filtering", "title": "Collaborative filtering", "text": "<p>External users give information on how they see the items, and the algorithm can use that data to decide which ones are relevant for a desired user. It's how social networks operate, and if you use Mastodon, Reddit, HackerNews, Facebook, Twitter or similar, then you may not even be interested in this article at all.</p> <p>All those platforms have one or more of the next flaws for me:</p> <ul> <li>There is no one place that aggregates the information of all information     sources.</li> <li>You can't mark content as seen.</li> <li>Your data lives in the servers of other people.</li> <li>They are based on closed sourced software.</li> <li>There's a mix of information with conversation between users.</li> <li>You may not be interested in all the elements the source publishes.</li> </ul> <p>Some of them support the export to RSS and if they don't, you'll probably can find bridge platforms that do. That'll solve all the problems but the two last ones. I've used the RSS feed of mastodon users, and I finally removed them because there was a lot of content I didn't like. I've also used the reddit and hackernews rss, but again, most of the posts weren't interesting to me.</p> <p>A partial solution I've been using with my friends is to share relevant content through wallabag. It's a read-it-later application that creates RSS feeds for the liked elements. That way you can get filtered content from the people you know. The downsides are:</p> <ul> <li>You get one feed for all the content, you don't have the possibility to filter     out by categories or tags.</li> <li>It's not very collaborative. You need to ask one by one for their RSS.</li> </ul> <p>A prettier (but more difficult) solution would be to create communities that commit to share the articles interesting for a specific topic. Like HackerNews but at smaller level, where you know and trust the members of the community, and where people outside the community can not upload data, just read it.</p> <p>This could be done through one instance of <code>lemmy</code>, with a closed community, but that service is envisioned to be federated and to encourage the interaction of the users on the site.</p> <p>Another possibility would be to create a simple backend that mimics the api interface of wallabag, so that users could use the browser addon and all the interfaces existent, for example the Feeder integration with wallabag instances. You'll be able to create communities, and invite users to them, they will be able to link a \"wallabag\" tag with a specific community channel. That way, an RSS feed will be created per community with all the articles shared by their members. Optionally, users could decide to ignore the entries of another user.</p> <p>But for any of these solutions to work, you'll need to convince the people to tweak how they browse the internet in order to contribute to the system, which it's kind of difficult :(.</p>"}, {"location": "news_management/#content-based-filtering", "title": "Content based filtering", "text": "<p>If you don't manage to convince the people to use collaborative filtering, you're on your own. Your best bet then is to deduce which elements are interesting based on how you rated other elements.</p> <p>I haven't found any rss reader that does good filtering of the elements. I've used Newsblur in the past, you're able to assign scores for the element tags and user. But I didn't find it very useful. Also when it comes to self host it, it's a little bit of a nightmare.</p> <p>Intermediate solutions between the sources and the reader aren't a viable option either, as you need to interact with that middleware outside the RSS reader.</p>"}, {"location": "notmuch/", "title": "notmuch", "text": "<p>notmuch is a command-line based program for indexing, searching, reading, and tagging large collections of email messages.</p>"}, {"location": "notmuch/#installation", "title": "Installation", "text": "<p>In order to use Notmuch, you will need to have your email messages stored in your local filesystem, one message per file. You can use <code>mbsync</code> to do that.</p> <pre><code>sudo apt-get install notmuch\n</code></pre>"}, {"location": "notmuch/#configuration", "title": "Configuration", "text": "<p>To configure Notmuch, just run</p> <pre><code>notmuch\n</code></pre> <p>This will interactively guide you through the setup process, and save the configuration to <code>~/.notmuch-config</code>. If you'd like to change the configuration in the future, you can either edit that file directly, or run <code>notmuch setup</code>.</p> <p>If you plan to use <code>afew</code> set the tags to <code>new</code>.</p> <p>To test everything works as expected, and create a database that indexes all of your mail run:</p> <pre><code>notmuch new\n</code></pre>"}, {"location": "notmuch/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "ombi/", "title": "Ombi", "text": "<p>Ombi is a self-hosted web application that automatically gives your shared Jellyfin users the ability to request content by themselves! Ombi can be linked to multiple TV Show and Movie DVR tools to create a seamless end-to-end experience for your users.</p> <p>If Ombi is not for you, you may try Overseerr.</p>"}, {"location": "ombi/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "openproject/", "title": "OpenProject", "text": "<p>OpenProject is an Open source project management software.</p> <p>The benefits over other similar software are:</p> <ul> <li>It's popular: More than 6.2k stars on github, 1.7k forks.</li> <li>It's development is active: in the   last week they've merged 44 merged pull requests by 16 people.</li> <li>They use their own software to   track their bugs</li> <li>Easy to install</li> <li>Easy to use</li> <li>The community version is flexible enough to adapt to different workflows.</li> <li>Good installation and operation's documentation.</li> <li>Very good API documentation.</li> <li>Supports LDAP</li> </ul> <p>The things I don't like are:</p> <ul> <li>It's not keyboard driven, you use the mouse a lot.</li> <li>The task editor doesn't support markdown</li> <li>You can't sort the work package views</li> <li>You   can't fold the hierarchy trees   so it's difficult to manage the tasks once you have many. You can see   my struggles with this issue here.</li> <li> <p>You can't order the tasks inside the <code>Relations</code> tab of a task.</p> </li> <li> <p>You can't propagate easily the change of attributes to all it's children. For   example if you want to make a parent task and all it's children appear on a   report that is searching for an attribute. You need to go to a view where you   see all the tasks (an hierarchy view) select them all and do a bulk edit.</p> </li> <li> <p>Versions or sprints can't be used across projects even if they are subprojects   of a project.</p> </li> <li>The manual order of the tasks is not saved across views, so you need to have   independent views in order not to get confused on which is the prioritized   list.</li> <li>Data can be exported as XML or CSV but it doesn't export everything. You have   access to the database though, so if you'd like a better extraction of the   data you in theory can do a selective dump of whatever you need.</li> <li>It doesn't yet have   tag support.   You can meanwhile add the strings you would use as tags in the description,   and then filter by text in description.</li> <li>There is no demo instance where you can try it. It's easy though to launch a   Proof of Concept environment yourself if you already know   <code>docker-compose</code>.</li> <li>You can't hide an element from a report for a day. For example if there is a   blocked task that you can't work on for today, you can't hide it till   tomorrow.</li> <li>Even thought the   Community (free) version has many features   the next aren't:</li> <li>The status column is not showing the status color.</li> <li>Status boards:     you can't have Kanban boards that show the state of the issues as columns.     You can make it yourself through a Basic board and with the columns as the     name of the state. But when you transition an issue from state, you need to     move the issue and change the property yourself. I've thought of creating a     script that works with the API to do this automatically, maybe through the     webhooks of the openproject, but it would make more sense to spend time on     <code>pydo</code>.</li> <li>Version boards:     Useful to transition issues between sprints when you didn't finish them in     time. Probably this is easily solved through bulk editing the issues.</li> <li>Custom actions     looks super cool, but as this gives additional value compared with the     competitors, I understand it's a paid feature.</li> <li>Display relations in the work package list:     It would be useful to quickly see which tasks are blocked, by whom and why.     Nothing critical though.</li> <li>Multiselect custom fields:     You can only do single valued fields. Can't understand why this is a paid     feature.</li> <li>2FA authentication is only an Enterprise feature.</li> <li>OpenID and SAML     are an enterprise feature.</li> </ul>"}, {"location": "openproject/#installation", "title": "Installation", "text": ""}, {"location": "openproject/#proof-of-concept", "title": "Proof of Concept", "text": "<p>It can be installed both on kubernetes and through docker-compose). I'm going to follow the <code>docker-compose</code> instructions for a Proof of Concept:</p> <ul> <li>Clone this repository:</li> </ul> <pre><code>git clone https://github.com/opf/openproject-deploy --depth=1 --branch=stable/12 openproject\ncd openproject/compose\n</code></pre> <ul> <li>Make sure you are using the latest version of the Docker images:</li> </ul> <pre><code>docker-compose pull\n</code></pre> <ul> <li>Launch the containers:</li> </ul> <pre><code>OPENPROJECT_HTTPS=false PORT=127.0.0.1:8080 docker-compose up\n</code></pre> <p>Where:</p> <ul> <li><code>OPENPROJECT_HTTPS=false</code>: Is required if you want to try it locally and you     haven't yet configured the proxy to do the ssl termination.</li> <li><code>PORT=127.0.0.1:8080</code>: Is required so that you only expose the service to     your localhost.</li> </ul> <p>After a while, OpenProject should be up and running on http://localhost:8080.</p>"}, {"location": "openproject/#production", "title": "Production", "text": "<p>It can be installed both on kubernetes and through docker-compose). I'm going to follow the <code>docker-compose</code>:</p> <ul> <li>Clone this repository:</li> </ul> <pre><code>git clone https://github.com/opf/openproject-deploy --depth=1 --branch=stable/12 /data/config\ncd /data/config/compose\n</code></pre> <ul> <li>Make sure you are using the latest version of the Docker images:</li> </ul> <pre><code>docker-compose pull\n</code></pre> <ul> <li> <p>Tweak the <code>docker-compose.yaml</code> file through the <code>docker-compose.override.yml</code></p> </li> <li> <p>Add the required environmental variables through a <code>.env</code> file</p> </li> </ul> <pre><code>OPENPROJECT_HOST__NAME=openproject.example.com\nOPENPROJECT_SECRET_KEY_BASE=secret\nPGDATA=/path/to/postgres/data\nOPDATA=/path/to/openproject/data\n</code></pre> <p>Where <code>secret</code> is the value of   <code>head /dev/urandom | tr -dc A-Za-z0-9 | head   -c 32 ; echo ''</code></p> <ul> <li>Launch the containers:</li> </ul> <pre><code>docker-compose up\n</code></pre> <ul> <li> <p>Configure the ssl proxy.</p> </li> <li> <p>Connect with user <code>admin</code> and password <code>admin</code>.</p> </li> <li> <p>Create the systemd service in <code>/etc/systemd/system/openproject.service</code></p> </li> </ul> <pre><code>[Unit]\nDescription=openproject\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nUser=root\nGroup=docker\nWorkingDirectory=/data/config/compose\n# Shutdown container (if running) when unit is started\nTimeoutStartSec=100\nRestartSec=2s\n# Start container when unit is started\nExecStart=/usr/bin/docker-compose up\n# Stop container when unit is stopped\nExecStop=/usr/bin/docker-compose down\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"}, {"location": "openproject/#operations", "title": "Operations", "text": "<ul> <li>Doing backups</li> <li>Upgrading</li> <li>Restoring the service</li> </ul>"}, {"location": "openproject/#workflows", "title": "Workflows", "text": ""}, {"location": "openproject/#the-plans", "title": "The plans", "text": "<p>I usually do a day, week, month, trimestre and year plans. To model this in OpenProjects I've created a version with each of these values. To sort them as I want them to appear I had to append a number so it would be:</p> <ul> <li> <ol> <li>Day</li> </ol> </li> <li> <ol> <li>Week</li> </ol> </li> <li> <ol> <li>Month</li> </ol> </li> <li>...</li> </ul>"}, {"location": "openproject/#tips", "title": "Tips", "text": ""}, {"location": "openproject/#bulk-editing", "title": "Bulk editing", "text": "<p>Select the work packages to edit holding the <code>Ctrl</code> key and then right click over them and select <code>Bulk Edit</code>.</p>"}, {"location": "openproject/#form-editing", "title": "Form editing", "text": "<p>Even though it looks that you can't tweak the forms of the issues you can add the sections on the right grey column to the ones on the left blue. You can't however:</p> <ul> <li>Remove a section.</li> <li>Rename a section.</li> </ul>"}, {"location": "openproject/#tweaking-the-work-package-status", "title": "Tweaking the work package status", "text": "<p>Once you create a new status you need to tweak the workflows to be able to transition the different statuses.</p> <p>In the admin settings select \u201cWorkflow\u201d in the left menu, select the role and Type to which you want to assign the status. Then uncheck \u201cOnly display statuses that are used by this type\u201d and click \u201cEdit\u201d.</p> <p>In the table check the transitions you want to allow for the selected role and type and click \u201cSave\u201d.</p>"}, {"location": "openproject/#deal-with-big-number-of-tasks", "title": "Deal with big number of tasks", "text": "<p>As the number of tasks increase, the views of your work packages starts becoming more cluttered. As you can't fold the hierarchy trees it's difficult to efficiently manage your backlog.</p> <p>I've tried setting up a work package type that is only used for the subtasks so that they are filtered out of the view, but then you don't know if they are parent tasks unless you use the details window. It's inconvenient but having to collapse the tasks every time it's more cumbersome. You'll also need to reserve the selected subtask type (in my case <code>Task</code>) for the subtasks.</p>"}, {"location": "openproject/#sorting-work-package-views", "title": "Sorting work package views", "text": "<p>They are sorted alphabetically, so the only way to sort them is by prepending a number. You can do <code>0. Today</code> instead of <code>Today</code>. It's good to do big increments between numbers, so the next report could be <code>10. Backlog</code>. That way if you later realize you want another report between Today and Backlog, you can use <code>5. New Report</code> and not rename all the reports.</p>"}, {"location": "openproject/#pasting-text-into-the-descriptions", "title": "Pasting text into the descriptions", "text": "<p>When I paste the content of the clipboard in the description, all new lines are removed (<code>\\n</code>), the workaround is to paste it inside a <code>code snippet</code>.</p>"}, {"location": "openproject/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Bug tracker</li> <li>Git</li> <li>Homepage</li> <li>Upgrading notes</li> </ul>"}, {"location": "oracle_database/", "title": "Oracle Database", "text": "<p>Oracle Database is an awful proprietary database, run away from it!</p>"}, {"location": "oracle_database/#install", "title": "Install", "text": "<ul> <li>Download or clone the files of their docker     repository.</li> <li>Create an account in their page to be able to download the required binary     files.     Fake person generator might come     handy for this step.</li> <li> <p>Download the     files.</p> </li> <li> <p>After downloading the file we need to copy it to the folder referring to the oracle version in the cloned folder. In this case, 19.3.0:</p> <pre><code>mv ~/Download/LINUX.X64_193000_db_home.zip ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/\n</code></pre> </li> <li> <p>The next step is to build the image. You need at least 20G free in     <code>/var/lib/docker</code>.</p> <pre><code>./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/buildDockerImage.sh -v 19.3.0 -e\n</code></pre> </li> <li> <p>Confirm that the image was created     <pre><code>docker images\n\nREPOSITORY                                TAG           IMAGE ID       CREATED          SIZE\noracle/database                           19.3.0-ee     d8be8934332d   53 minutes ago   6.54GB\n</code></pre></p> </li> <li> <p>Run the database docker.     <pre><code>docker run --name myOracle1930 \\\n -p 127.0.0.1:1521:1521 \\\n -p 127.0.0.1:5500:5500 \\\n -e ORACLE_SID=ORCLCDB \\\n -e ORACLE_PDB=ORCLPDB1 \\\n -e ORACLE_PWD=root \\\n -e INIT_SGA_SIZE=1024 \\\n -e INIT_PGA_SIZE=1024 \\\n -e ORACLE_CHARACTERSET=AL32UTF8 \\\n oracle/database:19.3.0-ee\n</code></pre></p> </li> </ul>"}, {"location": "origami/", "title": "Origami", "text": "<p>Origami, is the art of paper folding, it comes from ori meaning \"folding\", and kami meaning \"paper\" (kami changes to gami due to rendaku)). In modern usage, the word \"origami\" is used as an inclusive term for all folding practices, regardless of their culture of origin. The goal is to transform a flat square sheet of paper into a finished sculpture through folding and sculpting techniques. Modern origami practitioners generally discourage the use of cuts, glue, or markings on the paper.</p>"}, {"location": "origami/#references", "title": "References", "text": "<ul> <li>Mark1626 Digital garden origami     section, for example the     Clover and Hydrangea     Tesslation.</li> </ul>"}, {"location": "osmand/", "title": "OsmAnd", "text": "<p>OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps. Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.</p>"}, {"location": "osmand/#issues", "title": "Issues", "text": "<ul> <li>Live map update not     working: test that it     works.</li> <li>Add amenity to favourites:     Once it's done, remove the \"Restaurant\", or any other amenity words from the     name of the favourite.</li> <li>Search within favourites     description: Nothing to     do.</li> <li>Favourites inherit the POI     data: Nothing to do.</li> </ul>"}, {"location": "osmand/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li>Reddit</li> </ul>"}, {"location": "outrun/", "title": "Outrun", "text": "<p>Outrun lets you execute a local command using the processing power of another Linux machine.</p>"}, {"location": "outrun/#installation", "title": "Installation", "text": "<pre><code>pip install outrun\n</code></pre>"}, {"location": "outrun/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "park_programming/", "title": "Park programming", "text": "<p>Park programming is as you may guess, programming in parks, something I'm trying these days, let's see how it goes.</p>"}, {"location": "park_programming/#how-to-park-program", "title": "How to park program", "text": "<p>To get the best experience out of it I've seen that before you get out it's good to:</p> <ul> <li>Store music in a directory so you don't have to stream it online.</li> <li>Find the best spots.</li> <li>Find the best times.</li> <li>Get out of your dark themed comfort zone and configure a light theme for the     windows you're going to use.</li> <li>Put on sun screen!</li> <li>Bring enough water.</li> <li>Bring a small cushion for the ass, and maybe another for the back.</li> <li>Pee before going out.</li> <li>Go with someone. I haven't yet tried how comfortable is park pair programming     though.</li> </ul> <p>While you're at it:</p> <ul> <li>Teether the internet from the mobile.</li> <li>Use headphones, be respectful <code>(\u0482\u2323\u0300_\u2323\u0301)</code>.</li> <li>Sit well, with your back straight.</li> </ul>"}, {"location": "park_programming/#find-the-best-spots", "title": "Find the best spots", "text": "<p>The best spots are the ones that meet the next criteria:</p> <ul> <li>Quiet places with few human transit.</li> <li>Green surrounding environment.</li> <li>Pleasant heat, sunlight and humidity.</li> <li>Correct seat height.</li> <li>Has laptop support.</li> <li>Seat with back support.</li> <li>Far from bugs.</li> </ul> <p>So far the best spots I've found have been:</p> <ul> <li>Benches in parks.</li> <li>Chess desks.</li> </ul> <p>Look out for different spots to fulfill the different scenarios in terms of amount of heat and sunlight.</p>"}, {"location": "park_programming/#find-the-best-times", "title": "Find the best times", "text": "<p>The best times are the ones that maximize the best spots criteria so:</p> <ul> <li>Weekdays over weekend</li> <li>Early mornings and late afternoons on summer, middle day on spring and autumn.</li> </ul>"}, {"location": "park_programming/#benefits", "title": "Benefits", "text": "<p>By doing park programming you:</p> <ul> <li>Move your ass out of your lair.</li> <li>Get a sun bath</li> <li>Breath fresh open air.</li> <li>Get to smile at passing by dogs or enjoy seeing them play.</li> <li>See trees and nature.</li> <li>Get higher chances to interact with other humans.</li> </ul>"}, {"location": "park_programming/#inconveniences", "title": "Inconveniences", "text": "<ul> <li>You'll probably don't be as comfortable as in your common development place.</li> </ul>"}, {"location": "pdm/", "title": "PDM", "text": "<p>PDM is a modern Python package manager with PEP 582 support. It installs and manages packages in a similar way to npm that doesn't need to create a virtualenv at all!</p>"}, {"location": "pdm/#features", "title": "Features", "text": "<ul> <li>PEP 582 local package installer and runner, no virtualenv involved at all.</li> <li>Simple and relatively fast dependency resolver, mainly for large binary   distributions.</li> <li>A PEP 517 build backend.</li> <li>PEP 621 project metadata.</li> </ul>"}, {"location": "pdm/#installation", "title": "Installation", "text": ""}, {"location": "pdm/#recommended-installation-method", "title": "Recommended installation method", "text": "<pre><code>curl -sSL https://raw.githubusercontent.com/pdm-project/pdm/main/install-pdm.py | python3 -\n</code></pre> <p>For security reasons, you should verify the checksum. The sha256 checksum is: <code>70ac95c53830ff41d700051c9caebd83b2b85b5d6066e8f853006f9f07293ff0</code>, if it doesn't match check if there is a newer version.</p>"}, {"location": "pdm/#other-methods", "title": "Other methods", "text": "<pre><code>pip install --user pdm\n</code></pre>"}, {"location": "pdm/#enable-pep-582-globally", "title": "Enable PEP 582 globally", "text": "<p>To make the Python interpreters aware of PEP 582 packages, one need to add the <code>pdm/pep582/sitecustomize.py</code> to the Python library search path.</p> <pre><code>pdm --pep582 zsh &gt;&gt; ~/.zshrc\n</code></pre>"}, {"location": "pdm/#use-it-with-the-ide", "title": "Use it with the IDE", "text": "<p>Now there are not built-in support or plugins for PEP 582 in most IDEs, you have to configure your tools manually. They say how to configure Pycharm and VSCode, but there's still no instructions for vim.</p> <p>PDM will write and store project-wide configurations in <code>.pdm.toml</code> and you are recommended to add following lines in the <code>.gitignore</code>:</p> <pre><code>.pdm.toml\n__pypackages__/\n</code></pre>"}, {"location": "pdm/#usage", "title": "Usage", "text": "<p>PDM provides a bunch of handful commands to help manage your project and dependencies.</p>"}, {"location": "pdm/#initialize-a-project", "title": "Initialize a project", "text": "<pre><code>pdm init\n</code></pre> <p>Answer several questions asked by PDM and a <code>pyproject.toml</code> will be created for you in the project root:</p> <pre><code>[project]\nname = \"pdm-test\"\nversion = \"0.0.0\"\ndescription = \"\"\nrequires-python = \"&gt;=3.7\"\ndependencies = []\n[[project.authors]]\nname = \"Frost Ming\"\nemail = \"mianghong@gmail.com\"\n\n[project.license]\ntext = \"MIT\"\n</code></pre> <p>If <code>pyproject.toml</code> is already present, it will be updated with the metadata following the PEP 621 specification.</p>"}, {"location": "pdm/#import-project-metadata-from-existing-project-files", "title": "Import project metadata from existing project files", "text": "<p>If you are already other package manager tools like <code>Pipenv</code> or <code>Poetry</code>, it is easy to migrate to PDM. PDM provides <code>import</code> command so that you don't have to initialize the project manually, it now supports:</p> <ol> <li>Pipenv's <code>Pipfile</code></li> <li>Poetry's section in <code>pyproject.toml</code></li> <li>Flit's section in <code>pyproject.toml</code></li> <li><code>requirements.txt</code> format used by Pip</li> </ol> <p>Also, when you are executing <code>pdm init</code> or <code>pdm install</code>, PDM can auto-detect possible files to import if your PDM project has not been initialized yet.</p>"}, {"location": "pdm/#adding-dependencies", "title": "Adding dependencies", "text": "<p><code>pdm add</code> can be followed by one or several dependencies, and the dependency specification is described in PEP 508, you have a summary of the possibilities here.</p> <pre><code>pdm add requests\n</code></pre> <p>PDM also allows extra dependency groups by providing <code>-G/--group &lt;name&gt;</code> option, and those dependencies will go to <code>[project.optional-dependencies.&lt;name&gt;]</code> table in the project file, respectively.</p> <p>After that, dependencies and sub-dependencies will be resolved properly and installed for you, you can view <code>pdm.lock</code> to see the resolved result of all dependencies.</p>"}, {"location": "pdm/#add-local-dependencies", "title": "Add local dependencies", "text": "<p>Local packages can be added with their paths:</p> <pre><code>pdm add ./sub-package\n</code></pre> <p>Local packages can be installed in editable mode (just like <code>pip install -e &lt;local project path&gt;</code> would) using <code>pdm add -e/--editable &lt;local project path&gt;</code>.</p>"}, {"location": "pdm/#add-development-only-dependencies", "title": "Add development only dependencies", "text": "<p>PDM also supports defining groups of dependencies that are useful for development, e.g. some for testing and others for linting. We usually don't want these dependencies appear in the distribution's metadata so using optional-dependencies is probably not a good idea. We can define them as development dependencies:</p> <pre><code>pdm add -d pytest\n</code></pre> <p>This will result in a <code>pyproject.toml</code> as following:</p> <pre><code>[tool.pdm.dev-dependencies]\ntest = [ \"pytest\",]\n</code></pre>"}, {"location": "pdm/#save-version-specifiers", "title": "Save version specifiers", "text": "<p>If the package is given without a version specifier like <code>pdm add requests</code>. PDM provides three different behaviors of what version specifier is saved for the dependency, which is given by <code>--save-&lt;strategy&gt;</code>(Assume <code>2.21.0</code> is the latest version that can be found for the dependency):</p> <ul> <li><code>minimum</code>: Save the minimum version specifier: <code>&gt;=2.21.0</code> (default).</li> <li><code>compatible</code>: Save the compatible version specifier:   <code>&gt;=2.21.0,&lt;3.0.0</code>(default).</li> <li><code>exact</code>: Save the exact version specifier: <code>==2.21.0</code>.</li> <li><code>wildcard</code>: Don't constrain version and leave the specifier to be wildcard:   <code>*</code>.</li> </ul>"}, {"location": "pdm/#supporting-pre-releases", "title": "Supporting pre-releases", "text": "<p>To help package maintainers, you can allow pre-releases to be validate candidates, that way you'll get the issues sooner. It will mean more time to maintain the broken CIs if you update your packages daily (as you should!), but it's the least you can do to help your downstream library maintainers</p> <p>By default, <code>pdm</code>'s dependency resolver will ignore prereleases unless there are no stable versions for the given version range of a dependency. This behavior can be changed by setting allow_prereleases to true in <code>[tool.pdm]</code> table:</p> <pre><code>[tool.pdm]\nallow_prereleases = true\n</code></pre>"}, {"location": "pdm/#update-existing-dependencies", "title": "Update existing dependencies", "text": "<p>To update all dependencies in the lock file use:</p> <pre><code>pdm update\n</code></pre> <p>To update the specified package(s):</p> <pre><code>pdm update requests\n</code></pre> <p>To update multiple groups of dependencies:</p> <pre><code>pdm update -G security -G http\n</code></pre> <p>To update a given package in the specified group:</p> <pre><code>pdm update -G security cryptography\n</code></pre> <p>If the group is not given, PDM will search for the requirement in the default dependencies set and raises an error if none is found.</p> <p>To update packages in development dependencies:</p> <pre><code># Update all default + dev-dependencies\npdm update -d\n# Update a package in the specified group of dev-dependencies\npdm update -dG test pytest\n</code></pre> <p>Keep in mind that <code>pdm update</code> doesn't touch the constrains in <code>pyproject.toml</code>, if you want to update them you'd need to use the <code>--unconstrained</code> flag which will ignore all the constrains of downstream packages and update them to the latest version setting the pin accordingly to your update strategy.</p> <p>Updating the <code>pyproject.toml</code> constrains to match the <code>pdm.lock</code> as close as possible makes sense to avoid unexpected errors when users use other version of the libraries, as the tests are run only against the versions specified in <code>pdm.lock</code>.</p>"}, {"location": "pdm/#about-update-strategy", "title": "About update strategy", "text": "<p>Similarly, PDM also provides 2 different behaviors of updating dependencies and sub-dependencies\uff0c which is given by <code>--update-&lt;strategy&gt;</code> option:</p> <ul> <li><code>reuse</code>: Keep all locked dependencies except for those given in the command   line (default).</li> <li><code>eager</code>: Try to lock a newer version of the packages in command line and their   recursive sub-dependencies and keep other dependencies as they are.</li> </ul>"}, {"location": "pdm/#update-packages-to-the-versions-that-break-the-version-specifiers", "title": "Update packages to the versions that break the version specifiers", "text": "<p>One can give <code>-u/--unconstrained</code> to tell PDM to ignore the version specifiers in the <code>pyproject.toml</code>. This works similarly to the <code>yarn upgrade -L/--latest</code> command. Besides, <code>pdm update</code> also supports the <code>--pre/--prerelease</code> option.</p>"}, {"location": "pdm/#remove-existing-dependencies", "title": "Remove existing dependencies", "text": "<p>To remove existing dependencies from project file and the library directory:</p> <pre><code># Remove requests from the default dependencies\npdm remove requests\n# Remove h11 from the 'web' group of optional-dependencies\npdm remove -G web h11\n# Remove pytest-cov from the `test` group of dev-dependencies\npdm remove -d pytest-cov\n</code></pre>"}, {"location": "pdm/#install-the-packages-pinned-in-lock-file", "title": "Install the packages pinned in lock file", "text": "<p>There are two similar commands to do this job with a slight difference:</p> <ul> <li><code>pdm install</code> will check the lock file and relock if it mismatches with   project file, then install.</li> <li><code>pdm sync</code> installs dependencies in the lock file and will error out if it   doesn't exist. Besides, <code>pdm sync</code> can also remove unneeded packages if   <code>--clean</code> option is given.</li> </ul> <p>All development dependencies are included as long as <code>--prod</code> is not passed and <code>-G</code> doesn't specify any dev groups.</p> <p>Besides, if you don't want the root project to be installed, add <code>--no-self</code> option, and <code>--no-editable</code> can be used when you want all packages to be installed in non-editable versions. With <code>--no-editable</code> turn on, you can safely archive the whole <code>__pypackages__</code> and copy it to the target environment for deployment.</p>"}, {"location": "pdm/#show-what-packages-are-installed", "title": "Show what packages are installed", "text": "<p>Similar to <code>pip list</code>, you can list all packages installed in the packages directory:</p> <pre><code>pdm list\n</code></pre> <p>Or show a dependency graph by:</p> <pre><code>$ pdm list --graph\ntempenv 0.0.0\n\u2514\u2500\u2500 click 7.0 [ required: &lt;7.0.0,&gt;=6.7 ]\nblack 19.10b0\n\u251c\u2500\u2500 appdirs 1.4.3 [ required: Any ]\n\u251c\u2500\u2500 attrs 19.3.0 [ required: &gt;=18.1.0 ]\n\u251c\u2500\u2500 click 7.0 [ required: &gt;=6.5 ]\n\u251c\u2500\u2500 pathspec 0.7.0 [ required: &lt;1,&gt;=0.6 ]\n\u251c\u2500\u2500 regex 2020.2.20 [ required: Any ]\n\u251c\u2500\u2500 toml 0.10.0 [ required: &gt;=0.9.4 ]\n\u2514\u2500\u2500 typed-ast 1.4.1 [ required: &gt;=1.4.0 ]\nbump2version 1.0.0\n</code></pre>"}, {"location": "pdm/#solve-the-locking-failure", "title": "Solve the locking failure", "text": "<p>If PDM is not able to find a resolution to satisfy the requirements, it will raise an error. For example,</p> <pre><code>pdm django==3.1.4 \"asgiref&lt;3\"\n...\n\ud83d\udd12 Lock failed\nUnable to find a resolution for asgiref because of the following conflicts:\nasgiref&lt;3 (from project)\nasgiref&lt;4,&gt;=3.2.10 (from &lt;Candidate django 3.1.4 from https://pypi.org/simple/django/&gt;)\nTo fix this, you could loosen the dependency version constraints in pyproject.toml. If that is not possible, you could also override the resolved version in [tool.pdm.overrides] table.\n</code></pre> <p>You can either change to a lower version of <code>django</code> or remove the upper bound of <code>asgiref</code>. But if it is not eligible for your project, you can tell PDM to forcely resolve <code>asgiref</code> to a specific version by adding the following lines to <code>pyproject.toml</code>:</p> <pre><code>[tool.pdm.overrides]\nasgiref = \"&gt;=3.2.10\"\n</code></pre> <p>Each entry of that table is a package name with the wanted version. The value can also be a URL to a file or a VCS repository like <code>git+https://...</code>. On reading this, PDM will pin <code>asgiref@3.2.10</code> or the greater version in the lock file no matter whether there is any other resolution available.</p> <p>Note: By using <code>[tool.pdm.overrides]</code> setting, you are at your own risk of any incompatibilities from that resolution. It can only be used if there is no valid resolution for your requirements and you know the specific version works. Most of the time, you can just add any transient constraints to the <code>dependencies</code> array.</p>"}, {"location": "pdm/#solve-circular-dependencies", "title": "Solve circular dependencies", "text": "<p>Sometimes <code>pdm</code> is not able to locate the best package combination, or it does too many loops, so to help it you can update your version constrains so that it has the minimum number of candidates.</p> <p>To solve circular dependencies we first need to locate what are the conflicting packages, <code>pdm</code> doesn't make it easy to detect them. To do that first try to update each of your groups independently with <code>pdm update -G group_name</code>. If that doesn't work remove from your <code>pyproject.toml</code> groups of dependencies until the command works and add back one group by group until you detect the ones that fail.</p> <p>Also it's useful to reduce the number of possibilities of versions of each dependency to make things easier to <code>pdm</code>. Locate all the outdated packages by doing <code>pdm show</code> on each package until this issue is solved and run <code>pdm update {package} --unconstrained</code> for each of them. If you're already on the latest version, update your <code>pyproject.toml</code> to match the latest state.</p> <p>Once you have everything to the latest compatible version, you can try to upgrade the rest of the packages one by one to the latest with <code>--unconstrained</code>.</p> <p>In the process of doing these steps you'll see some conflicts in the dependencies that can be manually solved by preventing those versions to be installed or maybe changing the <code>python-requires</code>, although this should be done as the last resource.</p> <p>It also helps to run <code>pdm update</code> with the <code>-v</code> flag, that way you see which are the candidates that are rejected, and you can put the constrain you want. For example, I was seeing the next traceback:</p> <pre><code>pdm.termui: Conflicts detected:\n  pyflakes&gt;=3.0.0 (from &lt;Candidate autoflake 2.0.0 from https://pypi.org/simple/autoflake/&gt;)\n  pyflakes&lt;2.5.0,&gt;=2.4.0 (from &lt;Candidate flake8 4.0.1 from unknown&gt;)\n</code></pre> <p>So I added a new dependency to pin it:</p> <pre><code>[tool.pdm.dev-dependencies]\n# The next ones are required to manually solve the dependencies issues\ndependencies = [\n    # Until flakeheaven supports flake8 5.x\n    # https://github.com/flakeheaven/flakeheaven/issues/132\n    \"flake8&gt;=4.0.1,&lt;5.0.0\",\n    \"pyflakes&lt;2.5.0\",\n]\n</code></pre> <p>If none of the above works, you can override them:</p> <pre><code>[tool.pdm.overrides]\n# To be removed once https://github.com/flakeheaven/flakeheaven/issues/132 is solved\n\"importlib-metadata\" = \"&gt;=3.10\"\n</code></pre> <p>If you get lost in understanding your dependencies, you can try using <code>pydeps</code> to get your head around it.</p>"}, {"location": "pdm/#building-packages", "title": "Building packages", "text": "<p>PDM can act as a PEP 517 build backend, to enable that, write the following lines in your <code>pyproject.toml</code>.</p> <pre><code>[build-system]\nrequires = [ \"pdm-pep517\",]\nbuild-backend = \"pdm.pep517.api\"\n</code></pre> <p><code>pip</code> will read the backend settings to install or build a package.</p>"}, {"location": "pdm/#choose-a-python-interpreter", "title": "Choose a Python interpreter", "text": "<p>If you have used <code>pdm init</code>, you must have already seen how PDM detects and selects the Python interpreter. After initialized, you can also change the settings by <code>pdm use &lt;python_version_or_path&gt;</code>. The argument can be either a version specifier of any length, or a relative or absolute path to the python interpreter, but remember the Python interpreter must conform with the <code>python_requires</code> constraint in the project file.</p>"}, {"location": "pdm/#how-requires-python-controls-the-project", "title": "How <code>requires-python</code> controls the project", "text": "<p>PDM respects the value of <code>requires-python</code> in the way that it tries to pick package candidates that can work on all python versions that <code>requires-python</code> contains. For example, if <code>requires-python</code> is <code>&gt;=2.7</code>, PDM will try to find the latest version of <code>foo</code>, whose <code>requires-python</code> version range is a superset of <code>&gt;=2.7</code>.</p> <p>So, make sure you write <code>requires-python</code> properly if you don't want any outdated packages to be locked.</p>"}, {"location": "pdm/#build-distribution-artifacts", "title": "Build distribution artifacts", "text": "<pre><code>$ pdm build\n- Building sdist...\n- Built pdm-test-0.0.0.tar.gz\n- Building wheel...\n- Built pdm_test-0.0.0-py3-none-any.whl\n</code></pre>"}, {"location": "pdm/#publishing-artifacts", "title": "Publishing artifacts", "text": "<p>The artifacts can then be uploaded to PyPI by twine or through the <code>pdm-publish</code> plugin. The main developer didn't thought it was worth it, so branchvincent made the plugin (I love this possibility).</p> <p>Install it with <code>pdm plugin add pdm-publish</code>.</p> <p>Then you can upload them with;</p> <pre><code># Using token auth\npdm publish --password token\n# To test PyPI using basic auth\npdm publish -r testpypi -u username -P password\n# To custom index\npdm publish -r https://custom.index.com/\n</code></pre> <p>If you don't want to use your credentials in plaintext on the command, you can use the environmental variables <code>PDM_PUBLISH_PASSWORD</code> and <code>PDM_PUBLISH_USER</code>.</p>"}, {"location": "pdm/#show-the-current-python-environment", "title": "Show the current Python environment", "text": "<pre><code>$ pdm info\nPDM version:        1.11.3\nPython Interpreter: /usr/local/bin/python3.9 (3.9)\nProject Root:       /tmp/tmp.dBlK2rAn2x\nProject Packages:   /tmp/tmp.dBlK2rAn2x/__pypackages__/3.9\n</code></pre> <pre><code>$ pdm info --env\n{\n  \"implementation_name\": \"cpython\",\n  \"implementation_version\": \"3.9.8\",\n  \"os_name\": \"posix\",\n  \"platform_machine\": \"x86_64\",\n  \"platform_release\": \"4.19.0-5-amd64\",\n  \"platform_system\": \"Linux\",\n  \"platform_version\": \"#1 SMP Debian 4.19.37-5+deb10u1 (2019-07-19)\",\n  \"python_full_version\": \"3.9.8\",\n  \"platform_python_implementation\": \"CPython\",\n  \"python_version\": \"3.9\",\n  \"sys_platform\": \"linux\"\n}\n</code></pre>"}, {"location": "pdm/#manage-project-configuration", "title": "Manage project configuration", "text": "<p>Show the current configurations:</p> <pre><code>pdm config\n</code></pre> <p>Get one single configuration:</p> <pre><code>pdm config pypi.url\n</code></pre> <p>Change a configuration value and store in home configuration:</p> <pre><code>pdm config pypi.url \"https://test.pypi.org/simple\"\n</code></pre> <p>By default, the configuration are changed globally, if you want to make the config seen by this project only, add a <code>--local</code> flag:</p> <pre><code>pdm config --local pypi.url \"https://test.pypi.org/simple\"\n</code></pre> <p>Any local configurations will be stored in <code>.pdm.toml</code> under the project root directory.</p> <p>The configuration files are searched in the following order:</p> <ol> <li><code>&lt;PROJECT_ROOT&gt;/.pdm.toml</code> - The project configuration.</li> <li><code>~/.pdm/config.toml</code> - The home configuration.</li> </ol> <p>If <code>-g/--global</code> option is used, the first item will be replaced by <code>~/.pdm/global-project/.pdm.toml</code>.</p> <p>You can find all available configuration items in Configuration Page.</p>"}, {"location": "pdm/#run-scripts-in-isolated-environment", "title": "Run Scripts in Isolated Environment", "text": "<p>With PDM, you can run arbitrary scripts or commands with local packages loaded:</p> <pre><code>pdm run flask run -p 54321\n</code></pre> <p>PDM also supports custom script shortcuts in the optional <code>[tool.pdm.scripts]</code> section of <code>pyproject.toml</code>.</p> <p>You can then run <code>pdm run &lt;shortcut_name&gt;</code> to invoke the script in the context of your PDM project. For example:</p> <pre><code>[tool.pdm.scripts]\nstart_server = \"flask run -p 54321\"\n</code></pre> <p>And then in your terminal:</p> <pre><code>$ pdm run start_server\nFlask server started at http://127.0.0.1:54321\n</code></pre> <p>Any extra arguments will be appended to the command:</p> <pre><code>$ pdm run start_server -h 0.0.0.0\nFlask server started at http://0.0.0.0:54321\n</code></pre> <p>PDM supports 3 types of scripts:</p>"}, {"location": "pdm/#normal-command", "title": "Normal command", "text": "<p>Plain text scripts are regarded as normal command, or you can explicitly specify it:</p> <pre><code>[tool.pdm.scripts.start_server]\ncmd = \"flask run -p 54321\"\n</code></pre> <p>In some cases, such as when wanting to add comments between parameters, it might be more convenient to specify the command as an array instead of a string:</p> <pre><code>[tool.pdm.scripts.start_server]\ncmd = [ \"flask\", \"run\", \"-p\", \"54321\",]\n</code></pre>"}, {"location": "pdm/#shell-script", "title": "Shell script", "text": "<p>Shell scripts can be used to run more shell-specific tasks, such as pipeline and output redirecting. This is basically run via <code>subprocess.Popen()</code> with <code>shell=True</code>:</p> <pre><code>[tool.pdm.scripts.filter_error]\nshell = \"cat error.log|grep CRITICAL &gt; critical.log\"\n</code></pre>"}, {"location": "pdm/#call-a-python-function", "title": "Call a Python function", "text": "<p>The script can be also defined as calling a python function in the form <code>&lt;module_name&gt;:&lt;func_name&gt;</code>:</p> <pre><code>[tool.pdm.scripts.foobar]\ncall = \"foo_package.bar_module:main\"\n</code></pre> <p>The function can be supplied with literal arguments:</p> <pre><code>[tool.pdm.scripts.foobar]\ncall = \"foo_package.bar_module:main('dev')\"\n</code></pre>"}, {"location": "pdm/#environment-variables-support", "title": "Environment variables support", "text": "<p>All environment variables set in the current shell can be seen by <code>pdm run</code> and will be expanded when executed. Besides, you can also define some fixed environment variables in your <code>pyproject.toml</code>:</p> <pre><code>[tool.pdm.scripts.start_server]\ncmd = \"flask run -p 54321\"\n\n[tool.pdm.scripts.start_server.env]\nFOO = \"bar\"\nFLASK_ENV = \"development\"\n</code></pre> <p>Note how we use TOML's syntax to define a compound dictionary.</p> <p>A dotenv file is also supported via <code>env_file = \"&lt;file_path&gt;\"</code> setting.</p> <p>For environment variables and/or dotenv file shared by all scripts, you can define <code>env</code> and <code>env_file</code> settings under a special key named <code>_</code> of <code>tool.pdm.scripts</code> table:</p> <pre><code>[tool.pdm.scripts]\nstart_server = \"flask run -p 54321\"\nmigrate_db = \"flask db upgrade\"\n\n[tool.pdm.scripts._]\nenv_file = \".env\"\n</code></pre> <p>Besides, PDM also injects the root path of the project via <code>PDM_PROJECT_ROOT</code> environment variable.</p>"}, {"location": "pdm/#load-site-packages-in-the-running-environment", "title": "Load site-packages in the running environment", "text": "<p>To make sure the running environment is properly isolated from the outer Python interpreter, site-packages from the selected interpreter WON'T be loaded into <code>sys.path</code>, unless any of the following conditions holds:</p> <ol> <li>The executable is from <code>PATH</code> but not inside the <code>__pypackages__</code> folder.</li> <li><code>-s/--site-packages</code> flag is following <code>pdm run</code>.</li> <li><code>site_packages = true</code> is in either the script table or the global setting    key <code>_</code>.</li> </ol> <p>Note that site-packages will always be loaded if running with PEP 582 enabled(without the <code>pdm run</code> prefix).</p>"}, {"location": "pdm/#show-the-list-of-scripts-shortcuts", "title": "Show the list of scripts shortcuts", "text": "<p>Use <code>pdm run --list/-l</code> to show the list of available script shortcuts:</p> <pre><code>$ pdm run --list\nName        Type  Script           Description\n----------- ----- ---------------- ----------------------\ntest_cmd    cmd   flask db upgrade\ntest_script call  test_script:main call a python function\ntest_shell  shell echo $FOO        shell command\n</code></pre> <p>You can add an <code>help</code> option with the description of the script, and it will be displayed in the <code>Description</code> column in the above output.</p>"}, {"location": "pdm/#manage-caches", "title": "Manage caches", "text": "<p>PDM provides a convenient command group to manage the cache, there are four kinds of caches:</p> <ul> <li><code>wheels/</code> stores the built results of non-wheel distributions and files.</li> <li><code>http/</code> stores the HTTP response content.</li> <li><code>metadata/</code> stores package metadata retrieved by the resolver.</li> <li><code>hashes/</code> stores the file hashes fetched from the package index or calculated   locally.</li> <li><code>packages/</code> The centrialized repository for installed wheels.</li> </ul> <p>See the current cache usage by typing <code>pdm cache info</code>. Besides, you can use <code>add</code>, <code>remove</code> and <code>list</code> subcommands to manage the cache content.</p>"}, {"location": "pdm/#manage-global-dependencies", "title": "Manage global dependencies", "text": "<p>Sometimes users may want to keep track of the dependencies of global Python interpreter as well. It is easy to do so with PDM, via <code>-g/--global</code> option which is supported by most subcommands.</p> <p>If the option is passed, <code>~/.pdm/global-project</code> will be used as the project directory, which is almost the same as normal project except that <code>pyproject.toml</code> will be created automatically for you and it doesn't support build features. The idea is taken from Haskell's stack.</p> <p>However, unlike <code>stack</code>, by default, PDM won't use global project automatically if a local project is not found. Users should pass <code>-g/--global</code> explicitly to activate it, since it is not very pleasing if packages go to a wrong place. But PDM also leave the decision to users, just set the config <code>auto_global</code> to <code>true</code>.</p> <p>If you want global project to track another project file other than <code>~/.pdm/global-project</code>, you can provide the project path via <code>-p/--project &lt;path&gt;</code> option.</p> <p>Warning: Be careful with <code>remove</code> and <code>sync --clean</code> commands when global project is used, because it may remove packages installed in your system Python.</p>"}, {"location": "pdm/#configuration", "title": "Configuration", "text": "<p>All available configurations can be seen here.</p>"}, {"location": "pdm/#dependency-specification", "title": "Dependency specification", "text": "<p>The <code>project.dependencies</code> is an array of dependency specification strings following the PEP 440 and PEP 508.</p> <p>Examples:</p> <pre><code>dependencies = [ \"requests\", \"flask &gt;= 1.1.0\", \"pywin32; sys_platform == 'win32'\", \"pip @ https://github.com/pypa/pip.git@20.3.1\",]\n</code></pre>"}, {"location": "pdm/#editable-requirement", "title": "Editable requirement", "text": "<p>Beside of the normal dependency specifications, one can also have some packages installed in editable mode. The editable specification string format is the same as Pip's editable install mode.</p> <p>Examples:</p> <pre><code>dependencies = [\n    ...,\n    # Local dependency\n    \"-e path/to/SomeProject\",\n    # Dependency cloned\n    \"-e git+http://repo/my_project.git#egg=SomeProject\"\n]\n</code></pre> <p>Note: About editable installation. One can have editable installation and normal installation for the same package. The one that comes at last wins. However, editable dependencies WON'T be included in the metadata of the built artifacts since they are not valid PEP 508 strings. They only exist for development purpose.</p>"}, {"location": "pdm/#optional-dependencies", "title": "Optional dependencies", "text": "<p>You can have some requirements optional, which is similar to <code>setuptools</code>' <code>extras_require</code> parameter.</p> <pre><code>[project.optional-dependencies]\nsocks = [ \"PySocks &gt;= 1.5.6, != 1.5.7, &lt; 2\",]\ntests = [ \"ddt &gt;= 1.2.2, &lt; 2\", \"pytest &lt; 6\", \"mock &gt;= 1.0.1, &lt; 4; python_version &lt; \\\"3.4\\\"\",]\n</code></pre> <p>To install a group of optional dependencies:</p> <pre><code>pdm install -G socks\n</code></pre> <p><code>-G</code> option can be given multiple times to include more than one group.</p>"}, {"location": "pdm/#development-dependencies-groups", "title": "Development dependencies groups", "text": "<p>You can have several groups of development only dependencies. Unlike <code>optional-dependencies</code>, they won't appear in the package distribution metadata such as <code>PKG-INFO</code> or <code>METADATA</code>. And the package index won't be aware of these dependencies. The schema is similar to that of <code>optional-dependencies</code>, except that it is in <code>tool.pdm</code> table.</p> <pre><code>[tool.pdm.dev-dependencies]\nlint = [ \"flake8\", \"black\",]\ntest = [ \"pytest\", \"pytest-cov\",]\ndoc = [ \"mkdocs\",]\n</code></pre> <p>To install all of them:</p> <pre><code>pdm install\n</code></pre> <p>For more CLI usage, please refer to Manage Dependencies</p>"}, {"location": "pdm/#show-outdated-packages", "title": "Show outdated packages", "text": "<pre><code>pdm update --dry-run --unconstrained\n</code></pre>"}, {"location": "pdm/#console-scripts", "title": "Console scripts", "text": "<p>The following content:</p> <pre><code>[project.scripts]\nmycli = \"mycli.__main__:main\"\n</code></pre> <p>will be translated to <code>setuptools</code> style:</p> <pre><code>entry_points = {\"console_scripts\": [\"mycli=mycli.__main__:main\"]}\n</code></pre> <p>Also, <code>[project.gui-scripts]</code> will be translated to <code>gui_scripts</code> entry points group in <code>setuptools</code> style.</p>"}, {"location": "pdm/#entry-points", "title": "Entry points", "text": "<p>Other types of entry points are given by <code>[project.entry-points.&lt;type&gt;]</code> section, with the same format of <code>[project.scripts]</code>:</p> <pre><code>[project.entry-points.pytest11]\nmyplugin = \"mypackage.plugin:pytest_plugin\"\n</code></pre>"}, {"location": "pdm/#include-and-exclude-package-files", "title": "Include and exclude package files", "text": "<p>The way of specifying include and exclude files are simple, they are given as a list of glob patterns:</p> <pre><code>includes = [ \"**/*.json\", \"mypackage/\",]\nexcludes = [ \"mypackage/_temp/*\",]\n</code></pre> <p>In case you want some files to be included in sdist only, you use the <code>source-includes</code> field:</p> <pre><code>includes = [...]\nexcludes = [...]\nsource-includes = [\"tests/\"]\n</code></pre> <p>Note that the files defined in <code>source-includes</code> will be excluded automatically from non-sdist builds.</p>"}, {"location": "pdm/#default-values-for-includes-and-excludes", "title": "Default values for includes and excludes", "text": "<p>If you don't specify any of these fields, PDM also provides smart default values to fit the most common workflows.</p> <ul> <li>Top-level packages will be included.</li> <li><code>tests</code> package will be excluded from non-sdist builds.</li> <li><code>src</code> directory will be detected as the <code>package-dir</code> if it exists.</li> </ul> <p>If your project follows the above conventions you don't need to config any of these fields and it just works. Be aware PDM won't add PEP 420 implicit namespace packages automatically and they should always be specified in <code>includes</code> explicitly.</p>"}, {"location": "pdm/#determine-the-package-version-dynamically", "title": "Determine the package version dynamically", "text": "<p>The package version can be retrieved from the <code>__version__</code> variable of a given file. To do this, put the following under the <code>[tool.pdm]</code> table:</p> <pre><code>[tool.pdm.version]\nfrom = \"mypackage/__init__.py\"\n</code></pre> <p>Remember set <code>dynamic = [\"version\"]</code> in <code>[project]</code> metadata.</p> <p>PDM can also read version from SCM tags. If you are using <code>git</code> or <code>hg</code> as the version control system, define the <code>version</code> as follows:</p> <pre><code>[tool.pdm.version]\nuse_scm = true\n</code></pre> <p>In either case, you MUST delete the <code>version</code> field from the <code>[project]</code> table, and include <code>version</code> in the <code>dynamic</code> field, or the backend will raise an error:</p> <pre><code>dynamic = [ \"version\",]\n</code></pre>"}, {"location": "pdm/#cache-the-installation-of-wheels", "title": "Cache the installation of wheels", "text": "<p>If a package is required by many projects on the system, each project has to keep its own copy. This may become a waste of disk space especially for data science and machine learning libraries.</p> <p>PDM supports caching the installations of the same wheel by installing it into a centralized package repository and linking to that installation in different projects. To enabled it, run:</p> <pre><code>pdm config feature.install_cache on\n</code></pre> <p>It can be enabled on a project basis, by adding <code>--local</code> option to the command.</p> <p>The caches are located under <code>$(pdm config cache_dir)/packages</code>. One can view the cache usage by <code>pdm cache info</code>. But be noted the cached installations are managed automatically. They get deleted when not linked from any projects. Manually deleting the caches from the disk may break some projects on the system.</p> <p>Note: Only the installation of named requirements resolved from PyPI can be cached.</p>"}, {"location": "pdm/#working-with-a-virtualenv", "title": "Working with a virtualenv", "text": "<p>Although PDM enforces PEP 582 by default, it also allows users to install packages into the virtualenv. It is controlled by the configuration item <code>use_venv</code>. When it is set to <code>True</code> (default), PDM will use the virtualenv if:</p> <ul> <li>A virtualenv is already activated.</li> <li>Any of <code>venv</code>, <code>.venv</code>, <code>env</code> is a valid virtualenv folder.</li> </ul> <p>Besides, when <code>use-venv</code> is on and the interpreter path given is a venv-like path, PDM will reuse that venv directory as well.</p> <p>For enhanced virtualenv support such as virtualenv management and auto-creation, please go for pdm-venv, which can be installed as a plugin.</p>"}, {"location": "pdm/#use-pdm-in-continuous-integration", "title": "Use PDM in Continuous Integration", "text": "<p>Fortunately, if you are using GitHub Action, there is pdm-project/setup-pdm to make this process easier. Here is an example workflow of GitHub Actions, while you can adapt it for other CI platforms.</p> <pre><code>Testing:\n  runs-on: ${{ matrix.os }}\n  strategy:\n    matrix:\n      python-version: [3.7, 3.8, 3.9, 3.10]\n      os: [ubuntu-latest, macOS-latest, windows-latest]\n\n  steps:\n    - uses: actions/checkout@v1\n    - name: Set up PDM\n      uses: pdm-project/setup-pdm@main\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        pdm sync -d -G testing\n    - name: Run Tests\n      run: |\n        pdm run -v pytest tests\n</code></pre> <p>Note: Tips for GitHub Action users, there is a known compatibility issue on Ubuntu virtual environment. If PDM parallel install is failed on that machine you should either set <code>parallel_install</code> to <code>false</code> or set env <code>LD_PRELOAD=/lib/x86_64-linux-gnu/libgcc_s.so.1</code>. It is already handled by the <code>pdm-project/setup-pdm</code> action.</p> <p>Note: If your CI scripts run without a proper user set, you might get permission errors when PDM tries to create its cache directory. To work around this, you can set the HOME environment variable yourself, to a writable directory, for example:</p> <pre><code>```bash\nexport HOME=/tmp/home\n```\n</code></pre>"}, {"location": "pdm/#how-does-it-work", "title": "How does it work", "text": ""}, {"location": "pdm/#why-you-dont-need-to-use-virtualenvs", "title": "Why you don't need to use virtualenvs", "text": "<p>When you develop a Python project, you need to install the project's dependencies. For a long time, tutorials and articles have told you to use a virtual environment to isolate the project's dependencies. This way you don't contaminate the working set of other projects, or the global interpreter, to avoid possible version conflicts.</p>"}, {"location": "pdm/#problems-of-the-virtualenvs", "title": "Problems of the virtualenvs", "text": "<p>Virtualenvs are confusing for people that are starting with python. They also use a lot of space, as many virtualenvs have their own copy of the same libraries. They help us isolate project dependencies though, but things get tricky when it comes to nested venvs. One installs the virtualenv manager(like Pipenv or Poetry) using a venv encapsulated Python, and creates more venvs using the tool which is based on an encapsulated Python. One day a minor release of Python is out and one has to check all those venvs and upgrade them if required before they can safely delete the out-dated Python version.</p> <p>Another scenario is global tools. There are many tools that are not tied to any specific virtualenv and are supposed to work with each of them. Examples are profiling tools and third-party REPLs. We also wish them to be installed in their own isolated environments. It's impossible to make them work with virtualenv, even if you have activated the virtualenv of the target project you want to work on because the tool is lying in its own virtualenv and it can only see the libraries installed in it. So we have to install the tool for each project.</p> <p>The solution has been existing for a long time. PEP 582 was originated in 2018 and is still a draft proposal till the time I copied this article.</p> <p>Say you have a project with the following structure:</p> <pre><code>.\n\u251c\u2500\u2500 __pypackages__\n\u2502   \u2514\u2500\u2500 3.8\n\u2502       \u2514\u2500\u2500 lib\n\u2514\u2500\u2500 my_script.py\n</code></pre> <p>As specified in the PEP 582, if you run <code>python3.8 /path/to/my_script.py</code>, <code>__pypackages__/3.8/lib</code> will be added to <code>sys.path</code>, and the libraries inside will become import-able in <code>my_script.py</code>.</p> <p>Now let's review the two problems mentioned above under PEP 582. For the first problem, the main cause is that the virtual environment is bound to a cloned Python interpreter on which the subsequent library searching based. It takes advantage of Python's existing mechanisms without any other complex changes but makes the entire virtual environment to become unavailable when the Python interpreter is stale. With the local packages directory, you don't have a Python interpreter any more, the library path is directly appended to <code>sys.path</code>, so you can freely move and copy it.</p> <p>For the second, once again, you just call the tool against the project you want to analyze, and the <code>__pypackages__</code> sitting inside the project will be loaded automatically. This way you only need to keep one copy of the global tool and make it work with multiple projects.</p> <p><code>pdm</code> installs dependencies into the local package directory <code>__package__</code> and makes Python interpreters aware of it with a very simple setup.</p>"}, {"location": "pdm/#how-we-make-pep-582-packages-available-to-the-python-interpreter", "title": "How we make PEP 582 packages available to the Python interpreter", "text": "<p>Thanks to the site packages loading on Python startup. It is possible to patch the <code>sys.path</code> by executing the <code>sitecustomize.py</code> shipped with PDM. The interpreter can search the directories for the nearest <code>__pypackage__</code> folder and append it to the <code>sys.path</code> variable.</p>"}, {"location": "pdm/#plugins", "title": "Plugins", "text": "<p>PDM is aiming at being a community driven package manager. It is shipped with a full-featured plug-in system, with which you can:</p> <ul> <li>Develop a new command for PDM.</li> <li>Add additional options to existing PDM commands.</li> <li>Change PDM's behavior by reading dditional config items.</li> <li>Control the process of dependency resolution or installation.</li> </ul> <p>If you want to write a plugin, start here.</p>"}, {"location": "pdm/#issues", "title": "Issues", "text": "<ul> <li>You can't still   run <code>mypy</code> with <code>pdm</code> without   virtualenvs. pawamoy created a   patch   that is supposed to work, but I'd rather use virtualenvs until it's supported.   Once it's supported check the   vim-test issue to see how   to integrate it.</li> <li>It's not yet supported by   dependabot. Once   supported add it back to the cookiecutter template and spread it.</li> </ul>"}, {"location": "pdm/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "pedal_pc/", "title": "Pedal PC", "text": "<p>The Pedal PC idea gathers crazy projects that try to use the energy of your pedaling while you are working on your PC. The most interesting is PedalPC, but still crazy.</p> <p>Pedal-Power is another similar project, although it looks unmaintained.</p>"}, {"location": "pedal_pc/#references", "title": "References", "text": "<ul> <li>PedalPC Blog</li> </ul>"}, {"location": "peek/", "title": "Peek", "text": "<p>Peek is a simple animated GIF screen recorder with an easy to use interface.</p>"}, {"location": "peek/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install peek\n</code></pre> <p>If you try to use it with i3, you're going to have a bad time, you'd need to install Compton, and then the elements may not even be clickable.</p> <p>With kitty it works though :)</p>"}, {"location": "peek/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "personal_interruption_analysis/", "title": "Personal Interruption Analysis", "text": "<p>This is the interruption analysis report applied to my personal life.</p> <p>I've identified the next interruption sources:</p> <ul> <li>Physical interruptions.</li> <li>Emails.</li> <li>Calls.</li> <li>Instant message applications.</li> <li>Calendar events.</li> <li>Other desktop notifications.</li> </ul>"}, {"location": "personal_interruption_analysis/#physical-interruptions", "title": "Physical interruptions", "text": "<p>The analysis is similar to the work physical interruptions.</p>"}, {"location": "personal_interruption_analysis/#emails", "title": "Emails", "text": "<p>Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as:</p> <ul> <li>Bill or bank receipt emails: I receive at least one per provider per month,     the associated action is to download the attached pdf and remove the email.     I've got it automated using a Python program that I need to manually run. In     the future I expect it to be done automatically without my     interaction. There is no urgency to     act on them.</li> <li>General information: In the past I was subscribed to newsletters, now I prefer     to use RSS. They don't usually require any direct action, so they can     wait more than two days.</li> <li>Videogame deals: I was subscribed to Humblebundle, GOG and Steam notifications     to be notified on the deals, but then I migrated to     IsThereAnyDeal because it only sends the     notifications of the deals that match a defined criteria (reducing the     amount of emails), and monitors all sites in one place. I can act on them     with one or two days of delay.</li> <li>Source code manager notifications: The web where I host my source code sends     me emails when there are new pull requests or when there are comments on     existent ones. I try to act on them daily.</li> <li>The CI sends notifications when some job fails. Unless it's a new     pipeline or I'm actively working on it, a failed work job can wait many days     broken before I need to interact with ithe.</li> <li>Infrastructure notifications: For example LetsEncrypt renewals or cloud provider     notification or support cases. The related actions can wait a day or more.</li> <li>Monitorization notifications: I've configured Prometheus's     alertmanager to send the notifications to the email as     a fallback channel, but it's to be checked only if the main channel is down.</li> <li>Stranger emails: People whom I don't know that contacts me asking questions.     These can be dealt with daily.</li> </ul> <p>In conclusion, I can check the personal emails twice a day, one after breakfast and another in the middle of the afternoon. So its safe to disable the notifications.</p> <p>I'm eager to start the email automation project so I can spend even less time and willpower managing the email.</p>"}, {"location": "personal_interruption_analysis/#calls", "title": "Calls", "text": "<p>People are not used to call anymore, most of them prefer to chat. Even though it is much less efficient. I prefer to have less frequent calls where you have full focused interaction rather than many chat sessions.</p> <p>I categorize the calls in two groups:</p> <ul> <li>Social interactions: managed similar as the physical     social interactions,     with the people that I speak regularly, we arrange meetings that suit us     both, the others I tell which are good time spans to call me. If the     conversation allows it, I try to use headphones and simultaneously do     mindless tasks such as folding the clothes or cleaning the kitchen. To     prioritize and adjust the time between calls for each people I use     relationship management processes.</li> <li>Spam callers: Hateful events where you can't dump all the frustration that     they produce on the person that calls you as it's not their fault and they     surely are not enjoying either the situation. They have the lowest priority     and can be safely ignored and blocked. You can manually do it in the phone,     although it's not very effective as they change numbers. A better approach     is to add your number to do not call registries which legally allow you to     scare them off.</li> </ul> <p>As calls are very rare and of high priority, I have my phone configured to ring on incoming calls.</p>"}, {"location": "personal_interruption_analysis/#instant-messages", "title": "Instant messages", "text": "<p>It's the main communication channel for most people, so it has a great volume of events but most have low priority. They can be categorized as:</p> <ul> <li>Asking for help through direct messages: We don't have many as we've agreed to     use groups as much as     possible.     So they have high priority and I have the notifications enabled.</li> <li>Social interaction through direct messages: I don't have many as I try to     arrange one on one calls     instead,     so they have a low priority.</li> <li>Team group or support rooms: We've defined the interruption role so I check them     whenever an chosen interruption event comes.</li> <li>Information rooms: They have no priority and can be checked daily.</li> </ul> <p>In conclusion, I can check the personal chat applications three times per day, for example, after each meal. As I usually need them when I'm off the computer, I only have them configured at my mobile phone, with no sound notifications. That way I only check them when I want to.</p>"}, {"location": "personal_interruption_analysis/#desktop-notifications", "title": "Desktop notifications", "text": "<p>I have none but I've seen people have a notification each time the music player changes of song. It makes no sense at all.</p>"}, {"location": "pexpect/", "title": "pexpect", "text": "<p>pexpect is a pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands.</p>"}, {"location": "pexpect/#installation", "title": "Installation", "text": "<pre><code>pip install pexpect\n</code></pre>"}, {"location": "pexpect/#usage", "title": "Usage", "text": "<pre><code>import pexpect\n\nchild = pexpect.spawn('ftp ftp.openbsd.org')\nchild.expect('Name .*: ')\nchild.sendline('anonymous')\n</code></pre> <p>If you're using it to spawn a program that asks something and then ends, you can catch the end with <code>.expect_exact(pexpect.EOF)</code>.</p> <pre><code>tui = pexpect.spawn(\"python source.py\", timeout=5)\ntui.expect(\"Give me .*\")\ntui.sendline(\"HI\")\ntui.expect_exact(pexpect.EOF)\n</code></pre> <p>The <code>timeout=5</code> is useful if the <code>pexpect</code> interaction is not well defined, so that the script is not hung forever.</p>"}, {"location": "pexpect/#send-key-presses", "title": "Send key presses", "text": "<p>To simulate key presses, you can use prompt_toolkit keys with REVERSE_ANSI_SEQUENCES.</p> <pre><code>from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES\nfrom prompt_toolkit.keys import Keys\n\ntui = pexpect.spawn(\"python source.py\", timeout=5)\ntui.send(REVERSE_ANSI_SEQUENCES[Keys.ControlC])\n</code></pre> <p>To make your code cleaner you can use a helper class:</p> <pre><code>from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES\nfrom prompt_toolkit.keys import Keys\n\nclass Keyboard(str, Enum):\n    ControlH = REVERSE_ANSI_SEQUENCES[Keys.ControlH]\n    Enter = \"\\r\"\n    Esc = REVERSE_ANSI_SEQUENCES[Keys.Escape]\n\n    # Equivalent keystrokes in terminals; see python-prompt-toolkit for\n    # further explanations\n    Alt = Esc\n    Backspace = ControlH\n</code></pre>"}, {"location": "pexpect/#read-output-of-command", "title": "Read output of command", "text": "<pre><code>import sys\nimport pexpect\nchild = pexpect.spawn('ls')\nchild.logfile = sys.stdout\nchild.expect(pexpect.EOF)\n</code></pre> <p>For the tests, you can use the capsys fixture to do assertions on the content:</p> <pre><code>out, err = capsys.readouterr()\nassert \"WARNING! you took 1 seconds to process the last element\" in out\n</code></pre>"}, {"location": "pexpect/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "pilates/", "title": "Pilates", "text": "<p>Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability.</p> <p>Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises.</p> <p>You can think of yoga, but without the spiritual aspects.</p>"}, {"location": "pilates/#principles", "title": "Principles", "text": ""}, {"location": "pilates/#breathing", "title": "Breathing", "text": "<p>The breathing in Pilates is meant the to be deeper, with full inhalations and complete exhalations. In order to keep the lower abdominals close to the spine, the breathing needs to be directed to the back and sides of the lower rib cage. When exhaling, you need to squeeze out the lungs as they would wring a wet towel dry. To do that you need to contract the deep abdominal and pelvic floor muscles, feeling your bellybutton going to your back and a little bit up. When inhaling you need to maintain this engagement to keep the core in control.</p> <p>The difficult part comes when you try to properly coordinate this breathing practice with the exercise movement, breathes out with the effort and in on the return.</p> <p>This technique is important as it increases the intake of oxygen and the circulation of this oxygenated blood to every part of the body, cleaning and invigorating it.</p>"}, {"location": "pilates/#concentration", "title": "Concentration", "text": "<p>It demands intense focus, as you need to be aware of the position of each part of your body, and how they move to precisely do the exercise.</p>"}, {"location": "pilates/#control", "title": "Control", "text": "<p>You don't see many quick movements, most of the exercises are anaerobic. The difficult relies on controlling your muscles to do what you want them to while they fight against gravity, springs and other torture tools.</p>"}, {"location": "pilates/#flow", "title": "Flow", "text": "<p>Pilates aims for elegant economy of movement, creating flow through the use of appropriate transitions. Once precision has been achieved, the exercises are intended to flow within and into each other in order to build strength and stamina.</p> <p>A smoothly doing a roll down (from seated position with your legs straight, slowly lay down, vertebrae by vertebrae) is a difficult challenge, as you need every muscle to coordinate to share the load of the weight. The muscles that we use more are stronger, and some of them are barely used, Pilates positions and slow transitions force you to use those weak, forgotten muscles, and when the load is transferred from the strong to the weak, your body starts shaking or breaks the movement rate thus breaking the flow.</p> <p>Even though it looks silly, it's tough.</p>"}, {"location": "pilates/#postural-alignment", "title": "Postural alignment", "text": "<p>Being more aware of your body by bringing to it's limits with each exercise, seeing where it fails, strengthening the weak muscles and practicing flow and control results in a better postural alignment.</p>"}, {"location": "pilates/#precision", "title": "Precision", "text": "<p>The focus is on doing one precise and perfect movement, rather than many halfhearted ones. The goal is for this precision to eventually become second nature and carry over into everyday life as grace and economy of movement.</p>"}, {"location": "pilates/#relaxation", "title": "Relaxation", "text": "<p>Correct muscle firing patterns and improved mental concentration are enhanced with relaxation.</p>"}, {"location": "pilates/#stamina", "title": "Stamina", "text": "<p>Stamina is increased through the gradual strengthening of your body, and with the increasing precision of the motion, making them more efficient so there is less stress to perform the exercises.</p>"}, {"location": "pilates/#positions", "title": "Positions", "text": "<ul> <li>Feet in flex: your toes go away from your shins, so your foot follows your     shin line.</li> </ul>"}, {"location": "pilates/#exercises", "title": "Exercises", "text": "<p>I'm going to annotate the exercises I like most, probably the name is incorrect and the explanation not perfect. If you do them, please be careful, and in case of doubt ask a Pilates teacher.</p>"}, {"location": "pilates/#swing-from-table", "title": "Swing from table", "text": "<p>Lvl 0:</p> <ul> <li>Starting position (Inhale): Start at step 1 of the table.</li> <li>Step 1 (Exhale): Instead of going to the mat, when exhaling move your ass     between your arms without touching the mat until it's behind them. Round     your spine in the process. You'll feel a nice spine movement similar to the     cat - cow movement.</li> <li>Return (Inhale): Slowly go back to starting position.</li> </ul> <p>Lvl 1:</p> <ul> <li>Starting position (Inhale): Start at step 1 of the  table with     one leg straight in the air, in the same line as your shoulders, hips and     knee.</li> <li>Step 1 (Exhale): Similar to Lvl 0 but make sure that the heel of the foot     doesn't touch the mat, feet in flex.</li> <li>Return (Inhale): Slowly go back to starting position, do X repetitions and     then switch to the other foot.</li> </ul> <p>Lvl 2:</p> <ul> <li>Starting position (Inhale): Start at step 1 of the inverted plank.</li> <li>Step 1 (Exhale): Similar to Lvl 0.</li> <li>Return (Inhale): Slowly go back to starting position.</li> </ul> <p>Lvl 3:</p> <p>Similar to Lvl 2 with the leg up like Lvl 1.</p> <p>I've found that Lvl 2 and Lvl 3 give a less pleasant spine rub.</p>"}, {"location": "pilates/#table", "title": "Table", "text": "<ul> <li> <p>Starting position (Exhale): Sit in your mat with your legs parallel, knees bent and     your feet at two or three fists from your ass, hands on the mat behind you,     fingers pointing to your ass.</p> <p>If you have shoulder aches, you can point the fingers at 45 degrees or away from your ass. * Step 1 (Inhale): Slowly move your ass up until your shoulders, knees and hips are on the same line. To avoid neck pain, keep your chin down so you're looking at your knees. Your knees should be over your ankles and your arms should be extended. * Return (Exhale): Slowly come back to the starting position.</p> </li> </ul>"}, {"location": "pilates/#references", "title": "References", "text": ""}, {"location": "pilates/#books", "title": "Books", "text": "<ul> <li>Pilates anatomy by Rael Isacowitz and Karen     Clippinger:     With gorgeous illustrations.</li> </ul>"}, {"location": "pipenv/", "title": "Pipenv", "text": "<p>Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world.</p> <p>It automatically creates and manages a virtualenv for your projects, as well as adds/removes packages from your Pipfile as you install/uninstall packages. It also generates the ever-important Pipfile.lock, which is used to produce deterministic builds.</p>"}, {"location": "pipenv/#features", "title": "Features", "text": "<ul> <li>Enables truly deterministic builds, while easily specifying only     what you want.</li> <li>Generates and checks file hashes for locked dependencies.</li> <li>Automatically install required Pythons, if <code>pyenv</code> is available.</li> <li>Automatically finds your project home, recursively, by looking for a     <code>Pipfile</code>.</li> <li>Automatically generates a <code>Pipfile</code>, if one doesn't exist.</li> <li>Automatically creates a virtualenv in a standard location.</li> <li>Automatically adds/removes packages to a <code>Pipfile</code> when they are     un/installed.</li> <li>Automatically loads <code>.env</code> files, if they exist.</li> </ul> <p>The main commands are <code>install</code>, <code>uninstall</code>, and <code>lock</code>, which generates a <code>Pipfile.lock</code>. These are intended to replace <code>$ pip install</code> usage, as well as manual virtualenv management (to activate a virtualenv, run <code>$ pipenv shell</code>).</p>"}, {"location": "pipenv/#basic-concepts", "title": "Basic Concepts", "text": "<ul> <li>A virtualenv will automatically be created, when one doesn't exist.</li> <li>When no parameters are passed to <code>install</code>, all packages     <code>[packages]</code> specified will be installed.</li> <li>Otherwise, whatever virtualenv defaults to will be the default.</li> </ul>"}, {"location": "pipenv/#other-commands", "title": "Other Commands", "text": "<ul> <li><code>shell</code> will spawn a shell with the virtualenv activated.</li> <li><code>run</code> will run a given command from the virtualenv, with any     arguments forwarded (e.g. <code>$ pipenv run python</code>).</li> <li><code>check</code> asserts that PEP 508 requirements are being met by the     current environment.</li> <li><code>graph</code> will print a pretty graph of all your installed     dependencies.</li> </ul>"}, {"location": "pipenv/#installation", "title": "Installation", "text": "<p>In Debian: <code>apt-get install pipenv</code></p> <p>Or <code>pip install pipenv</code>.</p>"}, {"location": "pipenv/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "pipx/", "title": "Pipx", "text": "<p>Pipx is a command line tool to install and run Python applications in isolated environments.</p> <p>Very useful not to pollute your user or device python environments.</p>"}, {"location": "pipx/#installation", "title": "Installation", "text": "<pre><code>pip install pipx\n</code></pre>"}, {"location": "pipx/#usage", "title": "Usage", "text": "<p>Now that you have pipx installed, you can install a program:</p> <pre><code>pipx install PACKAGE\n</code></pre> <p>for example</p> <pre><code>pipx install pycowsay\n</code></pre> <p>You can list programs installed:</p> <pre><code>pipx list\n</code></pre> <p>Or you can run a program without installing it:</p> <pre><code>pipx run pycowsay moooo!\n</code></pre> <p>You can view documentation for all commands by running pipx --help.</p>"}, {"location": "pipx/#upgrade", "title": "Upgrade", "text": "<p>You can use <code>pipx upgrade-all</code> to upgrade all your installed packages. If you want to just upgrade one, use <code>pipx upgrade PACKAGE</code>.</p> <p>If the package doesn't change the requirements of their dependencies so that the installed don't meet them, they won't be upgraded unless you use the <code>--pip-args '--upgrade-strategy eager'</code> flag.</p> <p>It uses the pip flag <code>upgrade-strategy</code> which can be one of:</p> <ul> <li><code>eager</code>: dependencies are upgraded regardless of whether the currently     installed version satisfies the requirements of the upgraded package(s).</li> <li><code>only-if-needed</code>: dependencies are upgraded only when they do not satisfy the     requirements of the upgraded package(s). This is the default value.</li> </ul>"}, {"location": "pipx/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "process_automation/", "title": "Process Automation", "text": "<p>I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes.</p> <p>Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness.</p> <p>I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, as well as life logging and task management, allows you to delegate those worries.</p> <p>Process automation can lead to habit building, which reduces even more the willpower consumption of processes, at the same time it reduces the error rate.</p> <p>The marvelous xkcd comic has gathered the essence and pitfalls of process automation many times:</p> <p></p> <p></p> <p></p>"}, {"location": "process_automation/#automating-home-chores", "title": "Automating home chores", "text": "<ul> <li>Using Grocy to maintain the house stock, shopping lists and meal   plans.</li> </ul>"}, {"location": "profanity/", "title": "profanity", "text": "<p>profanity is a console based XMPP client written in C using ncurses and libstrophe, inspired by Irssi.</p>"}, {"location": "profanity/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install profanity\n</code></pre>"}, {"location": "profanity/#usage", "title": "Usage", "text": ""}, {"location": "profanity/#connect", "title": "Connect", "text": "<p>To connect to an XMPP chat service:</p> <pre><code>/connect user@server.com\n</code></pre> <p>You will be prompted by the status bar to enter your password.</p>"}, {"location": "profanity/#send-one-to-one-message", "title": "Send one to one message", "text": "<p>To open a new window and send a message use the <code>/msg</code> command:</p> <pre><code>/msg mycontact@server.com Hello there!\n</code></pre> <p>Profanity uses the contact's nickname by default, if one exists. For example:</p> <pre><code>/msg Bob Are you there bob?\n</code></pre>"}, {"location": "profanity/#window-navigation", "title": "Window navigation", "text": "<p>To make a window visible in the main window area, use any of the following:</p> <ul> <li><code>Alt-1</code> to <code>Alt-0</code></li> <li><code>F1</code> to <code>F10</code></li> <li><code>Alt-left</code>, <code>Alt-right</code></li> </ul> <p>The <code>/win</code> command may also be used. Either the window number may be passed, or the window title:</p> <pre><code>/win 4\n/win someroom@chatserver.org\n/win MyBuddy\n</code></pre> <p>To close the current window:</p> <pre><code>/close\n</code></pre>"}, {"location": "profanity/#adding-contacts", "title": "Adding contacts", "text": "<p>To add someone to your roster:</p> <pre><code>/roster add newfriend@server.chat.com\n</code></pre> <p>To subscribe to a contacts presence (to be notified when they are online/offline etc):</p> <pre><code>/sub request newfriend@server.chat.com\n</code></pre> <p>To approve a contact's request to subscribe to your presence:</p> <pre><code>/sub allow newfriend@server.chat.com\n</code></pre>"}, {"location": "profanity/#giving-contacts-a-nickname", "title": "Giving contacts a nickname", "text": "<pre><code>/roster nick bob@company.org Bobster\n</code></pre>"}, {"location": "profanity/#logging-out", "title": "Logging out", "text": "<p>To quit profanity:</p> <pre><code>/quit\n</code></pre>"}, {"location": "profanity/#configure-omemo", "title": "Configure OMEMO", "text": "<pre><code>/omemo gen\n/carbons on\n</code></pre>"}, {"location": "profanity/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Quickstart</li> </ul>"}, {"location": "prompt_toolkit_fullscreen_applications/", "title": "Prompt toolkit full screen applications", "text": "<p>Prompt toolkit can be used to build full screen interfaces. This section focuses in how to do it. If you want to build REPL applications instead go to this other article.</p> <p>Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings.</p> <p>Every prompt_toolkit application is an instance of an <code>Application</code> object.</p> <pre><code>from prompt_toolkit import Application\n\napp = Application(full_screen=True)\napp.run()\n</code></pre> <p>When <code>run()</code> is called, the event loop will run until the application is done. An application will quit when exit() is called. The event loop is basically a while-true loop that waits for user input, and when it receives something (like a key press), it will send that to the appropriate handler, like for instance, a key binding.</p> <p>An application consists of several components. The most important are:</p> <ul> <li>I/O objects: the input and output device.</li> <li>The layout: this defines the graphical structure of the application. For     instance, a text box on the left side, and a button on the right side. You     can also think of the layout as a collection of \u2018widgets\u2019.</li> <li>A style: this defines what colors and underline/bold/italic styles are used     everywhere.</li> <li>A set of key bindings.</li> </ul>"}, {"location": "prompt_toolkit_fullscreen_applications/#the-layout", "title": "The layout", "text": "<p>With the <code>Layout</code> object you define the graphical structure of the application, it accepts as argument a nested structure of <code>Container</code> objects, these arrange the layout by splitting the screen in many regions, while controls (children of <code>UIControl</code>, such as <code>BufferControl</code> or <code>FormattedTextControl</code>) are responsible for generating the actual content.</p> <p>Some of the <code>Container</code>s you can use are: <code>HSplit</code>, <code>Vsplit</code>, <code>FloatContainer</code>, <code>Window</code> or <code>ScrollablePane</code>. The <code>Window</code> class itself is particular: it is a <code>Container</code> that can contain a <code>UIControl</code>. Thus, it\u2019s the adapter between the two. The <code>Window</code> class also takes care of scrolling the content and wrapping the lines if needed.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#conditional-containers", "title": "Conditional Containers", "text": "<p>Sometimes you only want to show containers when a condition is met, <code>ConditionalContainers</code> are meant to fulfill this case. They accept other containers, and a <code>filter</code> condition.</p> <p>You can read more about filters here, the simplest use case is if you have a boolean variable and you use the <code>to_filter</code> function.</p> <pre><code>from prompt_toolkit.layout import ConditionalContainer\nfrom prompt_toolkit.filters.utils import to_filter\n\nshow_header = True\nConditionalContainer(\n    Label('This is an optional text'), filter=to_filter(show_header)\n)\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#tables", "title": "Tables", "text": "<p>Currently they are not supported :(, although there is an old PR.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#controls", "title": "Controls", "text": ""}, {"location": "prompt_toolkit_fullscreen_applications/#focusing-windows", "title": "Focusing windows", "text": "<p>Focusing something can be done by calling the <code>focus()</code> method. This method is very flexible and accepts a <code>Window</code>, a <code>Buffer</code>, a <code>UIControl</code> and more.</p> <p>In the following example, we use <code>get_app()</code> for getting the active application.</p> <pre><code>from prompt_toolkit.application import get_app\n\n# This window was created earlier.\nw = Window()\n\n# ...\n\n# Now focus it.\nget_app().layout.focus(w)\n</code></pre> <p>To focus the next window in the layout you can use <code>app.layout.focus_next()</code>.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#key", "title": "[Key", "text": "<p>bindings](https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#key-bindings)</p> <p>In order to react to user actions, we need to create a <code>KeyBindings</code> object and pass that to our <code>Application</code>.</p> <p>There are two kinds of key bindings:</p> <ul> <li>Global key bindings, which are always active.</li> <li>Key bindings that belong to a certain <code>UIControl</code> and are only active when     this control is focused. Both <code>BufferControl</code> and <code>FormattedTextControl</code>     take a <code>key_bindings</code> argument.</li> </ul> <p>For complex keys you can always look at the <code>Keys</code> class.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#global-key", "title": "[Global key", "text": "<p>bindings](https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#global-key-bindings)</p> <p>Key bindings can be passed to the application as follows:</p> <pre><code>from prompt_toolkit import Application\nfrom prompt_toolkit.key_binding.key_processor import KeyPressEvent\n\nkb = KeyBindings()\napp = Application(key_bindings=kb)\napp.run()\n</code></pre> <p>To register a new keyboard shortcut, we can use the <code>add()</code> method as a decorator of the key handler:</p> <pre><code>from prompt_toolkit import Application\nfrom prompt_toolkit.key_binding import KeyBindings\nfrom prompt_toolkit.key_binding.key_processor import KeyPressEvent\n\nkb = KeyBindings()\n\n@kb.add('c-q')\ndef exit_(event: KeyPressEvent) -&gt; None:\n    \"\"\"\n    Pressing Ctrl-Q will exit the user interface.\n\n    Setting a return value means: quit the event loop that drives the user\n    interface and return this value from the `Application.run()` call.\n    \"\"\"\n    event.app.exit()\n\napp = Application(key_bindings=kb, full_screen=True)\napp.run()\n</code></pre> <p>Here you can read for more complex patterns with key bindings.</p> <p>A more programmatically way to add bindings is:</p> <pre><code>from prompt_toolkit.key_binding import KeyBindings\nfrom prompt_toolkit.key_binding.bindings.focus import focus_next\n\nkb = KeyBindings()\nkb.add(\"tab\")(focus_next)\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#pass-more-than-one-key", "title": "Pass more than one key", "text": "<p>To map an action to two key presses use <code>kb.add('g', 'g')</code>.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#styles", "title": "Styles", "text": "<p>Many user interface controls, like <code>Window</code> accept a style argument which can be used to pass the formatting as a string. For instance, we can select a foreground color:</p> <ul> <li><code>fg:ansired</code>: ANSI color palette</li> <li><code>fg:ansiblue</code>: ANSI color palette</li> <li><code>fg:#ffaa33</code>: hexadecimal notation</li> <li><code>fg:darkred</code>: named color</li> </ul> <p>Or a background color:</p> <ul> <li><code>bg:ansired</code>: ANSI color palette</li> <li><code>bg:#ffaa33</code>: hexadecimal notation</li> </ul> <p>Like we do for web design, it is not a good habit to specify all styling inline. Instead, we can attach class names to UI controls and have a style sheet that refers to these class names. The Style can be passed as an argument to the <code>Application</code>.</p> <pre><code>from prompt_toolkit.layout import VSplit, Window\nfrom prompt_toolkit.styles import Style\n\nlayout = VSplit([\n    Window(BufferControl(...), style='class:left'),\n    HSplit([\n        Window(BufferControl(...), style='class:top'),\n        Window(BufferControl(...), style='class:bottom'),\n    ], style='class:right')\n])\n\nstyle = Style([\n     ('left', 'bg:ansired'),\n     ('top', 'fg:#00aaaa'),\n     ('bottom', 'underline bold'),\n ])\n</code></pre> <p>You may need to define the 24bit color depths to see the colors you expect:</p> <pre><code>from prompt_toolkit.output.color_depth import ColorDepth\n\napp = Application(\n    color_depth=ColorDepth.DEPTH_24_BIT,\n    # ...\n)\n</code></pre> <p>If you want to see if a style is being applied in a component, set the style to <code>bg:#dc322f</code> and it will be highlighted in red.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#dynamically-changing-the-style", "title": "Dynamically changing the style", "text": "<p>You'll need to create a widget, you can take as inspiration the package widgets.</p> <p>To create a row that changes color when it's focused use:</p> <pre><code>from prompt_toolkit.layout.controls import FormattedTextControl\nfrom prompt_toolkit.layout.containers import Window\nfrom prompt_toolkit.application import get_app\n\nclass Row:\n    \"\"\"Define row.\n\n    Args:\n        text: text to print\n    \"\"\"\n\n    def __init__(\n        self,\n        text: str,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\"\"\"\n        self.text = text\n        self.control = FormattedTextControl(\n            self.text,\n            focusable=True,\n        )\n\n        def get_style() -&gt; str:\n            if get_app().layout.has_focus(self):\n                return \"class:row.focused\"\n            else:\n                return \"class:row\"\n\n        self.window = Window(\n            self.control, height=1, style=get_style, always_hide_cursor=True\n        )\n\n    def __pt_container__(self) -&gt; Window:\n        \"\"\"Return the window object.\n\n        Mandatory to be considered a widget.\n        \"\"\"\n        return self.window\n</code></pre> <p>An example of use would be:</p> <pre><code>layout = HSplit(\n    [\n        Row(\"Test1\"),\n        Row(\"Test2\"),\n        Row(\"Test3\"),\n    ]\n)\n\n# Key bindings\n\nkb = KeyBindings()\n\nkb.add(\"j\")(focus_next)\nkb.add(\"k\")(focus_previous)\n\n\n@kb.add(\"c-c\", eager=True)\n@kb.add(\"q\", eager=True)\ndef exit_(event: KeyPressEvent) -&gt; None:\n    \"\"\"Exit the user interface.\"\"\"\n    event.app.exit()\n\n\n# Styles\n\nstyle = Style(\n    [\n        (\"row\", \"bg:#073642 #657b83\"),\n        (\"row.focused\", \"bg:#002b36 #657b83\"),\n    ]\n)\n\n# Application\n\napp = Application(\n    layout=Layout(layout),\n    full_screen=True,\n    key_bindings=kb,\n    style=style,\n    color_depth=ColorDepth.DEPTH_24_BIT,\n)\n\napp.run()\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#examples", "title": "Examples", "text": "<p>The best way to understand how it works is by running the examples in the repository, some interesting ones in increasing order of difficult are:</p> <ul> <li>Managing     autocompletion</li> <li>Managing     focus</li> <li>Managing     floats,     and floats with     transparency</li> <li>Add     margins,     such as line number or a scroll bar.</li> <li>Managing     styles.</li> <li>Wrapping lines in     buffercontrol, and line prefixes</li> <li>A working REPL     example.     Interesting to see the <code>SearchToolbar</code> in use (press <code>Ctrl+r</code>), and how to     interact with other windows with handlers.</li> <li>A working     editor:     Complex application that shows how to build a working menu.</li> <li>pyvim: A rewrite of vim in python,     it shows how to test, handle commands and a lot more, very interesting.</li> <li>pymux: Another full screen     application example.</li> </ul>"}, {"location": "prompt_toolkit_fullscreen_applications/#testing", "title": "Testing", "text": "<p>Prompt toolkit application testing can be done at different levels:</p> <ul> <li>Component level: Useful to test how a component manages it's data by itself.</li> <li>Application level: Useful to test how a user interacts with the component.</li> </ul> <p>If you don't know how to test something, I suggest you check how prompt toolkit tests itself. You can also check how do third party packages do their tests too, such as <code>prompt-toolkit-table</code> or <code>pyvim</code>.</p> <p>Keep in mind that you don't usually want to check the result of the <code>stdout</code> or <code>stderr</code> directly, but the state of your component or the application itself.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#component-level", "title": "Component level", "text": "<p>If your component accepts some input and does some magic on that input without the need to load the application, import the object directly and run tests changing the input directly and asserting the results of the output.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#application-level", "title": "Application level", "text": "<p>If you want to test the interaction with your component at application level, for example what happens when a user presses a key, you need to instantiate a dummy application and play with it.</p> <p>Imagine we have a <code>TableControl</code> component we want to test that accepts some input in the form of <code>data</code>. We'll use the <code>set_dummy_app</code> function to configure an application that outputs to <code>DummyOutput</code>, and a helper function <code>get_app_and_processor</code> to return the active app and a <code>processor</code> to send key presses.</p> <pre><code>def set_dummy_app(data: Any) -&gt; Any:\n    \"\"\"Return a context manager that starts the dummy application.\n\n    This is important, because we need an `Application` with `is_done=False`\n    flag, otherwise no keys will be processed.\n    \"\"\"\n    app: Application[Any] = Application(\n        layout=Layout(Window(TableControl(data))),\n        output=DummyOutput(),\n        input=create_pipe_input(),\n    )\n\n    return set_app(app)\n\n\ndef get_app_and_processor() -&gt; Tuple[Application[Any], KeyProcessor]:\n    \"\"\"Return the active application and it's key processor.\"\"\"\n    app = get_app()\n    key_bindings = app.layout.container.get_key_bindings()\n\n    if key_bindings is None:\n        key_bindings = KeyBindings()\n    processor = KeyProcessor(key_bindings)\n    return app, processor\n</code></pre> <p>We've loaded the <code>processor</code> with the key bindings defined in the container. If you want other bindings change them there. For example prompt-toolkit uses a fixture to set them. Remember that you have the <code>merge_key_bindings</code> to join two key binding objects with:</p> <pre><code>key_bindings = merge_key_bindings([key_bindings, control_bindings])\n</code></pre> <p>Once the functions are set, you can make your test. Imagine that we want to check that if the user presses <code>j</code>, the variable <code>_focused_row</code> is incremented by 1. This variable will be used by the component internally to change the style of the rows so that the next element is highlighted.</p> <pre><code>def test_j_moves_to_the_next_row(self, pydantic_data: PydanticData) -&gt; None:\n    \"\"\"\n    Given: A well configured table\n    When: j is press\n    Then: the focus is moved to the next line\n    \"\"\"\n    with set_dummy_app(pydantic_data):\n        app, processor = get_app_and_processor()\n\n        processor.feed(KeyPress(\"j\", \"j\"))  # act\n\n        processor.process_keys()\n        assert app.layout.container.content._focused_row == 1\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Projects using prompt_toolkit</li> </ul>"}, {"location": "prompt_toolkit_repl/", "title": "Prompt toolkit REPL", "text": "<p>Prompt toolkit can be used to build REPL interfaces. This section focuses in how to do it. If you want to build full screen applications instead go to this other article.</p>"}, {"location": "prompt_toolkit_repl/#testing", "title": "Testing", "text": "<p>Testing prompt_toolkit or any text-based user interface (TUI) with python is not well documented. Some of the main developers suggest mocking it while others use pexpect.</p> <p>With the first approach you can test python functions and methods internally but it can lead you to the over mocking problem. The second will limit you to test functionality exposed through your program's command line, as it will spawn a process and interact it externally.</p> <p>Given that the TUIs are entrypoints to your program, it makes sense to test them in end-to-end tests, so I'm going to follow the second option. On the other hand, you may want to test a small part of your TUI in a unit test, if you want to follow this other path, I'd start with monkeypatch, although I didn't have good results with it.</p> <pre><code>def test_method(monkeypatch):\n    monkeypatch.setattr('sys.stdin', io.StringIO('my input'))\n</code></pre>"}, {"location": "prompt_toolkit_repl/#using-pexpect", "title": "Using pexpect", "text": "<p>This method is useful to test end to end functions as you need to all the program as a command line. You can't use it to tests python functions internally.</p> <p>File: source.py</p> <pre><code>from prompt_toolkit import prompt\n\ntext = prompt(\"Give me some input: \")\nwith open(\"/tmp/tui.txt\", \"w\") as f:\n    f.write(text)\n</code></pre> <p>File: test_source.py</p> <p>```python import pexpect</p> <p>def test_tui() -&gt; None:     tui = pexpect.spawn(\"python source.py\", timeout=5)     tui.expect(\"Give me .*\")     tui.sendline(\"HI\")     tui.expect_exact(pexpect.EOF)</p> <pre><code>with open(\"/tmp/tui.txt\", \"r\") as f:\n    assert f.read() == \"HI\"\n</code></pre> <p>```</p> <p>Where:</p> <ul> <li>The <code>tui.expect_exact(pexpect.EOF)</code> line is required so that the tests aren't     run before the process has ended, otherwise the file might not exist yet.</li> <li>The <code>timeout=5</code> is required in case that the <code>pexpect</code> interaction is not well     defined, so that the test is not hung forever.</li> </ul> <p>Thank you Jairo Llopis for this solution.</p> <p>I've deduced the solution from his #260 PR into copier, and the comments of #1243</p>"}, {"location": "psu/", "title": "Power supply unit", "text": "<p>Power supply unit is the component of the computer that sources power from the primary source (the power coming from your wall outlet) and delivers it to its motherboard and all its components. Contrary to the common understanding, the PSU does not supply power to the computer; it instead converts the AC (Alternating Current) power from the source to the DC (Direct Current) power that the computer needs.</p> <p>There are two types of PSU: Linear and Switch-mode. Linear power supplies have a built-in transformer that steps down the voltage from the main to a usable one for the individual parts of the computer. The transformer makes the Linear PSU bulky, heavy, and expensive. Modern computers have switched to the switch-mode power supply, using switches instead of a transformer for voltage regulation. They\u2019re also more practical and economical to use because they\u2019re smaller, lighter, and cheaper than linear power supplies.</p> <p>PSU need to deliver at least the amount of power that each component requires, if it needs to deliver more, it simply won't work.</p> <p>Another puzzling question for most consumers is, \u201cDoes a PSU supply constant wattage to the computer?\u201d The answer is a flat No. The wattage you see on the PSUs casing or labels only indicates the maximum power it can supply to the system, theoretically. For example, by theory, a 500W PSU can supply a maximum of 500W to the computer. In reality, the PSU will draw a small portion of the power for itself and distributes power to each of the PC components according to its need. The amount of power the components need varies from 3.3V to 12V. If the total power of the components needs to add up to 250W, it would only use 250W of the 500W, giving you an overhead for additional components or future upgrades.</p> <p>Additionally, the amount of power the PSU supplies varies during peak periods and idle times. When the components are pushed to their limits, say when a video editor maximizes the GPU for graphics-intensive tasks, it would require more power than when the computer is used for simple tasks like web-browsing. The amount of power drawn from the PSU would depend on two things; the amount of power each component requires and the tasks that each component performs.</p>"}, {"location": "psu/#power-supply-efficiency", "title": "Power supply efficiency", "text": "<p>When PSU converts the AC power to DC, some of the power is wasted and is converted to heat. The more heat a PSU generates, the less efficient it is. Inefficient PSUs will likely damage the computer\u2019s components or shorten their lifespans in the long run. They also draw more power from the primary source resulting in higher electricity bills for consumers.</p> <p>You might\u2019ve seen 80 PLUS stickers on PSUs or its other variants like 80 PLUS Bronze, Silver, Gold, Platinum, and Titanium. 80 PLUS is the power supply\u2019s efficiency rating; the power supply must reach 80% efficiency to be certified. It\u2019s a voluntary standard, which means companies don\u2019t need to abide by the standard, but 80 PLUS certifications have become popular because a more efficient power supply can lessen the consumers\u2019 carbon footprint and help them save some bucks on their electric bills. Below is the efficiency rating that a PSU needs to achieve to get the desired rating.</p> Certification Levels Efficiency at 10% Load Efficiency at 20% Load Efficiency at 50% Load Efficiency at 100% Load 80 PLUS (White) \u2014 80% 80% 80% 80 PLUS Bronze \u2014 82% 85% 82% 80 PLUS Silver \u2014 85% 88% 85% 80 PLUS Gold \u2014 87% 90% 87% 80 PLUS Platinum \u2014 90% 92% 89% 80 PLUS Titanium 90% 92% 94% 90% <p>It\u2019s important to note that the 80% efficiency does not mean that the PSU will only supply 80% of its capacity to the computer. It means it will draw additional power from the primary source to only 20% of power is lost or generated as heat during the conversion. A 500W PSU will therefore draw 625W of power from the main to make it 80% efficient.</p> <p>Higher efficiency also means the internal components are subjected to less heat and are likely to have a longer lifespan. They may cost a bit more, but higher certified power supplies tend to be more reliable than others. Luckily, most manufacturers offer warranties.</p>"}, {"location": "psu/#power-supply-shopping-tips", "title": "Power supply shopping tips", "text": "<ul> <li> <p>Determine wattage requirements: You don't need to purchase much more potential     power capacity (wattage) than you\u2019ll ever use. You can calculate roughly how     much power your new or upgraded system will draw from the wall and look for     a capacity point that satisfies your demands. Several power supply sellers have     calculators that will give you a rough estimate of your system's power needs.     You can find a few below:</p> <ul> <li>PCPartPicker</li> <li>OuterVision PSU calculator</li> <li>be quiet! PSU     Calculator</li> <li>Cooler Master Power Calculator</li> <li>Seasonic Wattage Calculator</li> <li>MSI PSU Calculator</li> <li>Newegg PSU Calculator</li> </ul> </li> <li> <p>Consider upcoming GPU power requirements: Although the best graphics cards     are usually more power-efficient than previous generations, their power     consumption increases overall. This is why the latest 12+4 pin connector     that the upcoming generation graphics cards will use will provide up to 600     W of power. Currently, a pair of PCIe 6+2 pin connectors on dedicated cables     are officially rated for up to 300W, and three of these connectors can     deliver up to 450W safely. You should also add the up to 75W that the PCIe     slot can provide in these numbers.</p> <p>What troubles today's power supplies is not the maximum sustained power consumption of a GPU but its power spikes, and this is why various manufacturers suggest strong PSUs for high-end graphics cards. If the PSU's over current and over power protection features are conservatively set, the PSU can shut down once the graphics card asks for increased power, even for very short periods ( nanoseconds range). This is why EVGA offers two different OPP features in its G6 and P6 units, called firmware and hardware OPP. The first triggers at lower loads, in the millisecond range, while the latter triggers at higher loads that last for some nanoseconds. This way, short power spikes from the graphics card are addressed without shutting down the system.</p> <p>If you add the increased power demands of modern high-end CPUs, you can quickly figure out why strong PSUs are necessary again. Please look at our GPU Benchmarks and CPU Benchmarks hierarchies to see how each of these chips perform relative to each other.</p> </li> <li> <p>Check the physical dimensions of your case before buying: If you have     a standard ATX case, whether or not it is one of the best PC cases, an ATX     power supply will fit. But many higher-wattage PSUs are longer than the     typical 5.5 inches. So you'll want to be sure of your cases' PSU clearance.     If you have an exceptionally small or slim PC case, it may require a less     typical (and more compact) SFX power supply.</p> </li> <li> <p>Consider a modular power supply: If your case has lots of room behind the     motherboard, or your chassis doesn't have a window or glass side, you can     cable-wrap the wires you don't' need and stash them inside your rig. But if     the system you're' building doesn't' have space for this, or there is no     easy place to hide your cable mess, it's' worth paying extra for a modular     power supply. Modular PSUs let you only plug in the power cables you need     and leave the rest in the box.</p> </li> </ul>"}, {"location": "psu/#market-analysis", "title": "Market analysis", "text": "<p>I'm searching for a power supply unit that can deliver 264W and can grow up to 373W. This means that the load is:</p> Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% <p>Given that PSU look to be more efficient when they have a load of 50%, the 450W or 500W would be better. Although if the efficiency goes over 80 PLUS Gold, the difference is almost negligible. I'd prioritize first the efficiency. But any PSU from 400W to 500W will work for me.</p> <p>Toms Hardware, PCGamer, IGN recommends:</p> <ul> <li>Corsair CX450: It has 80 PLUS Bronze, so I'll discard it</li> <li>XPG Pylon 450: It has 80 PLUS Bronze too...</li> </ul> <p>None of the suggested PSU are suited for my case. I'm going to select then which is the brands that are more suggested:</p> Brand Tom's recommendations PCGamer recommendations IGN Corsair 6 2 3 Be quiet 1 1 1 Silverstone 1 1 1 Cooler Master 1 0 1 XPG 1 1 0 EVGA 1 0 0 Seasonic 0 1 0 <p>It looks that the most popular are Corsair, Be quiet and Silverstone.</p> <ul> <li>Corsair doesn't have anyone with certification above Bronce.</li> <li> <p>Silverstone     Under or equal to 500 it has one gold, at 520 it has a platinum and on the     550W it has has 4 gold and another platinum.</p> <ul> <li>ET500-MG</li> <li>NJ520</li> </ul> </li> <li> <p>Be quiet has both     gold and platinum on the range of 550 and above. Gold and bronze on the 500-400W     range.</p> <ul> <li>Pure Power 11 CM 500W</li> <li>Pure Power 11 500W</li> <li>SFX L Power 500W</li> <li>Straigt Power 11 450W</li> </ul> </li> </ul> <p>It looks like I have to forget of efficiency above Gold unless I want to go with 520W. After a quick search on the provider I see that the most interesting in price are:</p> <ul> <li>Be Quiet! Straight Power 11 450W</li> <li>Be Quiet! Pure Power 11 CM 500W</li> <li>Be Quiet! Pure Power 11 500W</li> </ul> <p>I'm also going to trust Be Quiet on the CPU cooler so I'm happy with staying on the same brand for all fans.</p> Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% Model Pure Power 11 CM 500W Pure Power 11 500W Straight Power 11 450W Continuous Power 500 500 450 Peak Power 550 550 500 Min load (my case) 52% 52% 59% Max load (my case) 74% 74% 83% Topology Active Clamp / SR / DC/DC Active Clamp / SR / DC/DC LLC / SR / DC/DC Fan dB(A) at 20% 9 9.3 9.4 Fan dB(A) at 50% 9.3 9.6 9.8 Fan dB(A) at 100% 18.8 21.6 12.3 dB(A) Min load 9.68 - 10.25 dB(A) Max load 13.86 - 11.45 SIP Protection Yes No Yes Efficiency Cert Gold Gold Gold Efficiency 20% 90.6 88.2 91 Efficiency 50% 92.1 91.3 93.1 Efficiency 100% 90.1 89.9 91.7 Cable management Semi-modular Fixed Modular Cable cm to mb 55 55 60 Max cable length 95 95 115 No. of cables 7 7 8 ATX-MB (20+4-pin) 1 1 1 P4+4 (CPU) 1 1 1 PCI-e 6+2 (GPU) 2 2 2 SATA 6 6 8 Dimensions 160 x 150 x 86 150 x 150 x 86 160 x 150 x 86 Warranty (Years) 5 5 5 Price (EUR) 85.93 79.02 99.60 <p>I'd discard the Pure Power 11 500W because it:</p> <ul> <li>Has significantly worse efficiency</li> <li>Doesn't have SIP protection</li> <li>The fan is the loudest</li> </ul> <p>Between the other two Straight Power 11 has the advantages:</p> <ul> <li>1% more efficiency.</li> <li>Will make 0.5dB more noise at min load but 2.41dB less at max load. So I expect it to be more silent</li> <li>Cables look better</li> <li>Has more cable length</li> <li>Has more SATA cables (equal to my number of drives)</li> </ul> <p>And the disadvantages:</p> <ul> <li>Is 13.66 EUR more expensive</li> <li>Has 50W less of power</li> </ul> <p>It doesn't look like I'm going to need the extra power, and if I need it (if I add a graphic card) then the 500W wouldn't work either. And the difference in money is not that big. Therefore I'll go with the Straight Power 11 450W</p>"}, {"location": "psu/#references", "title": "References", "text": "<ul> <li>Linuxhint article on PSU</li> </ul>"}, {"location": "pydantic_factories/", "title": "Pydantic factories", "text": "<p>Pydantic factories is a library offers powerful mock data generation capabilities for pydantic based models and dataclasses. It automatically creates FactoryBoy factories from a pydantic model.</p>"}, {"location": "pydantic_factories/#example", "title": "Example", "text": "<pre><code>from datetime import date, datetime\nfrom typing import List, Union\n\nfrom pydantic import BaseModel, UUID4\n\nfrom pydantic_factories import ModelFactory\n\n\nclass Person(BaseModel):\n    id: UUID4\n    name: str\n    hobbies: List[str]\n    age: Union[float, int]\n    birthday: Union[datetime, date]\n\n\nclass PersonFactory(ModelFactory[Any]):\n    __model__ = Person\n\n\nresult = PersonFactory.build()\n</code></pre> <p>This is possible because of the typing information available on the pydantic model and model-fields, which are used as a source of truth for data generation.</p> <p>The factory parses the information stored in the pydantic model and generates a dictionary of kwargs that are passed to the <code>Person</code> class' init method.</p>"}, {"location": "pydantic_factories/#installation", "title": "Installation", "text": "<pre><code>pip install pydantic-factories\n</code></pre>"}, {"location": "pydantic_factories/#basic-usage", "title": "Basic Usage", "text": ""}, {"location": "pydantic_factories/#build-methods", "title": "Build Methods", "text": "<p>The <code>ModelFactory</code> class exposes two build methods:</p> <ul> <li><code>.build(**kwargs)</code>: builds a single instance of the factory's model.</li> <li><code>.batch(size: int, **kwargs)</code>: build a list of size <code>n</code> instances.</li> </ul> <pre><code>result = PersonFactory.build()  # a single Person instance\n\nresult = PersonFactory.batch(size=5)  # list[Person, Person, Person, Person, Person]\n</code></pre> <p>Any kwargs you pass to <code>.build</code>, <code>.batch</code> or any of the persistence methods, will take precedence over whatever defaults are defined on the factory class itself.</p>"}, {"location": "pydantic_factories/#nested-models-and-complex-types", "title": "Nested Models and Complex types", "text": "<p>The automatic generation of mock data works for all types supported by pydantic, as well as nested classes that derive from <code>BaseModel</code> (including for 3<sup>rd</sup> party libraries) and complex types.</p>"}, {"location": "pydantic_factories/#defining-factory-attributes", "title": "Defining Factory Attributes", "text": "<p>The factory api is designed to be as semantic and simple as possible, lets look at several examples that assume we have the following models:</p> <pre><code>from datetime import date, datetime\nfrom enum import Enum\nfrom pydantic import BaseModel, UUID4\nfrom typing import Any, Dict, List, Union\n\n\nclass Species(str, Enum):\n    CAT = \"Cat\"\n    DOG = \"Dog\"\n\n\nclass Pet(BaseModel):\n    name: str\n    species: Species\n\n\nclass Person(BaseModel):\n    id: UUID4\n    name: str\n    hobbies: List[str]\n    age: Union[float, int]\n    birthday: Union[datetime, date]\n    pets: List[Pet]\n    assets: List[Dict[str, Dict[str, Any]]]\n</code></pre> <p>One way of defining defaults is to use hardcoded values:</p> <pre><code>pet = Pet(name=\"Roxy\", sound=\"woof woof\", species=Species.DOG)\n\n\nclass PersonFactory(ModelFactory):\n    __model__ = Person\n\n    pets = [pet]\n</code></pre> <p>In this case when we call <code>PersonFactory.build()</code> the result will be randomly generated, except the pets list, which will be the hardcoded default we defined.</p>"}, {"location": "pydantic_factories/#use-field", "title": "Use field", "text": "<p>This though is often not desirable. We could instead, define a factory for <code>Pet</code> where we restrict the choices to a range we like. For example:</p> <pre><code>from enum import Enum\nfrom pydantic_factories import ModelFactory, Use\nfrom random import choice\n\nfrom .models import Pet, Person\n\n\nclass Species(str, Enum):\n    CAT = \"Cat\"\n    DOG = \"Dog\"\n\n\nclass PetFactory(ModelFactory):\n    __model__ = Pet\n\n    name = Use(choice, [\"Ralph\", \"Roxy\"])\n    species = Use(choice, list(Species))\n\n\nclass PersonFactory(ModelFactory):\n    __model__ = Person\n\n    pets = Use(PetFactory.batch, size=2)\n</code></pre> <p>The signature for use is: <code>cb: Callable, *args, **defaults</code>, it can receive any sync callable. In the above example, we used the choice function from the standard library's random package, and the batch method of <code>PetFactory</code>.</p> <p>You do not need to use the <code>Use</code> field, you can place callables (including classes) as values for a factory's attribute directly, and these will be invoked at build-time. Thus, you could for example re-write the above <code>PetFactory</code> like so:</p> <pre><code>class PetFactory(ModelFactory):\n    __model__ = Pet\n\n    name = lambda: choice([\"Ralph\", \"Roxy\"])\n    species = lambda: choice(list(Species))\n</code></pre> <p><code>Use</code> is merely a semantic abstraction that makes the factory cleaner and simpler to understand.</p>"}, {"location": "pydantic_factories/#ignore-field", "title": "Ignore (field)", "text": "<p><code>Ignore</code> is another field exported by this library, and its used - as its name implies - to designate a given attribute as ignored:</p> <pre><code>from typing import TypeVar\n\nfrom odmantic import EmbeddedModel, Model\nfrom pydantic_factories import ModelFactory, Ignore\n\nT = TypeVar(\"T\", Model, EmbeddedModel)\n\n\nclass OdmanticModelFactory(ModelFactory[T]):\n    id = Ignore()\n</code></pre> <p>The above example is basically the extension included in pydantic-factories for the library ODMantic, which is a pydantic based mongo ODM.</p> <p>For ODMantic models, the id attribute should not be set by the factory, but rather handled by the odmantic logic itself. Thus, the id field is marked as ignored.</p> <p>When you ignore an attribute using <code>Ignore</code>, it will be completely ignored by the factory - that is, it will not be set as a kwarg passed to pydantic at all.</p>"}, {"location": "pydantic_factories/#require-field", "title": "Require (field)", "text": "<p>The <code>Require</code> field in turn specifies that a particular attribute is a required kwarg. That is, if a kwarg with a value for this particular attribute is not passed when calling <code>factory.build()</code>, a <code>MissingBuildKwargError</code> will be raised.</p> <p>What is the use case for this? For example, lets say we have a document called <code>Article</code> which we store in some DB and is represented using a non-pydantic model. We then need to store in our pydantic object a reference to an id for this article. This value should not be some mock value, but must rather be an actual id passed to the factory. Thus, we can define this attribute as required:</p> <pre><code>from pydantic import BaseModel\nfrom pydantic_factories import ModelFactory, Require\nfrom uuid import UUID\n\n\nclass ArticleProxy(BaseModel):\n    article_id: UUID\n    ...\n\n\nclass ArticleProxyFactory(ModelFactory):\n    __model__ = ArticleProxy\n\n    article_id = Require()\n</code></pre> <p>If we call <code>factory.build()</code> without passing a value for <code>article_id</code>, an error will be raised.</p>"}, {"location": "pydantic_factories/#creating-your-custom", "title": "[Creating your custom", "text": "<p>factories](https://starlite-api.github.io/pydantic-factories/usage/7-handling-custom-types/?h=custom)</p> <p>If your model has an attribute that is not supported by <code>pydantic-factories</code> and it depends on third party libraries, you can create your custom extension subclassing the <code>ModelFactory</code>, and overriding the <code>get_mock_value</code> method to add your logic.</p> <pre><code>from pydantic_factories import ModelFactory\n\nclass CustomFactory(ModelFactory[Any]):\n    \"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\"\n\n    @classmethod\n    def get_mock_value(cls, field_type: Any) -&gt; Any:\n        \"\"\"Add our custom mock value.\"\"\"\n        if str(field_type) == \"my_super_rare_datetime_field\":\n            return cls._get_faker().date_time_between()\n\n        return super().get_mock_value(field_type)\n</code></pre> <p>Where <code>cls._get_faker()</code> is a <code>faker</code> instance that you can use to build your returned value.</p>"}, {"location": "pydantic_factories/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "pydantic_factories/#use-pydantic-factories-with-pytest-freezegun", "title": "Use pydantic-factories with pytest-freezegun", "text": "<p>As <code>pytest-freezegun</code> overrides the <code>datetime.datetime</code> class, <code>pydantic-factories</code> is not able to find the correct generator for the datetime fields. It's solved by creating a custom factory</p> <pre><code>from typing import Any\nfrom pydantic_factories import ModelFactory\n\n\nclass CustomFactory(ModelFactory[Any]):\n    \"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\"\n\n    @classmethod\n    def get_mock_value(cls, field_type: Any) -&gt; Any:\n        \"\"\"Add our custom mock value.\"\"\"\n        if str(field_type) == \"&lt;class 'datetime.datetime'&gt;\":\n            return cls._get_faker().date_time_between()\n\n        return super().get_mock_value(field_type)\n</code></pre>"}, {"location": "pydantic_factories/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "pyment/", "title": "Pyment", "text": "<p>Pyment is a python3 program to automatically create, update or convert docstrings in existing Python files, managing several styles.</p>"}, {"location": "pyment/#installation", "title": "Installation", "text": "<pre><code>pip install pyment\n</code></pre>"}, {"location": "pyment/#usage", "title": "Usage", "text": "<pre><code>$: pyment  myfile.py    # will generate a patch\n$: pyment -w myfile.py  # will overwrite the file\n</code></pre> <p>As of 2021-11-17, the program is not production ready yet for me, I've tested it in one of my projects and found some bugs that needed to be fixed before it's usable. Despite the number of stars, it looks like the development pace has dropped dramatically, so it needs our help to get better :).</p>"}, {"location": "pyment/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "pytest_httpserver/", "title": "Pytest httpserver", "text": "<p>pytest-httpserver is a python package which allows you to start a real HTTP server for your tests. The server can be configured programmatically to how to respond to requests.</p>"}, {"location": "pytest_httpserver/#installation", "title": "Installation", "text": "<pre><code>pip install pytest-httpserver\n</code></pre>"}, {"location": "pytest_httpserver/#usage", "title": "Usage", "text": "<pre><code>import requests\n\ndef test_json_client(httpserver: HTTPServer):\n    httpserver.expect_request(\"/foobar\").respond_with_json({\"foo\": \"bar\"})\n    assert requests.get(httpserver.url_for(\"/foobar\")).json() == {'foo': 'bar'}\n</code></pre>"}, {"location": "pytest_httpserver/#specifying-responses", "title": "Specifying responses", "text": "<p>Once you have set up the expected request, it is required to set up the response which will be returned to the client.</p> <p>In the example we used <code>respond_with_json()</code> but it is also possible to respond with an arbitrary content.</p> <pre><code>respond_with_data(\"Hello world!\", content_type=\"text/plain\")\n</code></pre> <p>In the example above, we are responding a <code>text/plain</code> content. You can specify the status also:</p> <pre><code>respond_with_data(\"Not found\", status=404, content_type=\"text/plain\")\n</code></pre>"}, {"location": "pytest_httpserver/#give-a-dynamic-response", "title": "Give a dynamic response", "text": "<p>If you need to produce dynamic content, use the <code>respond_with_handler</code> method, which accepts a callable (eg. a python function):</p> <pre><code>from werkzeug.wrappers import Response\nfrom werkzeug.wrappers.request import Request\n\ndef my_handler(request: Request) -&gt; Response:\n    # here, examine the request object\n    return Response(\"Hello world!\")\n\nrespond_with_handler(my_handler)\n</code></pre>"}, {"location": "pytest_httpserver/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "python/", "title": "Python", "text": "<p>Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.</p>"}, {"location": "python/#install", "title": "Install", "text": "<pre><code>apt-get install python\n</code></pre>"}, {"location": "python/#install-a-specific-version", "title": "Install a specific version", "text": "<ul> <li> <p>Install dependencies     <pre><code>sudo apt install wget software-properties-common build-essential libnss3-dev zlib1g-dev libgdbm-dev libncurses5-dev libssl-dev libffi-dev libreadline-dev libsqlite3-dev libbz2-dev\n</code></pre></p> </li> <li> <p>Select the version in https://www.python.org/ftp/python/ and download it     <pre><code>wget https://www.python.org/ftp/python/3.9.2/Python-3.9.2.tgz\ncd Python-3.9.2/\n./configure --enable-optimizations\nsudo make altinstall\n</code></pre></p> </li> </ul>"}, {"location": "python/#generators", "title": "Generators", "text": "<p>Generator functions are a special kind of function that return a lazy iterator. These are objects that you can loop over like a list. However, unlike lists, lazy iterators do not store their contents in memory.</p> <p>An example would be an infinite sequence generator</p> <pre><code>def infinite_sequence():\n    num = 0\n    while True:\n        yield num\n        num += 1\n</code></pre> <p>You can use it as a list:</p> <pre><code>for i in infinite_sequence():\n...     print(i, end=\" \")\n...\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n30 31 32 33 34 35 36 37 38 39 40 41 42\n[...]\n</code></pre> <p>Instead of using a <code>for</code> loop, you can also call <code>next()</code> on the generator object directly. This is especially useful for testing a generator in the console:.</p> <pre><code>&gt;&gt;&gt; gen = infinite_sequence()\n&gt;&gt;&gt; next(gen)\n0\n&gt;&gt;&gt; next(gen)\n1\n&gt;&gt;&gt; next(gen)\n2\n&gt;&gt;&gt; next(gen)\n3\n</code></pre>"}, {"location": "python/#understanding-generators", "title": "Understanding Generators", "text": "<p>Generator functions look and act just like regular functions, but with one defining characteristic. Generator functions use the Python <code>yield</code> keyword instead of <code>return</code>.</p> <p><code>yield</code> indicates where a value is sent back to the caller, but unlike <code>return</code>, you don\u2019t exit the function afterward.Instead, the state of the function is remembered. That way, when <code>next()</code> is called on a generator object (either explicitly or implicitly within a for loop), the previously yielded variable <code>num</code> is incremented, and then yielded again.</p>"}, {"location": "python/#interesting-libraries-to-explore", "title": "Interesting libraries to explore", "text": "<ul> <li>di: a modern dependency injection     system, modeled around the simplicity of FastAPI's dependency injection.</li> <li>humanize: This modest package     contains various common humanization utilities, like turning a number into     a fuzzy human-readable duration (\"3 minutes ago\") or into a human-readable     size or throughput.</li> <li>tryceratops: A linter of     exceptions.</li> <li>schedule: Python job scheduling for     humans. Run Python functions (or any other callable) periodically using     a friendly syntax.</li> <li>huey: a little task queue for python.</li> <li>textual: Textual is a TUI (Text User     Interface) framework for Python using Rich as a renderer.</li> <li>parso: Parses Python code.</li> <li> <p>kivi: Create android/Linux/iOS/Windows applications with     python. Use it with kivimd to make it beautiful,     check the examples and the     docs.</p> <p>For beginner tutorials check the real python's and towards data science (and part 2). * apprise: Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications <code>(\u00ac\u00ba-\u00b0)\u00ac</code>. * aiomultiprocess: Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. * twint: An advanced Twitter scraping &amp; OSINT tool written in Python that doesn't use Twitter's API, allowing you to scrape a user's followers, following, Tweets and more while evading most API limitations. Maybe use <code>snscrape</code> (is below) if <code>twint</code> doesn't work. * snscrape: A social networking service scraper in Python. * tweepy: Twitter for Python.</p> </li> </ul>"}, {"location": "python/#interesting-sources", "title": "Interesting sources", "text": "<ul> <li>Musa 550 looks like a nice way to     learn how to process geolocation data.</li> </ul>"}, {"location": "python_elasticsearch/", "title": "Python elasticsearch", "text": "<p>Python elasticsearch is the Official low-level client for Elasticsearch. Its goal is to provide common ground for all Elasticsearch-related code in Python; because of this it tries to be opinion-free and very extendable.</p>"}, {"location": "python_elasticsearch/#installation", "title": "Installation", "text": "<pre><code>pip install elasticsearch\n</code></pre>"}, {"location": "python_elasticsearch/#usage", "title": "Usage", "text": ""}, {"location": "python_elasticsearch/#connect-to-the-database", "title": "Connect to the database", "text": "<pre><code>from elasticsearch import Elasticsearch\n\nclient = Elasticsearch(\"http://localhost:9200\")\n</code></pre>"}, {"location": "python_elasticsearch/#get-all-indices", "title": "Get all indices", "text": "<pre><code>client.indices.get(index=\"*\")\n</code></pre>"}, {"location": "python_elasticsearch/#get-all-documents", "title": "Get all documents", "text": "<pre><code>resp = client.search(index=\"test-index\", query={\"match_all\": {}})\ndocuments = resp.body[\"hits\"][\"hits\"]\n</code></pre> <p>Use <code>pprint</code> to analyze the content of each <code>document</code>.</p>"}, {"location": "python_elasticsearch/#update-a-document", "title": "Update a document", "text": "<pre><code>doc = {\"partial_document\": \"value\"}\nresp = client.update(index=INDEX, id=id_, doc=doc)\n</code></pre>"}, {"location": "python_elasticsearch/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>TowardsDataScience article</li> </ul>"}, {"location": "python_gnupg/", "title": "python-gnupg", "text": "<p>python-gnupg is a Python library to interact with <code>gpg</code> taking care of the internal details and allows its users to generate and manage keys, encrypt and decrypt data, and sign and verify messages.</p>"}, {"location": "python_gnupg/#installation", "title": "Installation", "text": "<pre><code>pip install python-gnupg\n</code></pre>"}, {"location": "python_gnupg/#usage", "title": "Usage", "text": "<p>You interface to the GnuPG functionality through an instance of the GPG class:</p> <pre><code>gpg = gnupg.GPG(gnupghome=\"/path/to/home/directory\")\n</code></pre> <ul> <li>Decrypt a file:</li> </ul> <pre><code>gpg.decrypt_file(\"path/to/file\")\n</code></pre> <p>Note: You can't pass <code>Path</code> arguments to <code>decrypt_file</code>.</p> <ul> <li>List private keys:</li> </ul> <pre><code>&gt;&gt;&gt; public_keys = gpg.list_keys()\n&gt;&gt;&gt; private_keys = gpg.list_keys(True)\n</code></pre>"}, {"location": "python_gnupg/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Issues</li> </ul>"}, {"location": "python_internationalization/", "title": "Python Internationalization", "text": "<p>To make your code accessible to more people, you may want to support more than one language. It's not as easy as it looks as it's not enough to translate it but also it must look and feel local. The answer is internationalization.</p> <p>Internationalization (numeronymed as i18n) can be defined as the design process that ensures a program can be adapted to various languages and regions without requiring engineering changes to the source code.</p> <p>Common internationalization tasks include:</p> <ul> <li>Facilitating compliance with Unicode.</li> <li>Minimizing the use of concatenated strings.</li> <li>Accommodating support for double-byte languages (e.g. Japanese) and     right-to-left languages (for example, Hebrew).</li> <li>Avoiding hard-coded text.</li> <li>Designing for independence from cultural conventions (e. g., date and time     displays), limiting language, and character sets.</li> </ul> <p>Localization (l10n) refers to the adaptation of your program, once internationalized, to the local language and cultural habits. In theory it looks simple to implement. In practice though, it takes time and effort to provide the best Internationalization and Localization experience for your global audience.</p> <p>In Python, there is a specific bundled module for that and it\u2019s called gettext, which consists of a public API and a set of tools that help extract and generate message catalogs from the source code.</p>"}, {"location": "python_internationalization/#references", "title": "References", "text": "<ul> <li>Phrase blog on Localizing with GNU gettext</li> <li>Phrase blog on internationalization</li> </ul>"}, {"location": "python_jinja2/", "title": "Jinja2", "text": "<p>Jinja2 is a modern and designer-friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment:</p> <pre><code>&lt;title&gt;{% block title %}{% endblock %}&lt;/title&gt;\n&lt;ul&gt;\n{% for user in users %}\n  &lt;li&gt;&lt;a href=\"{{ user.url }}\"&gt;{{ user.username }}&lt;/a&gt;&lt;/li&gt;\n{% endfor %}\n&lt;/ul&gt;\n</code></pre> <p>Features:</p> <ul> <li>Sandboxed execution.</li> <li>Powerful automatic HTML escaping system for XSS prevention.</li> <li>Template inheritance.</li> <li>Compiles down to the optimal python code just in time.</li> <li>Optional ahead-of-time template compilation.</li> <li>Easy to debug. Line numbers of exceptions directly point to the correct line     in the template.</li> <li>Configurable syntax.</li> </ul>"}, {"location": "python_jinja2/#installation", "title": "Installation", "text": "<pre><code>pip install Jinja2\n</code></pre>"}, {"location": "python_jinja2/#usage", "title": "Usage", "text": "<p>The most basic way to create a template and render it is through <code>Template</code>. This however is not the recommended way to work with it if your templates are not loaded from strings but the file system or another data source:</p> <p><pre><code>&gt;&gt;&gt; from jinja2 import Template\n&gt;&gt;&gt; template = Template('Hello {{ name }}!')\n&gt;&gt;&gt; template.render(name='John Doe')\nu'Hello John Doe!'\n</code></pre> Jinja uses a central object called the template <code>Environment</code>. Instances of this class are used to store the configuration and global objects, and are used to load templates from the file system or other locations.</p> <p>The simplest way to configure Jinja to load templates for your application looks roughly like this:</p> <pre><code>from jinja2 import Environment, PackageLoader, select_autoescape\nenv = Environment(\n    loader=PackageLoader('yourapplication', 'templates'),\n    autoescape=select_autoescape(['html', 'xml'])\n)\n</code></pre> <p>This will create a template environment with the default settings and a loader that looks up the templates in the templates folder inside the <code>yourapplication</code> python package. Different loaders are available and you can also write your own if you want to load templates from a database or other resources. This also enables autoescaping for HTML and XML files.</p> <p>To load a template from this environment you just have to call the get_template() method which then returns the loaded Template:</p> <pre><code>template = env.get_template('mytemplate.html')\n</code></pre> <p>To render it with some variables, just call the <code>render()</code> method:</p> <pre><code>print(template.render(the='variables', go='here'))\n</code></pre>"}, {"location": "python_jinja2/#template-guidelines", "title": "Template guidelines", "text": ""}, {"location": "python_jinja2/#variables", "title": "Variables", "text": "<p>Reference variables using <code>{{ braces }}</code> notation.</p>"}, {"location": "python_jinja2/#iterationloops", "title": "Iteration/Loops", "text": "<p>One in each line:</p> <pre><code>{% for host in groups['tag_Function_logdb'] %}\nelasticsearch_discovery_zen_ping_unicast_hosts = {{ host }}:9300\n{% endfor %}\n</code></pre> <p>Inline:</p> <pre><code>ALLOWED_HOSTS = [{% for domain in domains %}\" {{ domain }}\",{% endfor %}]\n</code></pre>"}, {"location": "python_jinja2/#get-the-counter-of-the-iteration", "title": "Get the counter of the iteration", "text": "<pre><code>&gt;&gt;&gt; from jinja2 import Template\n\n&gt;&gt;&gt; s = \"{% for element in elements %}{{loop.index}} {% endfor %}\"\n&gt;&gt;&gt; Template(s).render(elements=[\"a\", \"b\", \"c\", \"d\"])\n1 2 3 4\n</code></pre>"}, {"location": "python_jinja2/#lookup", "title": "Lookup", "text": ""}, {"location": "python_jinja2/#get-environmental-variable", "title": "Get environmental variable", "text": "<pre><code>lookup('env','HOME')\n</code></pre>"}, {"location": "python_jinja2/#join", "title": "Join", "text": "<pre><code>lineinfile: dest=/etc/hosts line=\"{{ item.ip }} {{ item.aliases|join(' ') }}\"\n</code></pre>"}, {"location": "python_jinja2/#map", "title": "Map", "text": "<p>Get elements of a dictionary</p> <pre><code>  set_fact: asg_instances=\"{{ instances.results | map(attribute='instances') | map('first') | map(attribute='public_ip_address') | list}}\"\n</code></pre>"}, {"location": "python_jinja2/#default", "title": "Default", "text": "<p>Set default value of variable</p> <pre><code>name: 'lol'\nnew_name: \"{{ name | default('trol') }}\"\n</code></pre>"}, {"location": "python_jinja2/#rejectattr", "title": "Rejectattr", "text": "<p>Exclude elements from a list.</p> <p>Filters a sequence of objects by applying a test to the specified attribute of each object, and rejecting the objects with the test succeeding.</p> <p>If no test is specified, the attribute's value will be evaluated as a boolean.</p> <pre><code>{{ users|rejectattr(\"is_active\") }}\n{{ users|rejectattr(\"email\", \"none\") }}\n</code></pre>"}, {"location": "python_jinja2/#regex", "title": "Regex", "text": "<pre><code>{{ 'ansible' | regex_replace('^a.*i(.*)$', 'a\\\\1') }}\n{{ 'foobar' | regex_replace('^f.*o(.*)$', '\\\\1') }}\n{{ 'localhost:80' | regex_replace('^(?P&lt;host&gt;.+):(?P&lt;port&gt;\\\\d+)$', '\\\\g&lt;host&gt;, \\\\g&lt;port&gt;') }}\n</code></pre>"}, {"location": "python_jinja2/#slice-string", "title": "Slice string", "text": "<pre><code>{{ variable_name[:-8] }}\n</code></pre>"}, {"location": "python_jinja2/#conditional", "title": "Conditional", "text": ""}, {"location": "python_jinja2/#conditional-variable-definition", "title": "Conditional variable definition", "text": "<pre><code>{{ 'Update' if files else 'Continue' }}\n</code></pre>"}, {"location": "python_jinja2/#check-if-variable-is-defined", "title": "Check if variable is defined", "text": "<pre><code>{% if variable is defined %}\nVariable: {{ variable }} defined\n{% endif %}\n</code></pre>"}, {"location": "python_jinja2/#with-two-statements", "title": "With two statements:", "text": "<pre><code>{% if (backend_environment == 'backend' and environment == 'Dev'): %}\n\n{% elif ... %}\n{% else %}\n{% endif %}\n</code></pre>"}, {"location": "python_jinja2/#extract-extension-from-file", "title": "Extract extension from file", "text": "<pre><code>s3_object        : code/frontal/vodafone-v8.18.81.zip\nremote_clone_dir : \"{{deploy_dir}}/{{ s3_object | basename | splitext | first}}\"\n</code></pre>"}, {"location": "python_jinja2/#comments", "title": "Comments", "text": "<p><code>{# comment here #}</code></p>"}, {"location": "python_jinja2/#inheritance", "title": "Inheritance", "text": "<p>For simple inclusions use <code>include</code> for more complex <code>extend</code>.</p>"}, {"location": "python_jinja2/#include", "title": "Include", "text": "<p>To include a snippet from another file you can use <pre><code>{% include '_post.html' %}\n</code></pre></p>"}, {"location": "python_jinja2/#extend", "title": "Extend", "text": "<p>To inherit from another document you can use the <code>block</code> control statement. Blocks are given a unique name, which derived templates can reference when they provide their content</p> <p>.base.html <pre><code>&lt;html&gt;\n    &lt;head&gt;\n      {% if title %}\n      &lt;title&gt;{{ title }} - Microblog &lt;/title&gt;\n      {% else %}\n      &lt;title&gt;Welcome to Microblog&lt;/title&gt;\n      {% endif %}\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div&gt;\n          Microblog: &lt;a href=\"/index\"&gt;Home&lt;/a&gt;\n        &lt;/div&gt;\n        &lt;hr&gt;\n        {% block content %}{% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> .index.html <pre><code>{% extends \"base.html\" %}\n\n{% block content %}\n  &lt;h1&gt;Hi&lt;/h1&gt;\n{% endblock %}\n</code></pre></p>"}, {"location": "python_jinja2/#execute-a-function-and-return-the-value-to-a-variable", "title": "Execute a function and return the value to a variable", "text": "<pre><code>{% with messages = get_flashed_messages() %}\n{% if messages %}\n&lt;ul&gt;\n  {% for message in messages %}\n  &lt;li&gt;{{ message }}&lt;/li&gt;\n  {% endfor %}\n&lt;/ul&gt;\n{% endif %}\n{% endwith %}\n</code></pre>"}, {"location": "python_jinja2/#macros", "title": "Macros", "text": "<p>Macros are comparable with functions in regular programming languages. They are useful to put often used idioms into reusable functions to not repeat yourself (\u201cDRY\u201d).</p> <pre><code>{% macro input(name, value='', type='text', size=20) -%}\n    &lt;input type=\"{{ type }}\" name=\"{{ name }}\" value=\"{{\n        value|e }}\" size=\"{{ size }}\"&gt;\n{%- endmacro %}\n</code></pre> <p>The macro can then be called like a function in the namespace:</p> <pre><code>&lt;p&gt;{{ input('username') }}&lt;/p&gt;\n&lt;p&gt;{{ input('password', type='password') }}&lt;/p&gt;\n</code></pre>"}, {"location": "python_jinja2/#wrap-long-lines-and-indent", "title": "Wrap long lines and indent", "text": "<p>You can prepend the given string with a newline character, then use the <code>wordwrap</code> filter to wrap the text into multiple lines first, and use the replace filter to replace newline characters with newline plus '    ':</p> <pre><code>{{ ('\\n' ~ item.comment) | wordwrap(76) | replace('\\n', '\\n    ') }}\n</code></pre> <p>The above assumes you want each line to be no more than 80 characters. Change 76 to your desired line width minus 4 to leave room for the indentation.</p>"}, {"location": "python_jinja2/#test-if-variable-is-none", "title": "Test if variable is None", "text": "<p>Use the <code>none</code> test (not to be confused with Python's <code>None</code> object!):</p> <pre><code>{% if p is not none %}\n    {{ p.User['first_name'] }}\n{% else %}\n    NONE\n{% endif %}\n</code></pre>"}, {"location": "python_jinja2/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "python_logging/", "title": "Python Logging", "text": "<p>Logging information in your Python programs makes it possible to debug problems when running.</p> <p>For command line application that the user is going to run directly, the <code>logging</code> module might be enough. For command line tools or APIs that are going to be run by a server, it might fall short. <code>logging</code> will write exceptions and breadcrumbs to a file, and unless you look at it directly most errors will pass unnoticed.</p> <p>To actively monitor and react to code exceptions use an application monitoring platform like sentry. They gather de data on your application and aggregate the errors in a user friendly way, such as:</p> <ul> <li>Showing the context of the error in a web interface.</li> <li>Gather both front and backend issues in one place.</li> <li>Show the trail of events that led to the exceptions with breadcrumbs.</li> <li>Show the probable commit that introduced the bug.</li> <li>Link problems with issue tracker issues.</li> <li>See the impact of each bug with the number of occurrences and users that are     experiencing it.</li> <li>Visualize all the data in dashboards.</li> <li>Get notifications on the issues raised.</li> </ul> <p>Check the demo to see its features.</p> <p>You can self-host sentry, but it uses a docker-compose that depends on 12 services, including postgres, redis and kafka with a minimum requirements of 4 cores and 8 GB of RAM.</p> <p>So I've looked for a simple solution, and arrived to GlitchTip, a similar solution that even uses the sentry SDK, but has a smaller system footprint, and it's open sourced, while sentry is not anymore. Check it's documentation and source code.</p>"}, {"location": "python_mysql/", "title": "MySQL Python", "text": ""}, {"location": "python_mysql/#installation", "title": "Installation", "text": "<pre><code>pip install mysql-connector-python\n</code></pre>"}, {"location": "python_mysql/#usage", "title": "Usage", "text": "<pre><code>import mysql.connector\n\n# Connect to server\ncnx = mysql.connector.connect(\n    host=\"127.0.0.1\",\n    port=3306,\n    user=\"mike\",\n    password=\"s3cre3t!\")\n\n# Get a cursor\ncur = cnx.cursor()\n\n# Execute a query\ncur.execute(\"SELECT CURDATE()\")\n\n# Fetch one result\nrow = cur.fetchone()\nprint(\"Current date is: {0}\".format(row[0]))\n\n# Close connection\ncnx.close()\n</code></pre>"}, {"location": "python_mysql/#iterate-over-the-results-of-the-cursor-execution", "title": "Iterate over the results of the cursor execution", "text": "<pre><code>cursor.execute(show_db_query)\nfor db in cursor:\n    print(db)\n</code></pre>"}, {"location": "python_mysql/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>RealPython tutorial</li> </ul>"}, {"location": "python_optimization/", "title": "Python Optimization", "text": "<p>Optimization can be done through different metrics, such as, CPU performance (execution time) or memory footprint.</p> <p>Optimizing your code makes sense when you are sure that the business logic in the code is correct and not going to change soon.</p> <p>\"First make it work. Then make it right. Then make it fast.\" ~ Kent Beck</p> <p>Unless you're developing a performance-intensive product or a code dependency that is going to be used by other projects which might be performance-intensive, optimizing every aspect of the code can be overkill. For most of the scenarios, the 80-20 principle (80 percent of performance benefits may come from optimizing 20 percent of your code) will be more appropriate.</p> <p>Most of the time we make intuitive guesses on what the bottlenecks are, but more often than not, our guesses are either wrong or just approximately correct. So, it's always advisable to use profiling tools to identify how often a resource is used and who is using the resource. For instance, a profiler designed for profiling execution time will measure how often and for how various long parts of the code are executed. Using a profiling mechanism becomes a necessity when the codebase grows large, and you still want to maintain efficiency.</p>"}, {"location": "python_optimization/#making-python-command-line-fast", "title": "Making Python command line fast", "text": "<p>People like using software that feels fast, and Python programs tend to be slow to start running. What qualifies as fast is subjective, and varies by the type of tool and by the user's expectations.</p> <p>Roughly speaking, for a command line program, people expect results almost instantaneously. For a tool that appears to be doing a simple task a sub-second result is enough, but under 200ms is even better.</p> <p>Obviously to achieve this, your program actually has to be fast at doing its work. But what if you've written your code in Python, and it can take 800ms just to import your code, let alone start running it.</p>"}, {"location": "python_optimization/#how-fast-can-a-python-program-be", "title": "How fast can a Python program be?", "text": "<p>TBC with the next sources</p> <ul> <li>https://files.bemusement.org/talks/OSDC2008-FastPython/</li> <li>https://files.bemusement.org/talks/OSDC2008-FastPython/</li> <li>https://stackoverflow.com/questions/4177735/best-practice-for-lazy-loading-python-modules</li> <li>https://snarky.ca/lazy-importing-in-python-3-7/</li> <li>https://levelup.gitconnected.com/python-trick-lazy-module-loading-df9b9dc111af</li> </ul>"}, {"location": "python_optimization/#minimize-the-relative-import-statements-on-command-line-tools", "title": "Minimize the relative import statements on command line tools", "text": "<p>When developing a library, it's common to expose the main objects into the package <code>__init__.py</code> under the variable <code>__all__</code>. The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable.</p> <p>Following this string, if you manage to minimize the relative imports, you'll make your code faster.</p> <p>Python's wiki discusses different places to locate your import statements. If you put them on the top, the imports that you don't need for that command in particular will worsen your load time, if you add them inside the functions, if you run the function more than once, the performance drops too, and it's a common etiquete to have all your imports on the top.</p> <p>One step that you can do is to mark the imports required for type checking under a conditional:</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n   from model import Object\n</code></pre> <p>This change can be negligible, and it will force you to use <code>'Object'</code>, instead of <code>Object</code> in the typing information, which is not nice, so it may not be worth it.</p> <p>If you are still unable to make the loading time drop below an acceptable time, you can migrate to a server-client architecture, where all the logic is loaded by the backend (once as it's always running), and have a \"silly\" client that only does requests to the backend. Beware though, as you will add the network latency.</p>"}, {"location": "python_optimization/#dont-dynamically-install-the-package", "title": "Don't dynamically install the package", "text": "<p>If you install the package with <code>pip install -e .</code> you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the <code>-e</code> flag.</p>"}, {"location": "python_optimization/#references", "title": "References", "text": "<ul> <li>Satwik Kansal article on Scout APM</li> </ul>"}, {"location": "python_package_management/", "title": "Python Package Management", "text": "<p>Managing Python libraries is a nightmare for most developers, it has driven me crazy trying to keep all the requirements of the projects I maintain updated.</p> <p>I tried with pip-tools, but I was probably using it wrong. As package management has evolved a lot in the latest years, I'm going to compare Poetry, pipenv, <code>pdm</code> with my current workflow.</p> Tool Stars Forks Latest commit Commits Issues Open/New/Closed PR Open/New/Merged Poetry 17.3k 1.4k 11h 1992 1.1k/58/80 149/13/77 Pipenv 22.5k 1.7k 5d 7226 555/12/54 32/0/22 pdm 1.3k 54 11h 1539 12/3/43 3/2/11 <p>The <code>New</code> and <code>Closed</code> are taken from the Pulse insights of the last month. This data was taken on the 2021-11-30 so it will probably be outdated.</p> <p>Both Poetry and Pipenv are very popular, it looks that <code>Poetry</code> is more alive this last month, but they are both actively developed. <code>pdm</code> is actively developed but at other level.</p> <p>Pipenv has broad support. It is an official project of the Python Packaging Authority, alongside pip. It's also supported by the Heroku Python buildpack, which is useful for anyone with Heroku or Dokku-based deployment strategies.</p> <p>Poetry is a one-stop shop for dependency management and package management. It simplifies creating a package, managing its dependencies, and publishing it. Compared to Pipenv, Poetry's separate add and install commands are more explicit, and it's faster for everything except for a full dependency install.</p>"}, {"location": "python_package_management/#solver", "title": "Solver", "text": "<p>A Solver tries to find a working set of dependencies that all agree with each other. By looking back in time, it\u2019s happy to solve very old versions of packages if newer ones are supposed to be incompatible. This can be helpful, but is slow, and also means you can easily get a very ancient set of packages when you thought you were getting the latest versions.</p> <p>Pip\u2019s solver changed in version 20.3 to become significantly smarter. The old solver would ignore incompatible transitive requirements much more often than the new solver does. This means that an upper cap in a library might have been ignored before, but is much more likely to break things or change the solve now.</p> <p>Poetry has a unique and very strict (and slower) solver that goes even farther hunting for solutions. It forces you to cap Python if a dependency does. One key difference is that Poetry has the original environment specification to work with every time, while pip does not know what the original environment constraints were. This enables Poetry to roll back a dependency on a subsequent solve, while pip does not know what the original requirements were and so does not know if an older package is valid when it encounters a new cap.</p>"}, {"location": "python_package_management/#poetry", "title": "Poetry", "text": "<p>Features I like:</p> <ul> <li>Stores program and development requirements in the <code>pyproject.toml</code>     file.</li> <li>Don't need to manually edit requirements files to add new packages to the     program or dev requirements, simply use <code>poetry add</code>.</li> <li>Easy initialization of the development environment with <code>poetry install</code>.</li> <li> <p>Powerful dependency specification</p> <ul> <li>Installable packages with git dependencies???</li> <li>Easy to specify local directory dependencies, even in editable mode.</li> <li>Specify different dependencies for different python versions</li> </ul> </li> <li> <p>It manage the building of your package, you don't need to manually configure     <code>sdist</code> and <code>wheel</code>.</p> </li> <li>Nice dependency view with <code>poetry show</code>.</li> <li>Nice dependency search interface with <code>poetry search</code>.</li> <li>Sync your environment packages with the lock file.</li> </ul> <p>Things I don't like that much:</p> <ul> <li> <p>It does upper version capping by default, it even ignores your pins and adds     the <code>^&lt;new_version</code> pin if you run <code>poetry add     &lt;package&gt;@latest</code>https://github.com/python-poetry/poetry/issues/3503.     Given that upper version capping is becoming a big problem in     the Python environment I'd stay away from <code>poetry</code>.</p> <p>This is specially useless when you add dependencies that follow CalVer. <code>poetry add</code> packaging will still do <code>^21</code> for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer.</p> <p>It's equally troublesome that it upper pins the python version.</p> </li> <li> <p>Have their own dependency specification format similar to <code>npm</code> and     incompatible with Python's     PEP508.</p> </li> <li> <p>No automatic process to update the dependencies constrains to match the latest     version available.  So if you have constrained a package to be <code>&lt;2.0.0</code> and     <code>3.0.0</code> is out there, you will have to manually edit the <code>pyproject.toml</code> so     that it accepts that new version. At least you can use <code>poetry show     --outdated</code> and it will tell you which is the new version, and if the output     is zero, you're sure you're on the last versions.</p> </li> </ul>"}, {"location": "python_package_management/#pdm", "title": "PDM", "text": "<p>Features I like:</p> <ul> <li>The pin strategy defaults to only add lower     pins helping preventing the upper     capping problem.</li> <li>It can't achieve dependency isolation without virtualenvs.</li> <li>Follows the Python's dependency specification format     PEP508.</li> <li>Supports different strategies to add and update dependencies.</li> <li>Command to update your requirements constrains when updating your packages.</li> <li>Sync your environment packages with the lock file.</li> <li>Easy to install package in editable mode.</li> <li>Easy to install local dependencies.</li> <li>You can force the installation of a package at your own risk even if it breaks     the version constrains. (Useful if you're blocked by a third party upper     bound)</li> <li>Changing the python version is as simple as running <code>python use     &lt;python_version&gt;</code>.</li> <li>Plugin system where adding functionality is feasible (like the <code>publish</code>     subcommand).</li> <li>Both global and local configuration.</li> <li>Nice interface to change the configuration.</li> <li>Automatic management of dependencies cache, where you only have one instance     of each package version, and if no project needs it, it will be removed.</li> <li>Has a nice interface to see the cache usage</li> <li>Has the possibility of managing the global packages too.</li> <li>Allows the definition of scripts possibly removing the need of a makefile</li> <li>It's able to read the version of the program from a file, avoiding the     duplication of the information.</li> <li>You can group your development dependencies in groups.</li> <li>Easy to define extra dependencies for your program.</li> <li>It has sensible defaults for <code>includes</code> and <code>excludes</code> when packaging.</li> <li>It's the fastest      and most     correct     one.</li> </ul> <p>Downsides:</p> <ul> <li>They don't say how to configure your environment to work with     vim.</li> </ul>"}, {"location": "python_package_management/#summary", "title": "Summary", "text": "<p>PDM offers the same features as Poetry with the additions of the possibility of selecting your version capping strategy, and doesn\u2019t cap as badly, and follows more PEP standards.</p>"}, {"location": "python_package_management/#references", "title": "References", "text": "<ul> <li>PDM developer comparison</li> <li>John Franey comparison</li> <li>Frost Ming comparison (developer of PDM)</li> <li>Henry Schreiner analysis on Poetry</li> </ul>"}, {"location": "python_plugin_system/", "title": "Python plugin system", "text": "<p>When building Python applications, it's good to develop the core of your program, and allow extension via plugins.</p> <p>I still don't know how to do it, but I'm going to gather interesting references until I tackle it.</p> <ul> <li>Beets plugin system     looks awesome.</li> </ul>"}, {"location": "python_poetry/", "title": "Poetry", "text": "<p>Poetry is a command line program that helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere.</p> <p><code>poetry</code> saves all the information in the <code>pyproject.toml</code> file, including the project development and program dependencies, for example:</p> <pre><code>[tool.poetry]\nname = \"poetry-demo\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"S\u00e9bastien Eustace &lt;sebastien@eustace.io&gt;\"]\n\n[tool.poetry.dependencies]\npython = \"*\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^3.4\"\n</code></pre>"}, {"location": "python_poetry/#installation", "title": "Installation", "text": "<p>Although the official docs tell you to run:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p><code>pip install poetry</code> works too, which looks safer than executing arbitrary code from an url.</p> <p>To enable shell completion for <code>zsh</code> run:</p> <pre><code># Zsh\npoetry completions zsh &gt; ~/.zfunc/_poetry\n\n# Oh-My-Zsh\nmkdir $ZSH_CUSTOM/plugins/poetry\npoetry completions zsh &gt; $ZSH_CUSTOM/plugins/poetry/_poetry\n</code></pre> <p>For <code>zsh</code>, you must then add the following line in your <code>~/.zshrc</code> before <code>compinit</code>:</p> <pre><code>fpath+=~/.zfunc\n</code></pre> <p>For <code>oh-my-zsh</code>, you must then enable poetry in your <code>~/.zshrc</code> plugins:</p> <pre><code>plugins(\n    poetry\n    ...\n    )\n</code></pre>"}, {"location": "python_poetry/#basic-usage", "title": "Basic Usage", "text": ""}, {"location": "python_poetry/#initializing-a-pre-existing-project", "title": "Initializing a pre-existing project", "text": "<p>Instead of creating a new project, Poetry can be used to \u2018initialise\u2019 a pre-populated directory with <code>poetry init</code>. You can use the next options</p> <ul> <li><code>--name</code>: Name of the package.</li> <li><code>--description</code>: Description of the package.</li> <li><code>--author</code>: Author of the package.</li> <li><code>--python</code>: Compatible Python versions.</li> <li><code>--dependency</code>: Package to require with a version constraint. Should be in     format <code>foo:1.0.0</code>.</li> <li><code>--dev-dependency</code>: Development requirements, see <code>--require</code>.</li> </ul>"}, {"location": "python_poetry/#installing-dependencies", "title": "Installing dependencies", "text": "<p>To install the defined dependencies for your project, just run the install command.</p> <pre><code>poetry install\n</code></pre> <p>When you run this command, one of two things may happen:</p> <ul> <li> <p>Installing without poetry.lock: If you have never run the command before and     there is also no <code>poetry.lock</code> file present, Poetry simply resolves all     dependencies listed in your <code>pyproject.toml</code> file and downloads the latest     version of their files.</p> <p>When Poetry has finished installing, it writes all of the packages and the exact versions of them that it downloaded to the <code>poetry.lock</code> file, locking the project to those specific versions. You should commit the <code>poetry.lock</code> file to your project repo so that all people working on the project are locked to the same versions of dependencies.</p> </li> <li> <p>Installing with poetry.lock: If there is already a <code>poetry.lock</code> file as     well as a <code>pyproject.toml</code>, <code>poetry</code> resolves and installs all dependencies     that you listed in <code>pyproject.toml</code>, but Poetry uses the exact versions listed     in <code>poetry.lock</code> to ensure that the package versions are consistent for     everyone working on your project. As a result you will have all dependencies     requested by your <code>pyproject.toml</code> file, but they may not all be at the very     latest available versions (some of the dependencies listed in the     <code>poetry.lock</code> file may have released newer versions since the file was     created). This is by design, it ensures that your project does not break     because of unexpected changes in dependencies.</p> </li> </ul> <p>The current project is installed in editable mode by default.</p> <p>If you don't want the development requirements use the <code>--no-dev</code> flag.</p> <p>To remove the untracked dependencies that are no longer in the lock file, use <code>--remove-untracked</code>.</p>"}, {"location": "python_poetry/#updating-dependencies-to-their-latest-versions", "title": "Updating dependencies to their latest versions", "text": "<p>The <code>poetry.lock</code> file prevents you from automatically getting the latest versions of your dependencies. To update to the latest versions, use the <code>update</code> command. This will fetch the latest matching versions (according to your <code>pyproject.toml</code> file) and update the lock file with the new versions. (This is equivalent to deleting the <code>poetry.lock</code> file and running <code>install</code> again.)</p> <p>The main problem is that <code>poetry add</code> does upper pinning of dependencies by default, which is a really bad idea. And they don't plan to change.</p> <p>There is currently no way of updating your <code>pyproject.toml</code> dependency definitions so they match the latest version beyond your constrains. So if you have constrained a package to be <code>&lt;2.0.0</code> and <code>3.0.0</code> is out there, you will have to manually edit the <code>pyproject.toml</code> so that it accepts that new version.  There is no automatic process that does this. At least you can use <code>poetry show --outdated</code> and it will tell you which is the new version, and if the output is zero, you're sure you're on the last versions.</p> <p>Some workarounds exists though, if you run <code>poetry add dependency@latest</code> it will update the lock to the latest. MousaZeidBaker made poetryup, a tool that is able to update the requirements to the latest version with <code>poetryup --latest</code> (although it still has some bugs). Given that it uses <code>poetry add &lt;package&gt;@latest</code> behind the scenes, it will change your version pin to  <code>^&lt;new_version&gt;</code>, which  as we've seen it's awful.</p> <p>Again, you should not be trying to do this, it's better to improve how you manage your dependencies.</p>"}, {"location": "python_poetry/#debugging-why-a-package-is-not-updated-to-the-latest-version", "title": "Debugging why a package is not updated to the latest version", "text": "<p>Sometimes packages are not updated with <code>poetry update</code> or <code>poetryup</code>, to debug why, you need to understand if some package is setting a constrain that prevents the upgrade. To do that, first check the outdated packages with <code>poetry show -o</code> and for each of them:</p> <ul> <li>Check what packages are using the     dependency.</li> <li>Search if there is an issue asking the maintainers to update their     dependencies, if it doesn't exist, create it.</li> </ul>"}, {"location": "python_poetry/#removing-a-dependency", "title": "Removing a dependency", "text": "<pre><code>poetry remove pendulum\n</code></pre> <p>With the <code>-D</code> or <code>--dev</code> flag, it removes the dependency from the development ones.</p>"}, {"location": "python_poetry/#building-the-package", "title": "Building the package", "text": "<p>Before you can actually publish your library, you will need to package it.</p> <pre><code>poetry build\n</code></pre> <p>This command will package your library in two different formats: <code>sdist</code> which is the source format, and <code>wheel</code> which is a compiled package.</p> <p>Once that\u2019s done you are ready to publish your library.</p>"}, {"location": "python_poetry/#publishing-to-pypi", "title": "Publishing to PyPI", "text": "<p>Poetry will publish to PyPI by default. Anything that is published to PyPI is available automatically through Poetry.</p> <pre><code>poetry publish\n</code></pre> <p>This will package and publish the library to PyPI, at the condition that you are a registered user and you have configured your credentials properly.</p> <p>If you pass the <code>--build</code> flag, it will also build the package.</p>"}, {"location": "python_poetry/#publishing-to-a-private-repository", "title": "Publishing to a private repository", "text": "<p>Sometimes, you may want to keep your library private but also being accessible to your team. In this case, you will need to use a private repository.</p> <p>You will need to add it to your global list of repositories.</p> <p>Once this is done, you can actually publish to it like so:</p> <pre><code>poetry publish -r my-repository\n</code></pre>"}, {"location": "python_poetry/#specifying-dependencies", "title": "Specifying dependencies", "text": "<p>If you want to add dependencies to your project, you can specify them in the <code>tool.poetry.dependencies</code> section.</p> <pre><code>[tool.poetry.dependencies]\npendulum = \"^1.4\"\n</code></pre> <p>As you can see, it takes a mapping of package names and version constraints.</p> <p>Poetry uses this information to search for the right set of files in package \u201crepositories\u201d that you register in the <code>tool.poetry.repositories</code> section, or on PyPI by default.</p> <p>Also, instead of modifying the <code>pyproject.toml</code> file by hand, you can use the add command.</p> <pre><code>poetry add pendulum\n</code></pre> <p>It will automatically find a suitable version constraint and install the package and subdependencies.</p> <p>If you want to add the dependency to the development ones, use the <code>-D</code> or <code>--dev</code> flag.</p>"}, {"location": "python_poetry/#using-your-virtual-environment", "title": "Using your virtual environment", "text": "<p>By default, <code>poetry</code> creates a virtual environment in <code>{cache-dir}/virtualenvs</code>. You can change the <code>cache-dir</code> value by editing the <code>poetry</code> config. Additionally, you can use the <code>virtualenvs.in-project</code> configuration variable to create virtual environment within your project directory.</p> <p>There are several ways to run commands within this virtual environment.</p> <p>To run your script simply use <code>poetry run python your_script.py</code>. Likewise if you have command line tools such as <code>pytest</code> or <code>black</code> you can run them using <code>poetry run pytest</code>.</p> <p>The easiest way to activate the virtual environment is to create a new shell with <code>poetry shell</code>.</p>"}, {"location": "python_poetry/#version-management", "title": "Version Management", "text": "<p><code>poetry version</code> shows the current version of the project. If you pass an argument, it will bump the version of the package, for example <code>poetry version minor</code>. But it doesn't read your commits to decide what kind of bump you apply, so I'd keep on using <code>pip-compile</code>.</p>"}, {"location": "python_poetry/#dependency-specification", "title": "Dependency Specification", "text": "<p>Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.</p> <p>They don't follow Python's specification PEP508</p>"}, {"location": "python_poetry/#caret-requirements", "title": "Caret Requirements", "text": "<p>Caret requirements allow SemVer compatible updates to a specified version. An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. In this case, if we ran <code>poetry update requests</code>, <code>poetry</code> would update us to the next versions:</p> Requirement Versions allowed <code>^1.2.3</code> <code>&gt;=1.2.3 &lt;2.0.0</code> <code>^1.2</code> <code>&gt;=1.2.0 &lt;2.0.0</code> <code>^1</code> <code>&gt;=1.0.0 &lt;2.0.0</code> <code>^0.2.3</code> <code>&gt;=0.2.3 &lt;0.3.0</code> <code>^0.0.3</code> <code>&gt;=0.0.3 &lt;0.0.4</code> <code>^0.0</code> <code>&gt;=0.0.0 &lt;0.1.0</code> <code>^0</code> <code>&gt;=0.0.0 &lt;1.0.0</code>"}, {"location": "python_poetry/#tilde-requirements", "title": "Tilde requirements", "text": "<p>Tilde requirements specify a minimal version with some ability to update. If you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed. If you only specify a major version, then minor- and patch-level changes are allowed.</p> Requirement Versions allowed <code>~1.2.3</code> <code>&gt;=1.2.3 &lt;1.3.0</code> <code>~1.2</code> <code>&gt;=1.2.0 &lt;1.3.0</code> <code>~1</code> <code>&gt;=1.0.0 &lt;2.0.0</code>"}, {"location": "python_poetry/#wildcard-requirements", "title": "Wildcard requirements", "text": "<p>Wildcard requirements allow for the latest (dependency dependent) version where the wildcard is positioned.</p> Requirement Versions allowed <code>*</code> <code>&gt;=0.0.0</code> <code>1.*</code> <code>&gt;=1.0.0 &lt;2.0.0</code> <code>1.2.*</code> <code>&gt;=1.2.0 &lt;1.3.0</code>"}, {"location": "python_poetry/#inequality-requirements", "title": "Inequality requirements", "text": "<p>Inequality requirements allow manually specifying a version range or an exact version to depend on.</p> <p>Here are some examples of inequality requirements:</p> <pre><code>&gt;= 1.2.0\n&gt; 1\n&lt; 2\n!= 1.2.3\n</code></pre>"}, {"location": "python_poetry/#exact-requirements", "title": "Exact requirements", "text": "<p>You can specify the exact version of a package. This will tell Poetry to install this version and this version only. If other dependencies require a different version, the solver will ultimately fail and abort any install or update procedures.</p> <p>Multiple version requirements can also be separated with a comma, e.g. <code>&gt;= 1.2, &lt; 1.5</code>.</p>"}, {"location": "python_poetry/#git-dependencies", "title": "git dependencies", "text": "<p>To depend on a library located in a git repository, the minimum information you need to specify is the location of the repository with the git key:</p> <pre><code>[tool.poetry.dependencies]\nrequests = { git = \"https://github.com/requests/requests.git\" }\n</code></pre> <p>Since we haven\u2019t specified any other information, Poetry assumes that we intend to use the latest commit on the <code>master</code> branch to build our project.</p> <p>You can combine the git key with the branch key to use another branch. Alternatively, use <code>rev</code> or <code>tag</code> to pin a dependency to a specific commit hash or tagged ref, respectively. For example:</p> <pre><code>[tool.poetry.dependencies]\n# Get the latest revision on the branch named \"next\"\nrequests = { git = \"https://github.com/kennethreitz/requests.git\", branch = \"next\" }\n# Get a revision by its commit hash\nflask = { git = \"https://github.com/pallets/flask.git\", rev = \"38eb5d3b\" }\n# Get a revision by its tag\nnumpy = { git = \"https://github.com/numpy/numpy.git\", tag = \"v0.13.2\" }\n</code></pre> <p>When using <code>poetry add</code> you can add:</p> <ul> <li>A https cloned repo: <code>poetry add     git+https://github.com/sdispater/pendulum.git</code></li> <li>A ssh cloned repo: <code>poetry add     git+ssh://git@github.com/sdispater/pendulum.git</code></li> </ul> <p>If you need to checkout a specific branch, tag or revision, you can specify it when using add:</p> <pre><code>poetry add git+https://github.com/sdispater/pendulum.git#develop\npoetry add git+https://github.com/sdispater/pendulum.git#2.0.5\n</code></pre>"}, {"location": "python_poetry/#path-dependencies", "title": "path dependencies", "text": "<p>To depend on a library located in a local directory or file, you can use the path property:</p> <pre><code>[tool.poetry.dependencies]\n# directory\nmy-package = { path = \"../my-package/\", develop = false }\n\n# file\nmy-package = { path = \"../my-package/dist/my-package-0.1.0.tar.gz\" }\n</code></pre> <p>When using <code>poetry add</code>, you can point them directly to the package or the file:</p> <pre><code>poetry add ./my-package/\npoetry add ../my-package/dist/my-package-0.1.0.tar.gz\npoetry add ../my-package/dist/my_package-0.1.0.whl\n</code></pre> <p>If you want the dependency to be installed in editable mode you can specify it in the <code>pyproject.toml</code> file. It means that changes in the local directory will be reflected directly in environment.</p> <pre><code>[tool.poetry.dependencies]\nmy-package = {path = \"../my/path\", develop = true}\n</code></pre>"}, {"location": "python_poetry/#url-dependencies", "title": "url dependencies", "text": "<p>To depend on a library located on a remote archive, you can use the url property:</p> <pre><code>[tool.poetry.dependencies]\n# directory\nmy-package = { url = \"https://example.com/my-package-0.1.0.tar.gz\" }\n</code></pre> <p>With the corresponding add call:</p> <pre><code>poetry add https://example.com/my-package-0.1.0.tar.gz\n</code></pre>"}, {"location": "python_poetry/#python-restricted-dependencies", "title": "Python restricted dependencies", "text": "<p>You can also specify that a dependency should be installed only for specific Python versions:</p> <pre><code>[tool.poetry.dependencies]\npathlib2 = { version = \"^2.2\", python = \"~2.7\" }\n\n[tool.poetry.dependencies]\npathlib2 = { version = \"^2.2\", python = \"~2.7 || ^3.2\" }\n</code></pre>"}, {"location": "python_poetry/#multiple-constraints-dependencies", "title": "Multiple constraints dependencies", "text": "<p>Sometimes, one of your dependency may have different version ranges depending on the target Python versions.</p> <p>Let\u2019s say you have a dependency on the package <code>foo</code> which is only compatible with Python <code>&lt;3.0</code> up to version <code>1.9</code> and compatible with Python <code>3.4+</code> from version <code>2.0</code>. You would declare it like so:</p> <pre><code>[tool.poetry.dependencies]\nfoo = [\n    {version = \"&lt;=1.9\", python = \"^2.7\"},\n    {version = \"^2.0\", python = \"^3.4\"}\n]\n</code></pre>"}, {"location": "python_poetry/#show-the-available-packages", "title": "Show the available packages", "text": "<p>To list all of the available packages, you can use the show command.</p> <pre><code>poetry show\n</code></pre> <p>If you want to see the details of a certain package, you can pass the package name.</p> <pre><code>poetry show pendulum\n\nname        : pendulum\nversion     : 1.4.2\ndescription : Python datetimes made easy\n\ndependencies:\n - python-dateutil &gt;=2.6.1\n - tzlocal &gt;=1.4\n - pytzdata &gt;=2017.2.2\n</code></pre> <p>By default it will print all the dependencies, if you pass <code>--no-dev</code> it will only show your package's ones.</p> <p>With the <code>-l</code> or <code>--latest</code> it will show the latest version of the packages, and with <code>-o</code> or <code>--outdated</code> it will show the latest version but only for packages that are outdated.</p>"}, {"location": "python_poetry/#search-for-dependencies", "title": "Search for dependencies", "text": "<p>This command searches for packages on a remote index.</p> <pre><code>poetry search requests pendulum\n</code></pre>"}, {"location": "python_poetry/#export-requirements-to", "title": "[Export requirements to", "text": "<p>requirements.txt](https://python-poetry.org/docs/cli/#export)</p> <pre><code>poetry export -f requirements.txt --output requirements.txt\n</code></pre>"}, {"location": "python_poetry/#project-setup", "title": "Project setup", "text": "<p>If you don't already have a cookiecutter for your python projects, you can use <code>poetry new poetry-demo</code>, and it will create the <code>poetry-demo</code> directory with the following content:</p> <pre><code>poetry-demo\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.rst\n\u251c\u2500\u2500 poetry_demo\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_poetry_demo.py\n</code></pre> <p>If you want to use the <code>src</code> project structure, pass the <code>--src</code> flag.</p>"}, {"location": "python_poetry/#checking-what-package-is-using-a-dependency", "title": "Checking what package is using a dependency", "text": "<p>Even though <code>poetry</code> is supposed to show the information of which packages depend on a specific package with <code>poetry show package</code>, I don't see it.</p> <p>Luckily snejus made a small script that shows the information. Save it somewhere in your <code>PATH</code>.</p> <pre><code>_RED='\\\\\\\\e[1;31m&amp;\\\\\\\\e[0m'\n_GREEN='\\\\\\\\e[1;32m&amp;\\\\\\\\e[0m'\n_YELLOW='\\\\\\\\e[1;33m&amp;\\\\\\\\e[0m'\n_format () {\n    tr -d '\"' |\n        sed \"s/ \\+&gt;[^ ]* \\+&lt;.*/$_YELLOW/\" | # ~ / ^ / &lt; &gt;= ~ a window\n        sed \"s/ \\+&gt;[^ ]* *$/$_GREEN/\" |     # &gt;= no upper limit\n        sed \"/&gt;/ !s/&lt;.*$/$_RED/\" |          # &lt; ~ upper limit\n        sed \"/&gt;\\|&lt;/ !s/ .*/  $_RED/\"        # == ~ locked version\n}\n\n_requires () {\n    sed -n \"/^name = \\\"$1\\\"/I,/\\[\\[package\\]\\]/{\n                /\\[package.dep/,/^$/{\n                    /^[^[]/ {\n                        s/= {version = \\(\\\"[^\\\"]*\\\"\\).*/, \\1/p;\n                        s/ =/,/gp\n             }}}\" poetry.lock |\n        sed \"/,.*,/!s/&lt;/,&lt;/; s/^[^&lt;]\\+$/&amp;,/\" |\n        column -t -s , | _format\n}\n\n_required_by () {\n    sed -n \"/\\[metadata\\]/,//d;\n            /\\[package\\]\\|\\[package\\.depen/,/^$/H;\n            /^name\\|^$1 = /Ip\" poetry.lock |\n        sed -n \"/^$1/I{x;G;p};h\" |\n        sed 's/.*\"\\(.*\\)\".*/\\1/' |\n        sed '$!N;s/\\n/ /' |\n        column -t | _format\n}\n\ndeps() {\n    echo\n    echo -e \"\\e[1mREQUIRES\\e[0m\"\n    _requires \"$1\" | xargs -i echo -e \"\\t{}\"\n    echo\n    echo -e \"\\e[1mREQUIRED BY\\e[0m\"\n    _required_by \"$1\" | xargs -i echo -e \"\\t{}\"\n    echo\n}\n\ndeps $1\n</code></pre>"}, {"location": "python_poetry/#configuration", "title": "Configuration", "text": "<p>Poetry can be configured via the <code>config</code> command (see more about its usage here) or directly in the <code>config.toml</code> file that will be automatically be created when you first run that command. This file can typically be found in <code>~/.config/pypoetry</code>.</p> <p>Poetry also provides the ability to have settings that are specific to a project by passing the <code>--local</code> option to the config command.</p> <pre><code>poetry config virtualenvs.create false --local\n</code></pre>"}, {"location": "python_poetry/#list-the-current-configuration", "title": "List the current configuration", "text": "<p>To list the current configuration you can use the <code>--list</code> option of the <code>config</code> command:</p> <pre><code>poetry config --list\n</code></pre> <p>Which will give you something similar to this:</p> <pre><code>cache-dir = \"/path/to/cache/directory\"\nvirtualenvs.create = true\nvirtualenvs.in-project = null\nvirtualenvs.path = \"{cache-dir}/virtualenvs\"  # /path/to/cache/directory/virtualenvs\n</code></pre>"}, {"location": "python_poetry/#adding-or-updating-a-configuration-setting", "title": "Adding or updating a configuration setting", "text": "<p>To change or otherwise add a new configuration setting, you can pass a value after the setting\u2019s name:</p> <pre><code>poetry config virtualenvs.path /path/to/cache/directory/virtualenvs\n</code></pre> <p>For a full list of the supported settings see Available settings.</p>"}, {"location": "python_poetry/#removing-a-specific-setting", "title": "Removing a specific setting", "text": "<p>If you want to remove a previously set setting, you can use the <code>--unset</code> option:</p> <pre><code>poetry config virtualenvs.path --unset\n</code></pre>"}, {"location": "python_poetry/#adding-a-repository", "title": "Adding a repository", "text": "<p>Adding a new repository is easy with the <code>config</code> command.</p> <pre><code>poetry config repositories.foo https://foo.bar/simple/\n</code></pre> <p>This will set the url for repository <code>foo</code> to <code>https://foo.bar/simple/</code>.</p>"}, {"location": "python_poetry/#configuring-credentials", "title": "Configuring credentials", "text": "<p>If you want to store your credentials for a specific repository, you can do so easily:</p> <pre><code>poetry config http-basic.foo username password\n</code></pre> <p>If you do not specify the password you will be prompted to write it.</p> <p>To publish to PyPI, you can set your credentials for the repository named <code>pypi</code>.</p> <p>Note that it is recommended to use API tokens when uploading packages to PyPI. Once you have created a new token, you can tell Poetry to use it:</p> <pre><code>poetry config pypi-token.pypi my-token\n</code></pre> <p>If a system keyring is available and supported, the password is stored to and retrieved from the keyring. In the above example, the credential will be stored using the name <code>poetry-repository-pypi</code>. If access to keyring fails or is unsupported, this will fall back to writing the password to the <code>auth.toml</code> file along with the username.</p> <p>Keyring support is enabled using the keyring library. For more information on supported backends refer to the library documentation. It doesn't support pass by default, but Steffen Vogel created a specific keyring backend. Alternatively, you can use environment variables to provide the credentials:</p> <pre><code>export POETRY_PYPI_TOKEN_PYPI=my-token\nexport POETRY_HTTP_BASIC_PYPI_USERNAME=username\nexport POETRY_HTTP_BASIC_PYPI_PASSWORD=password\n</code></pre> <p>I've tried setting up the keyring but I get the next error:</p> <pre><code>  UploadError\n\n  HTTP Error 403: Invalid or non-existent authentication information. See https://pypi.org/help/#invalid-auth for more information.\n\n  at ~/.venvs/autodev/lib/python3.9/site-packages/poetry/publishing/uploader.py:216 in _upload\n      212\u2502                     self._register(session, url)\n      213\u2502                 except HTTPError as e:\n      214\u2502                     raise UploadError(e)\n      215\u2502\n    \u2192 216\u2502             raise UploadError(e)\n      217\u2502\n      218\u2502     def _do_upload(\n      219\u2502         self, session, url, dry_run=False\n      220\u2502     ):  # type: (requests.Session, str, Optional[bool]) -&gt; None\n</code></pre> <p>The keyring was configured with:</p> <pre><code>poetry config pypi-token.pypi internet/pypi.token\n</code></pre> <p>And I'm sure that the keyring works because <code>python -m keyring get internet pypi.token</code> works.</p> <p>I've also tried with the environmental variable <code>POETRY_PYPI_TOKEN_PYPI</code> but it didn't work either. And setting the configuration as <code>poetry config http-basic.pypi __token__ internet/pypi.token</code>.</p> <p>Finally I had to hardcode the token with <code>poetry config pypi-token.pypi \"$(pass show internet/pypi.token)</code>. Although I can't find where it's storing the value :S.</p>"}, {"location": "python_poetry/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "python_profiling/", "title": "Python Profiling", "text": "<p>Profiling is to find out where your code spends its time. Profilers can collect several types of information: timing, function calls, interruptions or cache faults. It can be useful to identify bottlenecks, which should be the first step when trying to optimize some code, or to study the evolution of the performance of your code.</p>"}, {"location": "python_profiling/#profiling-types", "title": "Profiling types", "text": "<p>There are two types of profiling:</p> Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead."}, {"location": "python_profiling/#profiling-tools", "title": "Profiling tools", "text": "<p>The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like <code>line_profiler</code>. In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument, or if you're debugging a running process, using py-spy.</p>"}, {"location": "python_profiling/#deterministic-profiling", "title": "Deterministic Profiling", "text": ""}, {"location": "python_profiling/#cprofile", "title": "cProfile", "text": "<p>Python comes with two built-in modules for deterministic profiling: cProfile and profile.  Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it.</p> <pre><code>$: python3 -m cProfile script.py\n\n58 function calls in 9.419 seconds\n\nOrdered by: standard namen\n\ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   1    0.000    0.000    9.419    9.419 part1.py:1(&lt;module&gt;)\n   51   9.419    0.185    9.419    0.185 part1.py:1(computation)\n   1    0.000    0.000    9.419    9.419 part1.py:10(function1)\n   1    0.000    0.000    9.243    9.243 part1.py:15(function2)\n   1    0.000    0.000    0.176    0.176 part1.py:20(function3)\n   1    0.000    0.000    9.419    9.419 part1.py:24(main)\n</code></pre> <p>Where:</p> ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. <p>We can also sort the functions by some criteria, for example <code>python3 -m cProfile -s tottime script.py</code>.</p>"}, {"location": "python_profiling/#statistical-profiling", "title": "Statistical profiling", "text": ""}, {"location": "python_profiling/#py-spy", "title": "Py-spy", "text": "<p>Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code.</p> <p>To install it, just type:</p> <pre><code>pip install py-spy\n</code></pre> <p>To test the performance of a file use:</p> <pre><code>py-spy top python3 script.py\n</code></pre> <p>To assess the performance of a runnin process, specify it's PID:</p> <pre><code>py-spy top --pid $PID\n</code></pre> <p>They will show a <code>top</code> like interface showing the following data:</p> <pre><code>GIL: 100.00%, Active: 100.00%, Threads: 1\n\n  %Own   %Total  OwnTime  TotalTime  Function (filename:line)\n 61.00%  61.00%   10.50s    10.50s   computation (script.py:7)\n 39.00%  39.00%    7.50s     7.50s   computation (script.py:6)\n  0.00% 100.00%   0.000s    18.00s   &lt;module&gt; (script.py:30)\n  0.00% 100.00%   0.000s    18.00s   function2 (script.py:18)\n  0.00% 100.00%   0.000s    18.00s   main (script.py:26)\n  0.00% 100.00%   0.000s    18.00s   function1 (script.py:12)\n</code></pre>"}, {"location": "python_profiling/#pyinstrument", "title": "pyinstrument", "text": "<p>It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms.</p> <p>Install it with:</p> <pre><code>pip install pyinstrument\n</code></pre> <p>Use:</p> <p>The advantages are that:</p> <ul> <li>The output is far more attractive.</li> <li>It has less overhead, so it distorts less the results.</li> <li>Doesn't show the internal calls that make cProfiling result reading difficult.</li> <li>It uses wall-clock time instead of CPU time. So it takes into account the IO     time.</li> </ul> <pre><code>$: pyinstrument script.py\n\n  _     ._   __/__   _ _  _  _ _/_   Recorded: 15:45:20  Samples:  51\n /_//_/// /_\\ / //_// / //_'/ //     Duration: 4.517     CPU time: 4.516\n/   _/                      v3.3.0\n\nProgram: script.py\n\n4.516 &lt;module&gt;  script.py:2\n\u2514\u2500 4.516 main  script.py:25\n   \u2514\u2500 4.516 function1  script.py:11\n      \u251c\u2500 4.425 function2  script.py:16\n      \u2502  \u2514\u2500 4.425 computation  script.py:2\n      \u2514\u2500 0.092 function3  script.py:21\n         \u2514\u2500 0.092 computation  script.py:2\n</code></pre> <p>With the possibility to generate an HTML report.</p> <p></p> <p>The disadvantages are that it's only easy to profile python script files, not full packages.</p> <p>You can also profile a chunk of code, which can be useful when developing or for writing performance tests.</p> <pre><code>from pyinstrument import Profiler\n\nprofiler = Profiler()\nprofiler.start()\n\n# code you want to profile\n\nprofiler.stop()\n\nprint(profiler.output_text(unicode=True, color=True))\n</code></pre> <p>To explore the profile in a web browser, use <code>profiler.open_in_browser()</code>. To save this HTML for later, use <code>profiler.output_html()</code>.</p>"}, {"location": "python_profiling/#introduce-profiling-in-your-test-workflow", "title": "Introduce profiling in your test workflow", "text": "<p>I run out of time, so here are the starting points:</p> <ul> <li>Niklas Meinzer     post</li> <li>Pypi page of pytest-benchmark,     Docs,     Git</li> <li>Docs of pytest-profiling</li> <li>uwpce guide on using     pstats</li> </ul> <p>The idea is to develop the following ideas:</p> <ul> <li>How to integrate profiling with pytest.</li> <li>How to compare benchmark results between CI runs.</li> <li>Some guidelines on writing performance tests</li> </ul> <p>And memray looks very promising.</p>"}, {"location": "python_profiling/#references", "title": "References", "text": "<ul> <li>Antonio Molner article on Python Profiling</li> </ul>"}, {"location": "python_properties/", "title": "Python Properties", "text": "<p>The <code>@property</code> is the pythonic way to use getters and setters in object-oriented programming. It can be used to make methods look like attributes.</p> <p>The <code>property</code> decorator returns an object that proxies any request to set or access the attribute value through the methods we have specified.</p> <pre><code>class Foo:\n    @property\n    def foo(self):\n        return 'bar'\n</code></pre> <p>We can specify a setter function for the new property</p> <pre><code>class Foo:\n    @property\n    def foo(self):\n        return self._foo\n\n    @foo.setter\n    def foo(self, value):\n        self._foo = value\n</code></pre> <p>We first decorate the <code>foo</code> method a as getter. Then we decorate a second method with exactly the same name by applying the <code>setter</code> attribute of the originally decorated <code>foo</code> method. The <code>property</code> function returns an object; this object always comes with its own <code>setter</code> attribute, which can then be applied as a decorator to other functions. Using the same name for the get and set methods is not required, but it does help group the multiple methods that access one property together.</p> <p>We can also specify a deletion function with <code>@foo.deleter</code>. We cannot specify a docstring using <code>property</code> decorators, so we need  to rely on the property copying the docstring from the initial getter method</p> <pre><code>class Silly:\n    @property\n    def silly(self):\n        \"This is a silly property\"\n        print(\"You are getting silly\")\n        return self._silly\n\n    @silly.setter\n    def silly(self, value):\n        print(\"You are making silly {}\".format(value))\n        self._silly = value\n\n    @silly.deleter\n    def silly(self):\n        print(\"Whoah, you kicked silly!\")\n        del self.silly\n</code></pre> <pre><code>&gt;&gt;&gt; s = Silly()\n&gt;&gt;&gt; s.silly = \"funny\"\nYou are making silly funny\n&gt;&gt;&gt; s.silly\nYou are getting silly\n'funny'\n&gt;&gt;&gt; del s.silly\nWhoah, you kicked silly!\n</code></pre>"}, {"location": "python_properties/#when-to-use-properties", "title": "When to use properties", "text": "<p>The most common use of a property is when we have some data on a class that we later want to add behavior to.</p> <p>The fact that methods are just callable attributes, and properties are just customizable attributes can help us make the decision. Methods should typically represent actions; things that can be done to, or performed by, the object. When you call a method, even with only one argument, it should do something. Method names a generally verbs.</p> <p>Once confirming that an attribute is not an action, we need to decide between standard data attributes and properties. In general, always use a standard attribute until you need to control access to that property in some way. In either case, your attribute is usually a noun . The only difference between an attribute and a property is that we can invoke custom actions automatically when a property is retrieved, set, or deleted</p>"}, {"location": "python_properties/#cache-expensive-data", "title": "Cache expensive data", "text": "<p>A common need for custom behavior is caching a value that is difficult to calculate or expensive to look up.</p> <p>We can do this with a custom getter on the property. The first time the value is retrieved, we perform the lookup or calculation. Then we could locally cache the value as a private attribute on our object, and the next time the value is requested, we return the stored data.</p> <pre><code>from urlib.request import urlopen\n\nclass Webpage:\n    def __init__(self, url):\n        self.url = url\n        self._content = None\n\n    @property\n    def content(self):\n        if not self._content:\n            print(\"Retrieving New Page..\")\n            self._content = urlopen(self.url).read()\n        return self._content\n</code></pre> <pre><code>&gt;&gt;&gt; import time\n&gt;&gt;&gt; webpage = Webpage(\"http://ccphillips.net/\")\n&gt;&gt;&gt; now = time.time()\n&gt;&gt;&gt; content1 = webpage.content\nRetrieving new Page...\n&gt;&gt;&gt; time.time() - now\n22.43316\n&gt;&gt;&gt; now = time.time()\n&gt;&gt;&gt; content2 = webpage.content\n&gt;&gt;&gt; time.time() -now\n1.926645\n&gt;&gt;&gt; content1 == content2\nTrue\n</code></pre>"}, {"location": "python_properties/#attributes-calculated-on-the-fly", "title": "Attributes calculated on the fly", "text": "<p>Custom getters are also useful for attributes that need to be calculated on the fly, based on other object attributes.</p> <p><pre><code>clsas AverageList(list):\n    @property\n    def average(self):\n        return sum(self) / len(self)\n</code></pre> <pre><code>&gt;&gt;&gt; a = AverageList([1,2,3,4])\n&gt;&gt;&gt; a.average\n2.5\n</code></pre></p> <p>Of course we could have made this a method instead, but then we should call it <code>calculate_average()</code>, since methods represent actions. But a property called <code>average</code> is more suitable, both easier to type, and easier to read.</p>"}, {"location": "python_properties/#abstract-properties", "title": "Abstract properties", "text": "<p>Sometimes you want to define properties in your abstract classes, to do that, use:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass C(ABC):\n    @property\n    @abstractmethod\n    def my_abstract_property(self):\n        ...\n</code></pre> <p>If you want to use an abstract setter, you'll encounter the mypy <code>Decorated property not supported</code> error, you'll need to add a <code># type: ignore</code> until this issue is solved.</p>"}, {"location": "python_sh/", "title": "SH", "text": "<p>sh is a full-fledged subprocess replacement so beautiful that makes you want to cry. It allows you to call any program as if it were a function:</p> <pre><code>from sh import ifconfig\nprint(ifconfig(\"wlan0\"))\n</code></pre> <p>Output:</p> <pre><code>wlan0   Link encap:Ethernet  HWaddr 00:00:00:00:00:00\n        inet addr:192.168.1.100  Bcast:192.168.1.255  Mask:255.255.255.0\n        inet6 addr: ffff::ffff:ffff:ffff:fff/64 Scope:Link\n        UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n        RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n        TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n        collisions:0 txqueuelen:1000\n        RX bytes:0 (0 GB)  TX bytes:0 (0 GB)\n</code></pre> <p>Note that these aren't Python functions, these are running the binary commands on your system by dynamically resolving your $PATH, much like Bash does, and then wrapping the binary in a function. In this way, all the programs on your system are available to you from within Python.</p>"}, {"location": "python_sh/#installation", "title": "Installation", "text": "<pre><code>pip install sh\n</code></pre>"}, {"location": "python_sh/#usage", "title": "Usage", "text": ""}, {"location": "python_sh/#passing-arguments", "title": "Passing arguments", "text": "<pre><code>sh.ls(\"-l\", \"/tmp\", color=\"never\")\n</code></pre> <p>If the command gives you a syntax error (like <code>pass</code>), you can use bash.</p> <pre><code>sh.bash(\"-c\", \"pass\")\n</code></pre>"}, {"location": "python_sh/#handling-exceptions", "title": "Handling exceptions", "text": "<p>Normal processes exit with exit code 0. You can access the program return code with <code>RunningCommand.exit_code</code>:</p> <pre><code>output = ls(\"/\")\nprint(output.exit_code) # should be 0\n</code></pre> <p>If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class <code>ErrorReturnCode</code>:</p> <pre><code>try:\n    print(ls(\"/some/non-existant/folder\"))\nexcept sh.ErrorReturnCode_2:\n    print(\"folder doesn't exist!\")\n    create_the_folder()\nexcept sh.ErrorReturnCode:\n    print(\"unknown error\")\n</code></pre> <p>The exception object is an sh command object, which has, between other , the <code>stderr</code> and <code>stdout</code> bytes attributes with the errors. To show them use:</p> <pre><code>except sh.ErrorReturnCode as error:\n    print(str(error.stderr, 'utf8'))\n</code></pre>"}, {"location": "python_sh/#redirecting-output", "title": "Redirecting output", "text": "<pre><code>sh.ifconfig(_out=\"/tmp/interfaces\")\n</code></pre>"}, {"location": "python_sh/#running-in-background", "title": "Running in background", "text": "<p>By default, each running command blocks until completion. If you have a long-running command, you can put it in the background with the <code>_bg=True</code> special kwarg:</p> <pre><code># blocks\nsleep(3)\nprint(\"...3 seconds later\")\n\n# doesn't block\np = sleep(3, _bg=True)\nprint(\"prints immediately!\")\np.wait()\nprint(\"...and 3 seconds later\")\n</code></pre> <p>You\u2019ll notice that you need to call <code>RunningCommand.wait()</code> in order to exit after your command exits.</p> <p>Commands launched in the background ignore <code>SIGHUP</code>, meaning that when their controlling process (the session leader, if there is a controlling terminal) exits, they will not be signalled by the kernel. But because <code>sh</code> commands launch their processes in their own sessions by default, meaning they are their own session leaders, ignoring <code>SIGHUP</code> will normally have no impact. So the only time ignoring <code>SIGHUP</code> will do anything is if you use <code>_new_session=False</code>, in which case the controlling process will probably be the shell from which you launched python, and exiting that shell would normally send a <code>SIGHUP</code> to all child processes.</p> <p>If you want to terminate the process use <code>p.kill()</code>.</p>"}, {"location": "python_sh/#output-callbacks", "title": "Output callbacks", "text": "<p>In combination with <code>_bg=True</code>, <code>sh</code> can use callbacks to process output incrementally by passing a callable function to <code>_out</code> and/or <code>_err</code>. This callable will be called for each line (or chunk) of data that your command outputs:</p> <pre><code>from sh import tail\n\ndef process_output(line):\n    print(line)\n\np = tail(\"-f\", \"/var/log/some_log_file.log\", _out=process_output, _bg=True)\np.wait()\n</code></pre> <p>To \u201cquit\u201d your callback, simply <code>return True</code>. This tells the command not to call your callback anymore. This does not kill the process though see Interactive callbacks for how to kill a process from a callback.</p> <p>The line or chunk received by the callback can either be of type str or bytes. If the output could be decoded using the provided encoding, a str will be passed to the callback, otherwise it would be raw bytes.</p>"}, {"location": "python_sh/#interactive-callbacks", "title": "Interactive callbacks", "text": "<p>Commands may communicate with the underlying process interactively through a specific callback signature. Each command launched through <code>sh</code> has an internal STDIN <code>queue.Queue</code> that can be used from callbacks:</p> <pre><code>    def interact(line, stdin):\n        if line == \"What... is the air-speed velocity of an unladen swallow?\":\n            stdin.put(\"What do you mean? An African or European swallow?\")\n\n        elif line == \"Huh? I... I don't know that....AAAAGHHHHHH\":\n            cross_bridge()\n            return True\n\n        else:\n            stdin.put(\"I don't know....AAGGHHHHH\")\n            return True\n\n    p = sh.bridgekeeper(_out=interact, _bg=True)\np.wait()\n</code></pre> <p>You can also kill or terminate your process (or send any signal, really) from your callback by adding a third argument to receive the process object:</p> <pre><code>def process_output(line, stdin, process):\n    print(line)\n    if \"ERROR\" in line:\n        process.kill()\n        return True\n\np = tail(\"-f\", \"/var/log/some_log_file.log\", _out=process_output, _bg=True)\n</code></pre> <p>The above code will run, printing lines from <code>some_log_file.log</code> until the word <code>ERROR</code> appears in a line, at which point the tail process will be killed and the script will end.</p>"}, {"location": "python_sh/#interacting-with-programs-that-ask-input-from-the-user", "title": "Interacting with programs that ask input from the user", "text": "<p>Note</p> <p>Check the interactive callbacks or this issue, as it looks like a cleaner solution.</p> <p><code>sh</code> allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it.</p> <p>Imagine we've got a python script that asks the user to enter a username so it can save it in a file.</p> <p>File: /tmp/script.py</p> <pre><code>answer = input(\"Enter username: \")\n\nwith open(\"/tmp/user.txt\", \"w+\") as f:\n    f.write(answer)\n</code></pre> <p>When we run it in the terminal we get prompted and answer with <code>lyz</code>:</p> <pre><code>$: /tmp/script.py\nEnter username: lyz\n\n$: cat /tmp/user.txt\nlyz\n</code></pre> <p>To achieve the same goal automatically with <code>sh</code> we'll need to use the function callbacks. They are functions we pass to the sh command through the <code>_out</code> argument.</p> <pre><code>import sys\nimport re\n\naggregated = \"\"\n\ndef interact(char, stdin):\n    global aggregated\n    sys.stdout.write(char.encode())\n    sys.stdout.flush()\n    aggregated += char\n    if re.search(r\"Enter username: \", aggregated, re.MULTILINE):\n        stdin.put(\"lyz\\n\")\n\nsh.bash(\n    \"-c\",\n    \"/tmp/script.py\",\n    _out=interact,\n    _out_bufsize=0\n)\n</code></pre> <p>In the example above we've created an <code>interact</code> function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument <code>_out_bufsize=0</code>. Check the ssh password example to see why we need that.</p> <p>As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global <code>aggregated</code> variable. Once the regular expression matches what we want, sh will inject the desired value.</p> <p>Remember to add the <code>\\n</code> at the end of the string you want to inject.</p> <p>If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts.</p>"}, {"location": "python_sh/#testing", "title": "Testing", "text": "<p><code>sh</code> can be patched in your tests the typical way, with <code>unittest.mock.patch()</code>:</p> <pre><code>from unittest.mock import patch\nimport sh\n\ndef get_something():\n    return sh.pwd()\n\n@patch(\"sh.pwd\", create=True)\ndef test_something(pwd):\n    pwd.return_value = \"/\"\n    assert get_something() == \"/\"\n</code></pre> <p>The important thing to note here is that <code>create=True</code> is set. This is required because <code>sh</code> is a bit magical and patch will fail to find the <code>pwd</code> command as an attribute on the <code>sh</code> module.</p> <p>You may also patch the <code>Command</code> class:</p> <pre><code>from unittest.mock import patch\nimport sh\n\ndef get_something():\n    pwd = sh.Command(\"pwd\")\n    return pwd()\n\n@patch(\"sh.Command\")\ndef test_something(Command):\n    Command().return_value = \"/\"\n    assert get_something() == \"/\"\n</code></pre> <p>Notice here we do not need <code>create=True</code>, because <code>Command</code> is not an automatically generated object on the <code>sh</code> module (it actually exists).</p>"}, {"location": "python_sh/#tips", "title": "Tips", "text": ""}, {"location": "python_sh/#avoid-exception-logging-when-killing-a-background-process", "title": "Avoid exception logging when killing a background process", "text": "<p>In order to catch this exception execute your process with <code>_bg_exec=False</code> and execute <code>p.wait()</code> if you want to handle the exception. Otherwise don't use the <code>p.wait()</code>.</p> <pre><code>p = sh.sleep(100, _bg=True, _bg_exc=False)\ntry:\n    p.kill()\n    p.wait()\nexcept sh.SignalException_SIGKILL as err:\n    print(\"foo\")\n\nfoo\n</code></pre>"}, {"location": "python_sh/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "python_vlc/", "title": "Python VLC", "text": "<p>Python VLC is a library to control <code>vlc</code> from python.</p> <p>There is not usable online documentation, you'll have to go through the <code>help(&lt;component&gt;)</code> inside the python console.</p>"}, {"location": "python_vlc/#installation", "title": "Installation", "text": "<pre><code>pip install python-vlc\n</code></pre>"}, {"location": "python_vlc/#usage", "title": "Usage", "text": ""}, {"location": "python_vlc/#basic-usage", "title": "Basic usage", "text": "<p>You can create an instance of the <code>vlc</code> player with:</p> <pre><code>import vlc\n\nplayer = vlc.MediaPlayer('path/to/file.mp4')\n</code></pre> <p>The <code>player</code> has the next interesting methods:</p> <ul> <li><code>play()</code>: Opens the program and starts playing, if you've used <code>pause</code> it     resumes playing.</li> <li><code>pause()</code>: Pauses the video</li> <li><code>stop()</code>: Closes the player.</li> <li><code>set_fulscreen(1)</code>: Sets to fullscreen if you pass <code>0</code> as argument it returns     from fullscreen.</li> <li><code>set_media(vlc.Media('path/to/another/file.mp4'))</code>: Change the reproduced     file. It can even play pictures!</li> </ul> <p>If you want more control, it's better to use an <code>vlc.Instance</code> object to work with.</p>"}, {"location": "python_vlc/#configure-the-instance", "title": "Configure the instance", "text": "<pre><code>instance = Instance('--loop')\n</code></pre>"}, {"location": "python_vlc/#reproduce-many-files", "title": "Reproduce many files", "text": "<p>First you need to create a media list:</p> <pre><code>media_list = instance.media_list_new()\npath = \"/path/to/directory\"\nfiles = os.listdir(path)\nfor file_ in files:\n    media_list.add_media(instance.media_new(os.path.join(path,s)))\n</code></pre> <p>Then create the player:</p> <pre><code>media_player = instance.media_list_player_new()\nmedia_player.set_media_list(media_list)\n</code></pre> <p>Now you can use <code>player.next()</code> and <code>player.previous()</code>.</p>"}, {"location": "python_vlc/#set-playback-mode", "title": "Set playback mode", "text": "<pre><code>media_player.set_playback_mode(vlc.PlaybackMode.loop)\n</code></pre> <p>There are the next playback modes:</p> <ul> <li><code>default</code></li> <li><code>loop</code></li> <li><code>repeat</code></li> </ul>"}, {"location": "python_vlc/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Source</li> </ul>"}, {"location": "pythonping/", "title": "pythonping", "text": "<p>pythonping is simple way to ping in Python. With it, you can send ICMP Probes to remote devices like you would do from the terminal.</p> <p>Warning: Since using <code>pythonping</code> requires root permissions or granting <code>cap_net_raw</code> capability to the python interpreter, try to measure the latency to a server by other means such as using <code>requests</code>.</p>"}, {"location": "pythonping/#installation", "title": "Installation", "text": "<pre><code>pip install pythonping\n</code></pre> <p>By default it requires root permissions to run because Operating systems are designed to require root for creating raw IP packets, and sniffing the traffic on the network card. These actions are required to do the ping.</p> <p>If you don't want to run your script with root, you can use the capabilities framework. You can give Python the same capabilities as <code>/bin/ping</code> by doing:</p> <pre><code>sudo setcap cap_net_raw+ep $(readlink -f $(which python))\n</code></pre> <p>This will allow Python to capture raw packets, without having to give it full root permission.</p> <p>If you want to remove the permissions you can do:</p> <pre><code>sudo setcap -r $(readlink -f $(which python))\n</code></pre> <p>You can check that you've removed it with:</p> <pre><code>sudo getcap $(readlink -f $(which python))\n</code></pre> <p>If it doesn't return any output is that it doesn't have any capabilities.</p>"}, {"location": "pythonping/#usage", "title": "Usage", "text": "<p>If you want to see the output immediately, emulating what happens on the terminal, use the verbose flag as below. Otherwise it won't show any information on the <code>stdout</code>.</p> <pre><code>from pythonping import ping\n\nping(\"127.0.0.1\", verbose=True)\n</code></pre> <p>This will yield the following result.</p> <pre><code>Reply from 127.0.0.1, 9 bytes in 0.17ms\nReply from 127.0.0.1, 9 bytes in 0.14ms\nReply from 127.0.0.1, 9 bytes in 0.12ms\nReply from 127.0.0.1, 9 bytes in 0.12ms\n</code></pre> <p>Regardless of the verbose mode, the ping function will always return a <code>ResponseList</code> object. This is a special iterable object, containing a list of <code>Response</code> items. In each <code>Response</code>, you can find the packet received and some meta information, like:</p> <ul> <li><code>error_message</code>: contains a string describing the error this response   represents. For example, an error could be \u201cNetwork Unreachable\u201d or   \u201cFragmentation Required\u201d. If you got a successful response, this property is   None..</li> <li><code>success</code>: is a bool indicating if the response is successful.</li> <li><code>time_elapsed</code>: and time_elapsed_ms indicate how long it took to receive this   response, respectively in seconds and milliseconds..</li> </ul> <p>On top of that, <code>ResponseList</code> adds some intelligence you can access from its own members. The fields are self-explanatory:</p> <ul> <li><code>rtt_min</code> and <code>rtt_min_ms</code>.</li> <li><code>rtt_max</code> and <code>rtt_max_ms</code>.</li> <li><code>rtt_avg</code> and <code>rtt_avg_ms</code>.</li> </ul> <p>You can also tune your ping by using some of its additional parameters:</p> <ul> <li><code>size</code>: is an integer that allows you to specify the size of the ICMP payload   you desire.</li> <li><code>timeout</code>: is the number of seconds you wish to wait for a response, before   assuming the target is unreachable.</li> <li><code>payload</code>: allows you to use a specific payload (bytes).</li> <li><code>count</code>: specify allows you to define how many ICMP packets to send.</li> <li><code>interval</code>: the time to wait between pings, in seconds.</li> <li><code>sweep_start</code> and <code>sweep_end</code>: allows you to perform a ping sweep, starting   from payload size defined in sweep_start and growing up to size defined in   sweep_end. Here, we repeat the payload you provided to match the desired size,   or we generate a random one if no payload was provided. Note that if you   defined size, these two fields will be ignored. df is a flag that, if set to   True, will enable the Don't Fragment flag in the IP header verbose enables the   verbose mode, printing output to a stream (see out) out is the target stream   of verbose mode. If you enable the verbose mode and do not provide out,   verbose output will be send to the sys.stdout stream. You may want to use a   file here.</li> <li><code>match</code>: is a flag that, if set to True, will enable payload matching between   a ping request and reply (default behaviour follows that of Windows which   counts a successful reply by a matched packet identifier only; Linux behaviour   counts a non equivalent payload with a matched packet identifier in reply as   fail, such as when pinging 8.8.8.8 with 1000 bytes and the reply is truncated   to only the first 74 of request payload with a matching packet identifier).</li> </ul>"}, {"location": "pythonping/#references", "title": "References", "text": "<ul> <li>Git</li> <li>ictshore article on pythonping</li> </ul>"}, {"location": "questionary/", "title": "questionary", "text": "<p>questionary is a Python library based on Prompt Toolkit to effortlessly building pretty command line interfaces. It makes it very easy to query your user for input.</p> <p></p>"}, {"location": "questionary/#installation", "title": "Installation", "text": "<pre><code>pip install questionary\n</code></pre>"}, {"location": "questionary/#usage", "title": "Usage", "text": ""}, {"location": "questionary/#asking-a-single-question", "title": "Asking a single question", "text": "<p>Questionary ships with a lot of different Question Types to provide the right prompt for the right question. All of them work in the same way though.</p> <pre><code>import questionary\n\nanswer = questionary.text(\"What's your first name\").ask()\n</code></pre> <p>Since our question is a text prompt, answer will contain the text the user typed after they submitted it.</p>"}, {"location": "questionary/#asking-multiple-questions", "title": "Asking Multiple Questions", "text": "<p>You can use the <code>form()</code> function to ask a collection of <code>Questions</code>. The questions will be asked in the order they are passed to <code>questionary.form</code>.</p> <pre><code>import questionary\n\nanswers = questionary.form(\n    first=questionary.confirm(\"Would you like the next question?\", default=True),\n    second=questionary.select(\"Select item\", choices=[\"item1\", \"item2\", \"item3\"]),\n).ask()\n</code></pre> <p>The output will have the following format:</p> <pre><code>{'first': True, 'second': 'item2'}\n</code></pre> <p>The <code>prompt()</code> function also allows you to ask a collection of questions, however instead of taking Question instances, it takes a dictionary:</p> <pre><code>import questionary\n\nquestions = [\n    {\n        \"type\": \"confirm\",\n        \"name\": \"first\",\n        \"message\": \"Would you like the next question?\",\n        \"default\": True,\n    },\n    {\n        \"type\": \"select\",\n        \"name\": \"second\",\n        \"message\": \"Select item\",\n        \"choices\": [\"item1\", \"item2\", \"item3\"],\n    },\n]\n\nquestionary.prompt(questions)\n</code></pre>"}, {"location": "questionary/#conditionally-skip-questions", "title": "Conditionally skip questions", "text": "<p>Sometimes it is helpful to be able to skip a question based on a condition. To avoid the need for an if around the question, you can pass the condition when you create the question:</p> <pre><code>import questionary\n\nDISABLED = True\nresponse = questionary.confirm(\"Are you amazed?\").skip_if(DISABLED, default=True).ask()\n</code></pre> <p>If the condition (in this case <code>DISABLED</code>) is <code>True</code>, the question will be skipped and the default value gets returned, otherwise the user will be prompted as usual and the default value will be ignored.</p>"}, {"location": "questionary/#exit-when-using-control-c", "title": "Exit when using control + c", "text": "<p>If you want the question to exit when it receives a <code>KeyboardInterrupt</code> event, use <code>unsafe_ask</code> instead of <code>ask</code>.</p>"}, {"location": "questionary/#question-types", "title": "Question types", "text": "<p>The different question types are meant to cover different use cases. The parameters and configuration options are explained in detail for each type. But before we get into to many details, here is a cheatsheet with the available question types:</p> <ul> <li> <p>Use   <code>Text</code>   to ask for free text input.</p> </li> <li> <p>Use   <code>Password</code>   to ask for free text where the text is hidden.</p> </li> <li> <p>Use   <code>File   Path</code>   to ask for a file or directory path with autocompletion.</p> </li> <li> <p>Use   <code>Confirmation</code>   to ask a yes or no question.</p> </li> </ul> <pre><code>&gt;&gt;&gt; questionary.confirm(\"Are you amazed?\").ask()\n? Are you amazed? Yes\nTrue\n</code></pre> <ul> <li> <p>Use   <code>Select</code>   to ask the user to select one item from a beautiful list.</p> </li> <li> <p>Use   <code>Raw   Select</code>   to ask the user to select one item from a list.</p> </li> <li> <p>Use   <code>Checkbox</code>   to ask the user to select any number of items from a list.</p> </li> <li> <p>Use   <code>Autocomplete</code>   to ask for free text with autocomplete help.</p> </li> </ul> <p>Check the examples to see them in action and how to use them.</p>"}, {"location": "questionary/#styling", "title": "Styling", "text": ""}, {"location": "questionary/#dont-highlight-the-selected-option-by-default", "title": "Don't highlight the selected option by default", "text": "<p>If you don't want to highlight the default choice in the <code>select</code> question use the next style:</p> <pre><code>from questionary import Style\n\nchoice = select(\n    \"Question title: \",\n    choices=[\"a\", \"b\", \"c\"],\n    default=\"a\",\n    style=Style([(\"selected\", \"noreverse\")]),\n).ask()\n</code></pre>"}, {"location": "questionary/#testing", "title": "Testing", "text": "<p>To test questionary code, follow the guidelines of testing prompt_toolkit.</p>"}, {"location": "questionary/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "qwik/", "title": "Qwik", "text": "<p>Qwik is a new kind of web framework that can deliver instantly load web applications at any size or complexity. Your sites and apps can boot with about 1kb of JS (regardless of application complexity), and achieve consistent performance at scale.</p> <p>You can see a good overview in the Qwik presentation.</p>"}, {"location": "qwik/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> </ul>"}, {"location": "ram/", "title": "RAM", "text": "<p>RAM is a form of computer memory that can be read and changed in any order, typically used to store working data and machine code.</p> <p>A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory, in contrast with other direct-access data storage media (such as hard disks), where the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.</p> <p>They are the faster device either to read or to write your data.</p>"}, {"location": "ram/#properties", "title": "Properties", "text": "<p>RAM sticks vary on:</p> <ul> <li> <p>Size: the amount of data that they can hold, measured in GB.</p> </li> <li> <p>Frequency: how often the RAM is accessed per second, measured in MHz.</p> </li> <li> <p>Clock latency (CL): number of cycles before the RAM responds.</p> </li> <li> <p>Type: as the technology evolves there are different types, such as DDR4.</p> </li> <li> <p>Form factor: There are different types of RAM in regards of the devices   they'll fit in:</p> </li> <li> <p>260-pin SO-DIMMs: laptop RAM, shorter, slower, more expensive, and won't fit     in a desktop system.</p> </li> <li> <p>288-pin DIMMs: desktop RAM - required for most desktop motherboards.</p> </li> <li> <p>ECC: Whether it has error correction code.</p> </li> </ul>"}, {"location": "ram/#speed", "title": "Speed", "text": "<p>RAM's speed is measured as a combination of frequency and clock latency. More cycles per second means the RAM is 'faster', but you also have to consider latency as well. If you compare MHz and CL, you can get an idea of actual speed. For example, 3600 MHz CL18 and 3200 MHz CL16 are the same speed on paper since the faster 3600 MHz takes more clocks to respond, but there are more clocks per second, so the response time is actually the same.</p> <p>!!! note In reality, faster RAM will be a little bit faster in modern architectures. Also, Ryzen specifically prefers 3600 MHz RAM because of how its FCLK works it likes whole-number multipliers, so if it can run at 1800 MHz (x2 = 3600 MHz with the RAM), then it will run 2-3% faster than equivalent 3200 MHz RAM.</p> <p>Summing up, the higher the speed, and the lower the CL, the better the overall performance.</p> <pre><code>RAM latency (lower the better) = (CAS Latency (CL) x 2000 ) / Frequency (MHz)\n</code></pre>"}, {"location": "ram/#ecc", "title": "ECC", "text": "<p>Error correction code memory (ECC memory) is a type of computer data storage that uses an error correction code to detect and correct n-bit data corruption which occurs in memory. ECC memory is used in most computers where data corruption cannot be tolerated, for example when using zfs.</p>"}, {"location": "ram/#how-to-choose-a-ram-stick", "title": "How to choose a RAM stick", "text": ""}, {"location": "ram/#cpu-brand", "title": "CPU brand", "text": "<p>Depending on your CPU brand you need to take into account the next advices:</p> <ul> <li>Intel: Intel CPUs aren\u2019t massively reliant on the performance of memory while   running, which might explain why RAM speed support has historically been   rather limited outside of Intel\u2019s enthusiast chipsets (Z-Series) and capped to   2666Mhz (at least until recently).</li> </ul> <p>If you\u2019re the owner of an Intel CPU we certainly suggest getting a good   quality RAM kit, but the speed of that RAM isn\u2019t as important. Save your money   for other components or a RAM capacity upgrade if required.</p> <ul> <li>AMD: In stark contrast to Intel, AMD\u2019s more recent \u2018Zen\u2019 line of CPUs has RAM   speed almost baked into the architecture of the CPU.</li> </ul> <p>AMD\u2019s infinity fabric technology uses the speed of the RAM to pass information   across sections of the CPU. This means that better memory will serve to boost   the CPU performance as well as helping in those intense applications we   mentioned earlier.</p>"}, {"location": "ram/#motherboard", "title": "Motherboard", "text": "<p>Many manufacturers list specific RAM kits as \u2018verified\u2019 with their products, meaning that the manufacturer has tested the motherboard model in question with a specific RAM kit and has confirmed full support for that kit, at its advertised speed and CAS latency.</p> <p>Try to purchase RAM listed on your motherboard\u2019s QVL where possible, for the best compatibility. However, this is almost always impractical given the availability of exact RAM kits at any given time.</p>"}, {"location": "ram/#achieving-stability", "title": "Achieving stability", "text": "<p>Speed, CAS latency, module size, and module quantity; in order to avoid running into problems you should balance these factors when considering your purchase.</p> <p>For example, 16GB of 3600MHz CL16 memory is much more likely to be stable than 32GB of the same modules, even if the settings in BIOS remain the same.</p> <p>Consider another example - you may want to run 64GB of RAM at 3600MHz, but to get it to run properly you need to lower the speed to 3000MHz.</p>"}, {"location": "ram/#conclusion", "title": "Conclusion", "text": "<p>In summary, a high-performance 3600MHz memory kit is ideal for AMD Ryzen CPUs. Decide the size, speed, if you need ECC and make sure which type of RAM does your CPU and motherboard combo support (ie, DDR3, DDR4, or DDR5), and that you're choosing the correct form factor. Then, buy a kit that is in line with your budget.</p> <p>You're probably looking for DDR4, probably <code>2x8 = 16 GB</code>. The sweet spot there will likely be 3600 MHz CL18 or 3200 MHz CL16 for $55 or so. Technically, you should check your motherboard's QVL (list of RAM that is guaranteed to work), but most big-name brands should work. There are other things to consider - like, does your cooler interfere with RAM? But, generally only top-tier coolers have RAM fitment issues.</p>"}, {"location": "ram/#references", "title": "References", "text": "<ul> <li>How to choose RAM: Speed vs Capacity</li> </ul>"}, {"location": "redox/", "title": "Redox", "text": "<p>Redox</p>"}, {"location": "redox/#installation", "title": "Installation", "text": "<p>First flash:</p> <p>Download the hex from the via website</p> <p>Try to flash it many times reseting the promicros.</p> <pre><code>sudo avrdude -b 57600 -p m32u4 -P /dev/ttyACM0 -c avr109 -D -U flash:w:redox_rev1_base_via.hex\n</code></pre> <p>Once the write has finished install via:</p> <p>https://github.com/the-via/releases/releases</p> <p>Reboot the computer</p> <p>launch it with <code>via-nativia</code>.</p>"}, {"location": "redox/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "refinement_template/", "title": "Refinement template", "text": ""}, {"location": "refinement_template/#refinement", "title": "Refinement", "text": ""}, {"location": "refinement_template/#doubts", "title": "Doubts", "text": ""}, {"location": "refinement_template/#expected-current-sprint-undone-tasks", "title": "Expected current sprint undone tasks", "text": ""}, {"location": "refinement_template/#review-the-proposed-kanban-board", "title": "Review the proposed Kanban board", "text": ""}, {"location": "refinement_template/#sprint-goals", "title": "Sprint Goals", "text": "<p>With this proposed plan we'll:</p> <p>*</p>"}, {"location": "regicide/", "title": "Regicide", "text": "<p>Regicide is a wonderful cooperative card game for 1 to 4 players. It's awesome how they've built such a rich game dynamic with a normal deck of cards. Even if you can play it with any deck, I suggest to buy the deck they sell because their cards are magnificent and they deserve the money for their impressive game. Another thing I love about them is that even if you can't or don't want to pay for the game, they give the rules for free.</p> <p>If you don't like reading the rules directly from their pdf (although it's quite short), they explain them in this video.</p>"}, {"location": "regicide/#variations", "title": "Variations", "text": "<p>Each game is different and challenging despite your experience, even so, I've thought that to make it even more varied and rich, the players can use one or many of the next expansions:</p> <ul> <li>Situation modifiers.</li> <li>Player modifiers.</li> </ul> <p>Each of the expansions make the game both more different and more complex, so it's not suggested to use them all at the same time, try one, and once the players are used to it, add another one.</p> <p>These expansions are yet untested ideas, so they might break the playability.</p> <p>If you have any suggestion please contact me or open a pull request.</p> <p>Throughout the next sections you'll encounter the <code>1 x level</code> notation to define the times an action will take place. It's assumed that the level of the enemies is:</p> Card Level J 1 Q 2 K 3"}, {"location": "regicide/#situation-modifiers", "title": "Situation modifiers", "text": "<p>You can spice up each game by making each round different by applying situation modifiers. Once a new enemy arrives the scene, roll up a dice to choose one of the next situations:</p> <ol> <li>Disloyal: The fighters you use in this round that match the enemy's suit will    be disloyal to you and will join the enemy's ranks in the next round. The    player will receive damage of both the enemy and their minions. Players will    need to kill their minions before they hit the royal enemy.</li> <li>Cornered: You're cornered and the enemy archers are firing you. At the start of     each player turn, it takes <code>2 x level</code> amount of damage.</li> <li>Exterminator: When it deals damage, <code>1 x level</code> of the player discarded    cards are taken out of the game.</li> <li>Enemy ambush: When the enemy enters the scene, it deals <code>2 x level</code> amount of     damage to the players.</li> <li>Enemy Spy: It has moles inside your ranks. When it comes to scene, you     drop <code>1 x level</code> amount of cards of that suit.</li> <li>Necromancer: When it hits the first player, the smallest discarded card    goes to the enemy ranks instead of the discarded pile. Players need to kill    this minion before hitting the enemy.</li> <li>Represor Enemy: It kills <code>1 x level</code> people from the tavern at the start of each     player's turn.</li> <li>Dexterous Enemy: It has a <code>20% x level</code> of chances to dodge the player's    attack.</li> <li>Quick Enemy: The enemy hits you in the first phase of your turn, instead of the last.</li> <li>Stronger Enemy: It deals <code>2 x level</code> more damage.</li> <li> <p>Tougher Enemy: It has <code>3 x level</code> more health.</p> </li> <li> <p>Blind Enemy: It attacks to any player that makes a noise in addition to the    player that is currently fighting it.</p> </li> <li>Random Enemy: There is no logic in it's actions, instead of attacking the    player who is playing, it attacks a random one.</li> <li> <p>Uneventful round: Nothing happens, play the round as the vanilla game.</p> </li> <li> <p>Softer Enemy: It has <code>3 x level</code> less health.</p> </li> <li>Weaker Enemy: It deals <code>2 x level</code> less damage.</li> <li>Clumsy Enemy: It has a <code>20% x level</code> of chances to fail when hitting the    players.</li> <li>Resistance Spies: You have moles inside the enemy ranks that removes their    suit invulnerability.</li> <li>Ambush: When the enemy enters the scene, you all deal <code>2 x level</code> amount of     damage to the enemy.</li> <li>Communist/Anarchist \"enemy\": It really is on your side to bring down the    monarchy, so you all get a <code>1 x level</code> to all the cards you play, and a <code>2    x level</code> to it's suit.</li> <li>Enemy Cornered: You cornered the enemy, and your archers are firing them. At the start of     each player turn, the enemy takes <code>2 x level</code> amount of damage.</li> <li>Loyal: The fighters you use in this round that match the enemy's suit will    be loyal to you and won't be moved to the discarded pile at the end of the    round. On the first round of each player, they'll use both the card in the    table and the one that they use.</li> </ol>"}, {"location": "regicide/#player-modifiers", "title": "Player modifiers", "text": "<p>At the start of the game players can decide their suit, they will get a bonus on the played cards of their suit, and a penalization on the opposite suit. The opposite suits are:</p> <ul> <li>\u2660 opposite of \u2665</li> <li>\u2663 opposite of \u2666</li> </ul> <p>The bonus depends on the level of the enemy being:</p> <ul> <li>J: +1 or -1</li> <li>Q: +2 or -2</li> <li>K: +3 or -3</li> </ul> <p>Imagine that I've chosen \u2666 as my suit, if I were to play:</p> <ul> <li>The 8\u2666 against a J\u2665, I'd draw <code>8+1</code> cards from the deck, and deal <code>8+1</code> damage</li> <li>The 7\u2663 against a Q\u2660, I'd deal 10 of damage <code>(7-2) * 2</code>.</li> <li>The 4\u26664\u2665 against a K\u2663, I'd heal and draw 11 cards <code>(4+4+3)</code>.</li> <li>The 4\u26664\u2663 against a K\u2660, I'd draw 8 cards <code>(4+4+3-3)</code> and deal 16 of damage.</li> </ul> <p>I haven't decide yet if the bonus should apply at the time of receiving damage, we played one without counting and it was playable, will test the other soon.</p>"}, {"location": "regicide/#player-classes", "title": "Player classes", "text": "<ul> <li> <p>Create player classes: Each player class has abilities that can use <code>X</code> amount     of times in the match:</p> <ul> <li>Archer:</li> <li>Healer:</li> <li>Paladin:</li> <li>Warrior:</li> <li>Wizard:</li> </ul> </li> </ul>"}, {"location": "regicide/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "relationship_management/", "title": "Relationship Management", "text": "<p>I try to keep my mind as empty as possible of non relevant processes, that's why I use a task manager to handle my tasks and meetings. This system has a side effect, if there isn't something reminding you that you have to do something, you fail to do it. That principle applied to human relationships means that if you don't stumble that person in your daily life, it doesn't exist for you and you will probably not take enough care that the person deserves.</p> <p>To solve that problem I started creating periodic tasks to call these people or hang out. I've also used those tasks to keep a diary of the interactions.</p> <p>Recently I've found Monica a popular open source personal CRM that helps in the same direction.</p> <p>So I'm going to migrate all my information to the system and see how it goes.</p>"}, {"location": "remote_work/", "title": "Remote working", "text": "<p>Remote working is a work arrangement in which employees do not commute or travel (e.g. by bus, bicycle or car, etc.) to a central place of work, such as an office building, warehouse, or store.</p> <p>As a side effect, we're spending a lot of time in front of our computers, so we should be careful that our working environment helps us to stay healthy. For example we could:</p> <ul> <li> <p>Use an external monitor: Your laptop's screen is usually not big enough and     will force you to look down instead of look straight which can lead to neck     pain. Some prefer super big monitors (48 inches) while others feel that 24     inches is more than enough so you don't have to turn your head to reach each     side of the screen. For me the sweet spot is having two terminals with 100     characters of width one beside the other. If you use a tiling window manager     like i3wm, that should be enough.</p> <p>Some people valued that the screen was not fixed, so it could be tilted or it's height could be changed.</p> </li> <li> <p>Adjust the screen to your eye level: The center of the monitor should be at     eye level, if the monitor height adjustment is not enough, you can use some     old books or buy a screen support.</p> </li> <li> <p>Use an external keyboard: Sometimes the keys of the laptop keyboards have     a cheap feedback or a weird key disposition, which leads to finger and wrist     aches.  The use of an external keyboard (better if it's a mechanical one)     can help with this issue.</p> </li> <li> <p>The chair should support your back and don't be too hard to hurt your butt, nor too     soft.</p> </li> <li> <p>Your legs should not be hanging in the air, that will add unnecessary pressure on your     thighs which can lead to tingling. If you're in this situation, a leg     support comes handy.</p> <p>The legs shouldn't be crossed either in front or below you, they should be straight with a 90 degree angle between your thighs and your calves, with a waist level separation between the feet.</p> </li> <li> <p>The table height should be enough to have a 90 degree angle between your     forearms and your biceps , and your shoulders are in a relaxed stance. Small     people may need a table with no drawers between your elbows and your legs,     or you wont be able to fulfill the arm's requirement.</p> <p>The table height should be low enough to fulfill the leg's requirement above. Sometimes they are too high to be able to have a 90 degree angle between the thighs and calves even with feet support, in that case, change the desk or cut it's legs.</p> </li> <li> <p>Think about using a standing desk. Desk's with variable height are quite     expensive, but there is always the option to buy a laptop support that let's     you stand.</p> </li> <li> <p>Your hands should be at the same level as your forearms, you could use a wrist     support for that and also to soften the contact of your forearms with the     desk.</p> </li> <li> <p>If you're a heavy mouse user, think of using a vertical mouse instead of the     traditional to prevent the metacarpal syndrome. And try not to use it! learn     how to use a tiling window manager and Vim shortcuts for everything, such as     using tridactyl for Firefox.</p> </li> <li> <p>Keep your working place between 19 and 21 degrees centigrades, otherwise you     may unawarely contract your body.</p> </li> <li> <p>Use blue filter either in your glasses or in your screen.</p> </li> <li> <p>Have enough light so you don't need to strain your eyes. Having your monitor     screen as your only source of light is harmful.</p> </li> <li> <p>Try to promote initiatives that increase the social interaction between your     fellow workers.</p> </li> <li> <p>Stand up and walk around at least once each two hours. Meetings are a good     moment to do an outside walk.</p> </li> </ul> <p>Other tips non related with your work environment but with the consequences of your work experience can be:</p> <ul> <li> <p>Don't remain static, doing exercise daily is a must. As you don't need to go     out, it's quite easy to fall into the routine of waking up, sit in your     computer, eat and go back to sleep. Both anaerobic (pilates, yoga or     stretching) and aerobic (running, biking or dancing) give different     benefits.</p> </li> <li> <p>Drink enough water, around 8 to 10 glasses per day.</p> </li> <li> <p>Use the extra time that remote working gives you to strengthen your outside     work social relationships.</p> </li> <li> <p>Try not to be exposed to any screen light for an hour before you go to sleep.     If you use an e-book, don't rely on their builtin light, use an external     source instead.</p> </li> </ul> <p>If you have a remote work contract, make sure that your employer pays for any upgrades, it's their responsibility.</p>"}, {"location": "renovate/", "title": "Renovate", "text": "<p>Renovate is a program that does automated dependency updates. Multi-platform and multi-language.</p> <p>Why use Renovate?</p> <ul> <li>Get pull requests to update your dependencies and lock files.</li> <li>Reduce noise by scheduling when Renovate creates PRs.</li> <li>Renovate finds relevant package files automatically, including in monorepos.</li> <li>You can customize the bot's behavior with configuration files.</li> <li>Share your configuration with ESLint-like config presets.</li> <li>Get replacement PRs to migrate from a deprecated dependency to the community     suggested replacement (npm packages only).</li> <li>Open source.</li> <li>Popular (more than 9.7k stars and 1.3k forks)</li> <li>Beautifully integrate with main Git web applications (Gitea, Gitlab, Github).</li> <li>It supports most important languages: Python, Docker, Kubernetes, Terraform,     Ansible, Node, ...</li> </ul>"}, {"location": "renovate/#behind-the-scenes", "title": "Behind the scenes", "text": ""}, {"location": "renovate/#how-renovate-updates-a-package-file", "title": "How Renovate updates a package file", "text": "<p>Renovate:</p> <ul> <li>Scans your repositories to detect package files and their dependencies.</li> <li>Checks if any newer versions exist.</li> <li>Raises Pull Requests for available updates.</li> </ul> <p>The Pull Requests patch the package files directly, and include Release Notes for the newer versions (if they are available).</p> <p>By default:</p> <ul> <li>You'll get separate Pull Requests for each dependency.</li> <li>Major updates are kept separate from non-major updates.</li> </ul>"}, {"location": "renovate/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "requests/", "title": "Requests", "text": "<p>Requests is an elegant and simple HTTP library for Python, built for human beings.</p>"}, {"location": "requests/#installation", "title": "Installation", "text": "<pre><code>pip install requests\n</code></pre>"}, {"location": "requests/#usage", "title": "Usage", "text": ""}, {"location": "requests/#download-file", "title": "Download file", "text": "<pre><code>url = \"http://beispiel.dort/ichbineinbild.jpg\"\nfilename = url.split(\"/\")[-1]\nr = requests.get(url, timeout=0.5)\n\nif r.status_code == 200:\n    with open(filename, 'wb') as f:\n        f.write(r.content)\n</code></pre>"}, {"location": "requests/#encode-url", "title": "Encode url", "text": "<pre><code>requests.utils.quote('/test', safe='')\n</code></pre>"}, {"location": "requests/#get", "title": "Get", "text": "<pre><code>requests.get('{{ url }}')\n</code></pre>"}, {"location": "requests/#put-url", "title": "Put url", "text": "<pre><code>requests.put({{ url }})\n</code></pre>"}, {"location": "requests/#put-json-data-url", "title": "Put json data url", "text": "<pre><code>data = {\"key\": \"value\"}\nrequests.put({{ url }} json=data)\n</code></pre>"}, {"location": "requests/#use-cookies-between-requests", "title": "Use cookies between requests", "text": "<p>You can use Session objects to persists cookies or default data across all requests.</p> <pre><code>s = requests.Session()\n\ns.get('https://httpbin.org/cookies/set/sessioncookie/123456789')\nr = s.get('https://httpbin.org/cookies')\n\nprint(r.text)\n# '{\"cookies\": {\"sessioncookie\": \"123456789\"}}'\n\ns.auth = ('user', 'pass')\ns.headers.update({'x-test': 'true'})\n\n# both 'x-test' and 'x-test2' are sent\ns.get('https://httpbin.org/headers', headers={'x-test2': 'true'})\n</code></pre>"}, {"location": "requests/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "rich/", "title": "Rich", "text": "<p>Rich is a Python library for rich text and beautiful formatting in the terminal.</p>"}, {"location": "rich/#installation", "title": "Installation", "text": "<pre><code>pip install rich\n</code></pre>"}, {"location": "rich/#usage", "title": "Usage", "text": ""}, {"location": "rich/#progress-display", "title": "Progress display", "text": "<p>Rich can display continuously updated information regarding the progress of long running tasks / file copies etc. The information displayed is configurable, the default will display a description of the \u2018task\u2019, a progress bar, percentage complete, and estimated time remaining.</p> <p>Rich progress display supports multiple tasks, each with a bar and progress information. You can use this to track concurrent tasks where the work is happening in threads or processes.</p> <p>It's beautiful, check it out with <code>python -m rich.progress</code>.</p>"}, {"location": "rich/#basic-usage", "title": "Basic Usage", "text": "<p>For basic usage call the track() function, which accepts a sequence (such as a list or range object) and an optional description of the job you are working on. The track method will yield values from the sequence and update the progress information on each iteration. Here\u2019s an example:</p> <pre><code>from rich.progress import track\n\nfor n in track(range(n), description=\"Processing...\"):\n    do_work(n)\n</code></pre>"}, {"location": "rich/#tables", "title": "Tables", "text": "<pre><code>from rich.console import Console\nfrom rich.table import Table\n\ntable = Table(title=\"Star Wars Movies\")\n\ntable.add_column(\"Released\", justify=\"right\", style=\"cyan\", no_wrap=True)\ntable.add_column(\"Title\", style=\"magenta\")\ntable.add_column(\"Box Office\", justify=\"right\", style=\"green\")\n\ntable.add_row(\"Dec 20, 2019\", \"Star Wars: The Rise of Skywalker\", \"$952,110,690\")\ntable.add_row(\"May 25, 2018\", \"Solo: A Star Wars Story\", \"$393,151,347\")\ntable.add_row(\"Dec 15, 2017\", \"Star Wars Ep. V111: The Last Jedi\", \"$1,332,539,889\")\ntable.add_row(\"Dec 16, 2016\", \"Rogue One: A Star Wars Story\", \"$1,332,439,889\")\n\nconsole = Console()\nconsole.print(table)\n</code></pre>"}, {"location": "rich/#rich-text", "title": "Rich text", "text": "<pre><code>from rich.console import Console\nfrom rich.text import Text\n\nconsole = Console()\ntext = Text.assemble((\"Hello\", \"bold magenta\"), \" World!\")\nconsole.print(text)\n</code></pre>"}, {"location": "rich/#live-display-text", "title": "Live display text", "text": "<pre><code>import time\n\nfrom rich.live import Live\n\nwith Live(\"Test\") as live:\n    for row in range(12):\n        live.update(f\"Test {row}\")\n        time.sleep(0.4)\n</code></pre> <p>If you don't want the text to have the default colors, you can embed it all in a <code>Text</code> object.</p>"}, {"location": "rich/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "rtorrent/", "title": "Rtorrent", "text": ""}, {"location": "rtorrent/#debugging", "title": "Debugging", "text": "<ul> <li>Get into the docker with <code>docker exec -it docker_name bash</code></li> <li><code>cd /home/nobody</code></li> <li> <p>Open the <code>rtorrent.sh</code> file add <code>set -x</code> above the line you think is starting your <code>rtorrent</code> and <code>set +x</code> below to fetch the command that is launching your rtorrent instance, for example:</p> <pre><code>/usr/bin/tmux new-session -d -s rt -n rtorrent /usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211\n</code></pre> </li> </ul> <p>If you manually run <code>/usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211</code> you'll get more information on why <code>rtorrent</code> is not starting.</p>"}, {"location": "scrum/", "title": "Scrum", "text": "<p>Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management.  It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks.</p> <p>For my personal scrum workflow and in the DevOps and DevSecOps teams I've found that Sprint goals are not operative, as multiple unrelated tasks need to be done, so it doesn't make sense to define just one goal.</p>"}, {"location": "scrum/#the-meetings", "title": "The meetings", "text": "<p>Scrum tries to minimize the time spent in meetings while keeping a clearly defined direction and a healthy environment between all the people involved in the project.</p> <p>To achieve that is uses four types of meetings:</p> <ul> <li>Daily.</li> <li>Refinement.</li> <li>Retros.</li> <li>Reviews.</li> <li>Plannings.</li> </ul>"}, {"location": "scrum/#daily-meetings", "title": "Daily meetings", "text": "<p>Dailies or weeklies are the meetings where the development team exposes at high level of detail the current work. Similar to the dailies in the scrum terms, in the meeting each development team member exposes:</p> <ul> <li>The advances in the assigned tasks, with special interest in the encountered     problems and deviations from the steps defined in the refinement.</li> <li>An estimation of the tasks that are going to be left unfinished by the end of     the sprint.</li> </ul> <p>The goals of the meeting are:</p> <ul> <li>Get a general knowledge of what everyone else is doing.</li> <li>Learn from the experience gained by the others while doing their tasks.</li> <li>Get a clear idea of where we stand in terms of completing the sprint tasks.</li> </ul> <p>As opposed to what it may seem, this meeting is not meant to keep track of the productivity of each of us, we work based on trust, and know that each of us is working our best.</p>"}, {"location": "scrum/#refinement-meetings", "title": "Refinement meetings", "text": "<p>Refinement are the meetings where the development team reviews the issues in the backlog and prepares the tasks that will probably be done in the following sprint.</p> <p>The goals of the meeting are:</p> <ul> <li>Next sprint tasks are ready to be worked upon in the next sprint. That means     each task:<ul> <li>Meets the Definition of Ready.</li> <li>All disambiguation in task description, validation criteria and steps     is solved.</li> </ul> </li> <li>Make the Planning meeting more dynamic.</li> </ul> <p>The meeting is composed of the following phases:</p> <ul> <li>Scrum master preparation.</li> <li>Development team refinement.</li> <li>Product owner refinement.</li> </ul>"}, {"location": "scrum/#refinement-preparation", "title": "Refinement preparation", "text": "<p>To prepare the refinement, the scrum master has to:</p> <ul> <li>Make a copy of the Refinement document template.</li> <li>Open the OKRs document if you have one and for category in OKR categories:</li> <li>Select the category label in the issue tracker and select the milestone of       the semester.</li> <li>Review which of those issues might enter the next sprint, and set the sprint       project on them.</li> <li> <p>Remove the milestone from the issue filter to see if there are interesting       issues without the milestone set.</p> </li> <li> <p>Go to the next sprint Kanban board:</p> </li> <li>Order the issues by priority.</li> <li>Make sure there are tasks with the <code>Good first issue</code> label.</li> <li> <p>Make sure that there are more tasks than we can probably do so we can remove       some instead of need to review the backlog and add more in the       refinement.</p> </li> <li> <p>Fill up the sprint goals section of the refinement document.</p> </li> <li>Create the Refinement developer team and product owner meeting calendar     events.</li> </ul>"}, {"location": "scrum/#development-team-refinement-meeting", "title": "Development team refinement meeting", "text": "<p>In this meeting the development team with the help of the scrum master, reviews the tasks to be added to the next sprint. The steps are defined in the refinement template.</p>"}, {"location": "scrum/#product-owner-refinement-meeting", "title": "Product owner refinement meeting", "text": "<p>In this meeting the product owner with the help of the scrum master reviews the tasks to be added to the next sprint. With the refinement document as reference:</p> <ul> <li>The expected current sprint undone tasks are reviewed.</li> <li>The sprint goals are discussed, modified and agreed. If there are many     changes, we might think of setting the goals together in next sprints.</li> <li>The scrum master does a quick description of each issue.</li> <li>Each task priority is discussed and updated.</li> </ul>"}, {"location": "scrum/#retro-meetings", "title": "Retro meetings", "text": "<p>Retrospectives or Retros are the meetings where the scrum team plan ways to increase the quality and effectiveness of the team.</p> <p>The scrum master conducts different dynamics to help the rest of the scrum team inspect how the last Sprint went with regards to individuals, interactions, processes, tools, and their Definition of Done and Ready. Assumptions that led them astray are identified and their origins explored. The most impactful improvements are addressed as soon as possible. They may even be added to the backlog for the next sprint.</p> <p>Although improvements may be implemented at any time, the sprint retrospective provides a formal opportunity to focus on inspection and adaptation.</p> <p>The sprint retrospective concludes the sprint.</p> <p>The meeting consists of five phases, all of them conducted by the scrum master:</p> <ul> <li>Set the stage: There is an opening dynamic to give people time to \u201carrive\u201d     and get into the right mood.</li> <li>Gather Data: Help everyone remember. Create a shared pool of information     (everybody sees the world differently). There is an initial dynamic to     measure the general feeling of the team and the issues to analyze further.</li> <li>Generate insights: Analyze why did things happen the way they did, identify     patterns and see the big picture.</li> <li>Decide what to do: Pick a few issues to work on and create concrete action     plans of how you\u2019ll address them. Adding the as issues in the scrum board.</li> <li>Close the retrospective: Clarify follow-ups, show appreciations, leave the     meeting with a general good feeling, and analyze how could the     retrospectives improve.</li> </ul> <p>If you have no idea how to conduct this meeting, you can take ideas from retromat.</p> <p>The goals of the meeting are:</p> <ul> <li>Analyze and draft a plan to iteratively improve the team's well-being, quality     and efficiency.</li> </ul>"}, {"location": "scrum/#review-meetings", "title": "Review meetings", "text": "<p>Reviews are the meetings where the product owner presents the sprint work to the rest of the team and the stakeholders. The idea of what is going to be done in the next sprint is also defined in this meeting.</p> <p>The meeting goes as follows:</p> <ul> <li>The product owner explains what items have been \u201cDone\u201d and what has not been     \u201cDone\u201d.</li> <li>The product owner discuss what went well during the sprint, what problems they     ran into, and how those problems were solved.</li> <li>The developers demonstrate the work that it has \u201cDone\u201d and answers questions.</li> <li>The product owner discusses the Product Backlog as it stands in terms of the     semester OKRs.</li> <li>The entire group collaborates on what to do next, so that the Sprint Review     provides valuable input to subsequent Sprint Planning.</li> </ul> <p>As the target audience are the stakeholders, the language must be changed accordingly, we should give overall ideas and not get caught in complicated high tech detailed explanations unless they ask them.</p> <p>The goals of the meeting are:</p> <ul> <li> <p>Increase the transparency on what the team has done in the sprint. By     explaining to the stake holders:</p> <ul> <li>What has been done.</li> <li>The reasons why we've implemented the specific outcomes for the tasks.</li> <li>The deviation from the expected plan of action.</li> <li>The status of the unfinished tasks with an explanation of why weren't they     closed.</li> <li>The meaning of the work done in terms of the semester OKRs.</li> </ul> </li> <li> <p>Increase the transparency on what the team plans to do for the following     sprint by explaining to the stakeholders:</p> <ul> <li>What do we plan to do in the next semester.</li> <li>How we plan to do it.</li> <li>The meaning of the plan in terms of the semester OKRs.</li> </ul> </li> <li> <p>Get the feedback from the stakeholders. We expect to gather and process their     feedback by processing their opinions both of the work done of the past     sprint and the work to be done in the next one. It will be gathered by the     scrum master and persisted in the board on the planning meetings.</p> </li> <li> <p>Incorporate the stakeholders in the decision making process of the team. By     inviting them to define with the rest of the scrum team the tasks for the     next sprint.</p> </li> </ul>"}, {"location": "scrum/#planning-meetings", "title": "Planning meetings", "text": "<p>Plannings are the meetings where the scrum team decides what it's going to do in the following sprint. The decision is made with the information gathered in the refinement, retro and review sessions.</p> <p>Conducted by the scrum master, usually only the members of the scrum team (developers, product owner and scrum master) are present, but stakeholders can also be invited.</p> <p>If the job has been done in the previous sessions, the backlog should be priorized and refined, so we should only add the newest issues gathered in the retro and review, refine them and decide what we want to do this sprint.</p> <p>The meeting goes as follows:</p> <ul> <li>We add the issues raised in the review to the backlog.</li> <li>We analyze the tasks on the top of the backlog, add them to the sprint     board without assigning it to any developer.</li> <li>Once all tasks are added, we the stats of past sprints to see if the scope is     realistic.</li> </ul> <p>The goals of the meeting are:</p> <ul> <li>Assert that the tasks added to the sprint follow the global path defined by     the semester OKRs.</li> <li>All team has a clear view of what needs to be done.</li> <li>The team makes a realistic work commitment.</li> </ul>"}, {"location": "scrum/#the-roles", "title": "The roles", "text": "<p>There are three roles required in the scrum team:</p> <ul> <li>Product owner.</li> <li>Scrum master.</li> <li>Developer.</li> </ul>"}, {"location": "scrum/#product-owner", "title": "Product owner", "text": "<p>Scrum product owner is accountable for maximizing the value of the product resulting from the work of the scrum team.</p> <p>It's roles are:</p> <ul> <li> <p>Assist the scrum master with:</p> <ul> <li>Priorization of the semester OKRs.</li> <li>Monitorization of the status of the semester OKRs on reviews and     plannings.</li> <li>Priorization of the sprint tasks.</li> </ul> </li> <li> <p>Conduct the daily meetings:</p> <ul> <li>Show the Kanban board in the meeting</li> <li>Remind the number of weeks left until the review meeting.</li> <li>Make sure that the team is aware of what tasks are going to be left undone     at the end of the sprint.</li> <li>Inform the affected stakeholders of the possible delay.</li> </ul> </li> <li> <p>Prepare and conduct the review meeting:</p> <ul> <li>With the help of the scrum master, prepare the reports:<ul> <li>Create the report of the sprint, including:<ul> <li>Make sure that the Definition of Done is     met for the closed tasks.</li> <li>Explanation of the done tasks.</li> <li>Status of uncompleted tasks, and reason why they weren't complete.</li> <li>The meaning of the work done in terms of the semester OKRs.</li> </ul> </li> <li>Create the report of the proposed next sprint's planning, with     arguments behind why we do each task.</li> </ul> </li> <li>Conduct the review meeting presenting the reports to the stakeholders.</li> </ul> </li> <li>Attend the daily, review, retro and planning meetings.</li> </ul>"}, {"location": "scrum/#scrum-master", "title": "Scrum master", "text": "<p>Scrum master is accountable for establishing Scrum as defined in this document.</p> <p>This position is going to be rotated between the members of the scrum team with a period of two sprints.</p> <p>It's roles are:</p> <ul> <li> <p>Monitoring the status of the semester OKRs on reviews and plannings.</p> <ul> <li>Create new tasks required to meet the objectives.</li> </ul> </li> <li> <p>Refining the backlog:</p> <ul> <li>Adjust priority.</li> <li>Refine the tasks that are going to enter next sprint.</li> <li>Organize the required meetings to refine the backlog with the team     members.</li> <li>Delete deprecated tasks.</li> </ul> </li> <li> <p>Assert that issues that are going to enter the new sprint meet the Definition     of Ready.</p> </li> <li> <p>Arrange, prepare the daily meetings:</p> <ul> <li>Update the calendar events according to the week needs.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the review meeting:</p> <ul> <li>Create the calendar event inviting the scrum team and the stakeholders.</li> <li>With the help of the product owner, prepare the reports:<ul> <li>Create the report of the sprint, including:<ul> <li>Make sure that the Definition of Done is     met for the closed tasks.</li> <li>Explanation of the done tasks.</li> <li>Status of uncompleted tasks, and reason why they weren't complete.</li> <li>The meaning of the work done in terms of the semester OKRs.</li> </ul> </li> <li>Create the report of the proposed next sprint's planning, with     arguments behind why we do each task.</li> </ul> </li> <li>Update the planning with the requirements of the stakeholders.</li> <li>Upload the review reports to the documentation repository.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the refinement meetings:</p> <ul> <li>Prepare the tasks that need to be refined:<ul> <li>Adjust the priority of the backlog tasks.</li> <li>Select the tasks that are most probably going to enter the next     sprint.</li> <li>Expand the description of those tasks so it's understandable by any     team member.</li> <li>If the task need some steps to be done before it can be worked upon,     do them or create a task to do them before the original task.</li> </ul> </li> <li>Create the required refinement calendar events inviting the members of the     scrum team.</li> <li>Conduct the refinement meeting.</li> <li>Update the tasks with the outcome of the meeting.</li> <li>Prepare the next sprint's Kanban board.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the retro meeting:</p> <ul> <li>Prepare the dynamics of the meeting.</li> <li>Create the retro calendar event inviting the members of the scrum team.</li> <li>Conduct the retro meeting.</li> <li>Update the tasks with the outcome of the meeting.</li> <li>Upload the retro reports to the documentation repository.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the planning meeting:</p> <ul> <li>Make sure that you've done the required refinement sessions to have the     tasks and Kanban board ready for the next sprint.</li> <li>Create the planning calendar event inviting the members of the scrum team.</li> <li>Conduct the planning meeting.</li> <li>Update the tasks with the outcome of the meeting and start the sprint.</li> </ul> </li> </ul>"}, {"location": "scrum/#developer", "title": "Developer", "text": "<p>Developers are the people in the scrum team that are committed to creating any aspect of a usable increment each sprint.</p> <p>It's roles are:</p> <ul> <li>Attend the daily, refinement, review, retro and planning meetings.</li> <li>Focus on completing the assigned sprint tasks.<ul> <li>Do the required work or be responsible to coordinate the work that others     do for the task to be complete.</li> <li>Make sure that the Definition of Done is met     before closing the task.</li> </ul> </li> </ul>"}, {"location": "scrum/#inter-team-workflow", "title": "Inter team workflow", "text": "<p>To improve the communication between the teams, you can:</p> <ul> <li>Present more clearly the team objectives and reasons behind our tasks, and     make the rest of the teams part of the decision making.</li> <li>Be aware of the other team's needs and tasks.</li> </ul> <p>To solve the first point, you can offer the rest of the teams different solutions depending the time they want to invest in staying informed:</p> <ul> <li>You can invite the other team members to the sprint reviews, where you show the     sprint's work and present what you plan to do in the next sprint. This could     be the best way to stay informed, as you'll try to sum up everything they     need to know in the shortest time.</li> <li>For those that want to be more involved with the decision making inside the     team, they could be invited to the planning sessions and even the     refinement ones where they are involved.</li> <li>For those that don't want to attend the review, they can either get a summary     from other members of their team that did attend, or they can read the     meeting notes that you publish after each one.</li> </ul> <p>The second point means that your team members become more involved in the other team's work. The different levels of involvement are linked to the amount of time invested and the quality of the interaction.</p> <p>The highest level of involvement would be that a member of your team is also part of the other team. This is easier for those teams that already use Scrum as their agile framework, that means:</p> <ul> <li>Attending the team's meetings (retro, review, planning and refinement).</li> <li>Inform the rest of your team of the outcomes of those meetings in the     daily meeting.</li> <li>Focus on doing that team's sprint tasks.</li> <li>Populate and refine the tasks related to your team in the other team issue     tracker.</li> </ul> <p>For those teams that are smaller or don't use Scrum as their agile framework, a your team members could accompany them by:</p> <ul> <li>Setting periodic meetings (weekly/biweekly/monthly) to discuss what are they     doing, what do they plan to do and how.</li> <li>Create the team related tasks in your backlog, coordinating with the scrum     master to refine and prioritize them.</li> </ul>"}, {"location": "scrum/#definitions", "title": "Definitions", "text": ""}, {"location": "scrum/#definition-of-ready", "title": "Definition of Ready", "text": "<p>The Definition of Ready (DoR) is a list of criteria which must be met before any task can be added to a sprint. It is agreed by the whole scrum team and reviewed in the planning sessions.</p>"}, {"location": "scrum/#expected-benefits", "title": "Expected Benefits", "text": "<ul> <li>Avoids beginning work on features that do not have clearly defined completion     criteria, which usually translates into costly back-and-forth discussion or     rework.</li> <li>Provides the team with an explicit agreement allowing it to \u201cpush back\u201d on     accepting ill-defined features to work on.</li> <li>The Definition of Ready provides a checklist which usefully guides     pre-implementation activities: discussion, estimation, design.</li> </ul>"}, {"location": "scrum/#example-of-a-definition-of-ready", "title": "Example of a Definition of Ready", "text": "<p>A task needs to meet the following criteria before being added to a sprint.</p> <ul> <li>Have a short title that summarizes the goal of the task.</li> <li>Have a description clear enough so any team member can understand why we     need to do the task</li> <li>Have a validation criteria for the task to be done</li> <li>Have a checklist of steps required to meet the validation criteria, clear     enough so that any team member can understand them.</li> <li>Have a scope that can be met in one sprint.</li> <li>Have the <code>Priority:</code> label set.</li> <li>If other teams are involved in the task, add the <code>Team:</code> labels.</li> <li>If it's associated to an OKR set the <code>OKR:</code> label.</li> </ul>"}, {"location": "scrum/#definition-of-done", "title": "Definition of Done", "text": "<p>The Definition of Done (DoD) is a list of criteria which must be met before any task can be closed. It is agreed by the whole scrum team and reviewed in the planning sessions.</p>"}, {"location": "scrum/#expected-benefits_1", "title": "Expected Benefits", "text": "<ul> <li>The Definition of Done limits the cost of rework once a feature has been     accepted as \u201cdone\u201d.</li> <li>Having an explicit contract limits the risk of misunderstanding and conflict     between the development team and the customer or product owner.</li> </ul>"}, {"location": "scrum/#common-pitfalls", "title": "Common Pitfalls", "text": "<ul> <li>Obsessing over the list of criteria can be counter-productive; the list needs     to define the minimum work generally required to get a product increment to     the \u201cdone\u201d state.</li> <li>Individual features or user stories may have specific \u201cdone\u201d criteria in     addition to the ones that apply to work in general.</li> <li>If the definition of done is merely a shared understanding, rather than     spelled out and displayed on a wall, it may lose much of its effectiveness;     a good part of its value lies in being an explicit contract known to all     members of the team.</li> </ul>"}, {"location": "scrum/#example-of-a-definition-of-done", "title": "Example of a Definition of Done", "text": "<p>A task needs to meet the following criteria before being closed.</p> <ul> <li> All changes must be documented.</li> <li> All related pull requests must be merged.</li> </ul>"}, {"location": "selenium/", "title": "Selenium", "text": "<p>Selenium is a portable framework for testing web applications. It also provides a test domain-specific language (Selenese) to write tests in a number of popular programming languages.</p>"}, {"location": "selenium/#web-driver-backends", "title": "Web driver backends", "text": "<p>Selenium can be used with many browsers, such as Firefox, Chrome or PhantomJS. But first, install <code>selenium</code>:</p> <pre><code>pip install selenium\n</code></pre>"}, {"location": "selenium/#firefox", "title": "Firefox", "text": "<p>Assuming you've got firefox already installed, you need to download the geckodriver, unpack the tar and add the <code>geckodriver</code> binary somewhere in your <code>PATH</code>.</p> <pre><code>from selenium import webdriver\n\ndriver = webdriver.Firefox()\n\ndriver.get(\"https://duckduckgo.com/\")\n</code></pre> <p>If you need to get the status code of the requests use Chrome instead</p> <p>There is an issue with Firefox that doesn't support this feature.</p>"}, {"location": "selenium/#chrome", "title": "Chrome", "text": "<p>We're going to use Chromium instead of Chrome. Download the chromedriver of the same version as your Chromium, unpack the tar and add the <code>chromedriver</code> binary somewhere in your <code>PATH</code>.</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\nopts = Options()\nopts.binary_location = '/usr/bin/chromium'\ndriver = webdriver.Chrome(options=opts)\n\ndriver.get(\"https://duckduckgo.com/\")\n</code></pre> <p>If you don't want to see the browser, you can run it in headless mode adding the next line when defining the <code>options</code>:</p> <pre><code>opts.add_argument(\"--headless\")\n</code></pre>"}, {"location": "selenium/#phantomjs", "title": "PhantomJS", "text": "<p>PhantomJS is abandoned -&gt; Don't use it</p> <p>The development stopped in 2018</p> <p>PhantomJS is a headless Webkit, in conjunction with Selenium WebDriver, it can be used to run tests directly from the command line. Since PhantomJS eliminates the need for a graphical browser, tests run much faster.</p> <p>Don't install phantomjs from the official repos as it's not a working release -.-. <code>npm install -g phantomjs</code> didn't work either. I had to download the tar from the downloads page, which didn't work either. The project is abandoned, so don't use this.</p>"}, {"location": "selenium/#usage", "title": "Usage", "text": "<p>Assuming that you've got a configured <code>driver</code>, to get the url you're in after javascript has done it's magic use the <code>driver.current_url</code> method. To return the HTML of the page use <code>driver.page_source</code>.</p>"}, {"location": "selenium/#open-a-url", "title": "Open a URL", "text": "<pre><code>driver.get(\"https://duckduckgo.com/\")\n</code></pre>"}, {"location": "selenium/#get-page-source", "title": "Get page source", "text": "<pre><code>driver.page_source\n</code></pre>"}, {"location": "selenium/#get-current-url", "title": "Get current url", "text": "<pre><code>driver.current_url\n</code></pre>"}, {"location": "selenium/#click-on-element", "title": "Click on element", "text": "<p>Once you've opened the page you want to interact with <code>driver.get()</code>, you need to get the Xpath of the element to click on. You can do that by using your browser inspector, to select the element, and once on the code if you right click there is a \"Copy XPath\"</p> <p>Once that is done you should have something like this when you paste it down.</p> <pre><code>//*[@id=\u201dreact-root\u201d]/section/main/article/div[2]/div[2]/p/a\n</code></pre> <p>Similarly it is the same process for the input fields for username, password, and login button.</p> <p>We can go ahead and do that on the current page. We can store these xpaths as strings in our code to make it readable.</p> <p>We should have three xpaths from this page and one from the initial login.</p> <pre><code>first_login = '//*[@id=\u201dreact-root\u201d]/section/main/article/div[2]/div[2]/p/a'\nusername_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[2]/div/label/input'\npassword_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input'\nlogin_submit = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]/button/div'\n</code></pre> <p>Now that we have the xpaths defined we can now tell Selenium webdriver to click and send some keys over for the input fields.</p> <pre><code>from selenium.webdriver.common.by import By\n\ndriver.find_element(By.XPATH, first_login).click()\ndriver.find_element(By.XPATH, username_input).send_keys(\"username\")\ndriver.find_element(By.XPATH, password_input).send_keys(\"password\")\ndriver.find_element(By.XPATH, login_submit).click()\n</code></pre> <p>Note</p> <p>Many pages suggest to use methods like <code>find_element_by_name</code>, <code>find_element_by_xpath</code> or <code>find_element_by_id</code>. These are deprecated now. You should use <code>find_element(By.</code> instead. So, instead of:</p> <pre><code>driver.find_element_by_xpath(\"your_xpath\")\n</code></pre> <p>It should be now:</p> <pre><code>driver.find_element(By.XPATH, \"your_xpath\")\n</code></pre> <p>Where <code>By</code> is imported with <code>from selenium.webdriver.common.by import By</code>.</p>"}, {"location": "selenium/#close-the-browser", "title": "Close the browser", "text": "<pre><code>driver.close()\n</code></pre>"}, {"location": "selenium/#change-browser-configuration", "title": "Change browser configuration", "text": "<p>You can pass <code>options</code> to the initialization of the chromedriver to tweak how does the browser behave. To get a list of the actual <code>prefs</code> you can go to <code>chrome://prefs-internals</code>, there you can get the code you need to tweak.</p>"}, {"location": "selenium/#disable-loading-of-images", "title": "Disable loading of images", "text": "<pre><code>options = ChromeOptions()\noptions.add_experimental_option(\n    \"prefs\",\n    {\n        \"profile.default_content_setting_values.images\": 2,\n        \"profile.default_content_setting_values.cookies\": 2,\n    },\n)\n</code></pre>"}, {"location": "selenium/#disable-site-cookies", "title": "Disable site cookies", "text": "<pre><code>options = ChromeOptions()\noptions.add_experimental_option(\n    \"prefs\",\n    {\n        \"profile.default_content_setting_values.cookies\": 2,\n    },\n)\n</code></pre>"}, {"location": "selenium/#bypass-selenium-detectors", "title": "Bypass Selenium detectors", "text": "<p>Sometimes web servers react differently if they notice that you're using selenium. Browsers can be detected through different ways and some commonly used mechanisms are as follows:</p> <ul> <li>Implementing captcha / recaptcha to detect the automatic bots.</li> <li>Non-human behaviour (browsing too fast, not scrolling to the visible elements,     ...)</li> <li>Using an IP that's flagged as suspicious (VPN, VPS, Tor...)</li> <li>Detecting the term HeadlessChrome within headless Chrome UserAgent</li> <li>Using Bot Management service from Distil     Networks,     Akamai,     Datadome.</li> </ul> <p>They do it through different mechanisms:</p> <ul> <li>Use undetected-chromedriver</li> <li>Use Selenium stealth</li> <li>Rotate the user agent</li> <li>Changing browser properties</li> <li>Predefined Javascript variables</li> <li>Don't use selenium</li> </ul> <p>If you've already been detected, you might get blocked for a plethora of other reasons even after using these methods. So you may have to try accessing the site that was detecting you using a VPN, different user-agent, etc.</p>"}, {"location": "selenium/#use-undetected-chromedriver", "title": "Use undetected-chromedriver", "text": "<p><code>undetected-chromedriver</code> is a python library that uses an optimized Selenium Chromedriver patch which does not trigger anti-bot services like Distill Network / Imperva / DataDome / Botprotect.io Automatically downloads the driver binary and patches it.</p>"}, {"location": "selenium/#installation", "title": "Installation", "text": "<pre><code>pip install undetected-chromedriver\n</code></pre>"}, {"location": "selenium/#usage_1", "title": "Usage", "text": "<pre><code>import undetected_chromedriver.v2 as uc\ndriver = uc.Chrome()\ndriver.get('https://nowsecure.nl')  # my own test test site with max anti-bot protection\n</code></pre> <p>If you want to specify the path to the browser use <code>uc.Chrome(browser_executable_path=\"/path/to/your/file\")</code>.</p>"}, {"location": "selenium/#use-selenium-stealth", "title": "Use Selenium Stealth", "text": "<p><code>selenium-stealth</code> is a python package to prevent detection (by doing most of the steps of this guide) by making selenium more stealthy.</p> <p>Note</p> <p>It's less maintained than <code>undetected-chromedriver</code> so I'd use that other instead. I leave the section in case it's helpful if the other fails for you.</p>"}, {"location": "selenium/#installation_1", "title": "Installation", "text": "<pre><code>pip install selenium-stealth\n</code></pre>"}, {"location": "selenium/#usage_2", "title": "Usage", "text": "<pre><code>from selenium import webdriver\nfrom selenium_stealth import stealth\nimport time\n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"start-maximized\")\n\n# options.add_argument(\"--headless\")\n\noptions.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\noptions.add_experimental_option('useAutomationExtension', False)\ndriver = webdriver.Chrome(options=options, executable_path=r\"C:\\Users\\DIPRAJ\\Programming\\adclick_bot\\chromedriver.exe\")\n\nstealth(driver,\n        languages=[\"en-US\", \"en\"],\n        vendor=\"Google Inc.\",\n        platform=\"Win32\",\n        webgl_vendor=\"Intel Inc.\",\n        renderer=\"Intel Iris OpenGL Engine\",\n        fix_hairline=True,\n        )\n\nurl = \"https://bot.sannysoft.com/\"\ndriver.get(url)\ntime.sleep(5)\ndriver.quit()\n</code></pre> <p>You can test it with antibot.</p>"}, {"location": "selenium/#rotate-the-user-agent", "title": "Rotate the user agent", "text": "<p>Rotating the UserAgent in every execution of your Test Suite using <code>fake_useragent</code> module as follows:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom fake_useragent import UserAgent\n\noptions = Options()\nua = UserAgent()\nuserAgent = ua.random\nprint(userAgent)\noptions.add_argument(f'user-agent={userAgent}')\ndriver = webdriver.Chrome(chrome_options=options)\ndriver.get(\"https://www.google.co.in\")\ndriver.quit()\n</code></pre> <p>You can also rotate it with <code>execute_cdp_cmd</code>:</p> <pre><code>from selenium import webdriver\n\ndriver = webdriver.Chrome(executable_path=r'C:\\WebDrivers\\chromedriver.exe')\nprint(driver.execute_script(\"return navigator.userAgent;\"))\n# Setting user agent as Chrome/83.0.4103.97\ndriver.execute_cdp_cmd('Network.setUserAgentOverride', {\"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'})\nprint(driver.execute_script(\"return navigator.userAgent;\"))\n</code></pre>"}, {"location": "selenium/#changing-browser-properties", "title": "Changing browser properties", "text": "<ul> <li> <p>Changing the property value of navigator for webdriver to undefined as follows:</p> <pre><code>driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n  \"source\": \"\"\"\n    Object.defineProperty(navigator, 'webdriver', {\n      get: () =&gt; undefined\n    })\n  \"\"\"\n})\n</code></pre> <p>You can find a relevant detailed discussion in Selenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection</p> </li> <li> <p>Changing the values of navigator.plugins, navigator.languages, WebGL, hairline feature, missing image, etc.     You can find a relevant detailed discussion in Is there a version of     selenium webdriver that is not detectable?</p> </li> <li> <p>Changing the conventional Viewport</p> <p>You can find a relevant detailed discussion in How to bypass Google captcha with Selenium and python?</p> </li> </ul>"}, {"location": "selenium/#predefined-javascript-variables", "title": "Predefined Javascript variables", "text": "<p>One way of detecting Selenium is by checking for predefined JavaScript variables which appear when running with Selenium. The bot detection scripts usually look anything containing word <code>selenium</code>, <code>webdriver</code> in any of the variables (on window object), and also document variables called <code>$cdc_</code> and <code>$wdc_</code>. Of course, all of this depends on which browser you are on. All the different browsers expose different things.</p> <p>In Chrome, what people had to do was to ensure that <code>$cdc_</code> didn't exist as a document variable.</p> <p>You don't need to go compile the <code>chromedriver</code> yourself, if you open the file with <code>vim</code> and execute <code>:%s/cdc_/dog_/g</code> where <code>dog</code> can be any three characters that will work. With perl you can achieve the same result with:</p> <pre><code>perl -pi -e 's/cdc_/dog_/g' /path/to/chromedriver\n</code></pre>"}, {"location": "selenium/#dont-use-selenium", "title": "Don't use selenium", "text": "<p>Even with <code>undetected-chromedriver</code>, sometimes servers are able to detect that you're using selenium.</p> <p>A uglier but maybe efective way to go is not using selenium and do a combination of working directly with the chrome devtools protocol with <code>pycdp</code> (using this maintained fork) and doing the clicks with <code>pyautogui</code>. See an example on this answer.</p> <p>Keep in mind though that these tools don't look to be actively maintained, and that the approach is quite brittle to site changes. Is there really not other way to achieve what you want?</p>"}, {"location": "selenium/#set-timeout-of-a-response", "title": "Set timeout of a response", "text": "<p>For Firefox and Chromedriver:</p> <pre><code>driver.set_page_load_timeout(30)\n</code></pre> <p>The rest:</p> <pre><code>driver.implicitly_wait(30)\n</code></pre> <p>This will throw a <code>TimeoutException</code> whenever the page load takes more than 30 seconds.</p>"}, {"location": "selenium/#get-the-status-code-of-a-response", "title": "Get the status code of a response", "text": "<p>Surprisingly this is not as easy as with requests, there is no <code>status_code</code> method on the driver, you need to dive into the browser log to get it. Firefox has an open issue since 2016 that prevents you from getting this information. Use Chromium if you need this functionality.</p> <pre><code>from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\ncapabilities = DesiredCapabilities.CHROME.copy()\ncapabilities['goog:loggingPrefs'] = {'performance': 'ALL'}\n\ndriver = webdriver.Chrome(desired_capabilities=capabilities)\n\ndriver.get(\"https://duckduckgo.com/\")\nlogs = driver.get_log(\"performance\")\nstatus_code = get_status(driver.current_url, logs)\n</code></pre> <p>Where <code>get_status</code> is:</p> <pre><code>def get_status(url: str, logs: List[Dict[str, Any]]) -&gt; int:\n    \"\"\"Get the url response status code.\n\n    Args:\n        url: url to search\n        logs: Browser driver logs\n    Returns:\n        The status code.\n    \"\"\"\n    for log in logs:\n        if log[\"message\"]:\n            data = json.loads(log[\"message\"])\n            with suppress(KeyError):\n                if data[\"message\"][\"params\"][\"response\"][\"url\"] == url:\n                    return data[\"message\"][\"params\"][\"response\"][\"status\"]\n    raise ValueError(f\"Error retrieving the status code for url {url}\")\n</code></pre> <p>You have to use <code>driver.current_url</code> to handle well urls that redirect to other urls.</p> <p>If your url is not catched and you get a <code>ValueError</code>, use the next snippet inside the <code>with suppress(KeyError)</code> statement.</p> <p><pre><code>content_type = (\n    \"text/html\"\n    in data[\"message\"][\"params\"][\"response\"][\"headers\"][\"content-type\"]\n)\nresponse_received = (\n    data[\"message\"][\"method\"] == \"Network.responseReceived\"\n)\nif content_type and response_received:\n    __import__(\"pdb\").set_trace()  # XXX BREAKPOINT\n    pass\n</code></pre> And try to see why <code>url != data[\"message\"][\"params\"][\"response\"][\"url\"]</code>. Sometimes servers redirect the user to a url without the <code>www.</code>.</p>"}, {"location": "selenium/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "selenium/#chromedriver-hangs-up-unexpectedly", "title": "Chromedriver hangs up unexpectedly", "text": "<p>Some say that adding the <code>DBUS_SESSION_BUS_ADDRESS</code> environmental variable fixes it:</p> <pre><code>os.environ[\"DBUS_SESSION_BUS_ADDRESS\"] = \"/dev/null\"\n</code></pre> <p>But it still hangs for me. Right now the only solution I see is to assume it's going to hang and add functionality in your program to resume the work instead of starting from scratch. Ugly I know...</p>"}, {"location": "selenium/#issues", "title": "Issues", "text": "<ul> <li>Firefox driver doesn't have access to the     log: Update the section     above and start using Firefox instead of Chrome when you need to get the     status code of the responses.</li> </ul>"}, {"location": "semantic_versioning/", "title": "Semantic Versioning", "text": "<p>Semantic Versioning is a way to define your program's version based on the type of changes you've introduced. It's defined as a three-number string (separated with a period) in the format of <code>MAJOR.MINOR.PATCH</code>.</p> <p>Usually, it starts with 0.0.0. Then depending on the type of change you make to the library, you increment one of these and set subsequent numbers to zero:</p> <ul> <li><code>MAJOR</code> version if you make backward-incompatible changes.</li> <li><code>MINOR</code> version if you add a new feature.</li> <li><code>PATCH</code> version if you fix bugs.</li> </ul> <p>The version number in this context is used as a contract between the library developer and the systems pulling it in about how freely they can upgrade. For example, if you wrote your web server against <code>Django 3</code>, you should be good to go with all <code>Django 3</code> releases that are at least as new as your current one. This allows you to express your Django dependency in the format of <code>Django &gt;= 3.0.2, &lt;4</code>.</p> <p>In addition, we have to take into account the following considerations:</p> <ul> <li>A normal version number MUST take the form X.Y.Z where X, Y, and Z are   non-negative integers, and MUST NOT contain leading zeroes.</li> <li>Once a versioned package has been released, the contents of that version MUST   NOT be modified. Any modifications MUST be released as a new version.</li> <li>Major version zero (0.y.z) is for initial development. Anything may change at   any time. The public API should not be considered stable. But don't fall into   using ZeroVer instead.</li> <li>Releasing the version 1.0.0 is a declaration of intentions to your users that     the code is to be considered stable.</li> <li>Patch version Z (x.y.Z | x &gt; 0) MUST be incremented if only backwards   compatible bug fixes are introduced. A bug fix is defined as an internal   change that fixes incorrect behavior.</li> <li>Minor version Y (x.Y.z | x &gt; 0) MUST be incremented if new, backwards   compatible functionality is introduced to the public API. It MUST be   incremented if any public API functionality is marked as deprecated. It MAY be   incremented if substantial new functionality or improvements are introduced   within the private code. It MAY include patch level changes. Patch version   MUST be reset to 0 when minor version is incremented.</li> <li>Major version X (X.y.z | X &gt; 0) MUST be incremented if any backwards   incompatible changes are introduced to the public API. It MAY include minor   and patch level changes. Patch and minor version MUST be reset to 0 when major   version is incremented.</li> </ul> <p>!!! note \"Encoding this information in the version is just an extremely lossy, but very fast to parse and interpret, which may lead into issues</p> <p>By using this format whenever you rebuild your application, you\u2019ll automatically pull in any new feature/bugfix/security releases of Django, enabling you to use the latest and best version that still in theory guarantees to works with your project.</p> <p>This is great because:</p> <ul> <li>You enable automatic, compatible security fixes.</li> <li>It automatically pulls in bug fixes on the library side.</li> <li>Your application will keep building and working in the future as it did today     because the significant version pin protects you from pulling in versions     whose API would not match.</li> </ul>"}, {"location": "semantic_versioning/#commit-message-guidelines", "title": "Commit message guidelines", "text": "<p>If you like the idea behind Semantic Versioning, it makes sense to follow the Angular commit convention to automate the changelog maintenance and the program version bumping.</p> <p>Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;BLANK LINE&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\n</code></pre> <p>The header is mandatory and the scope of the header is optional.</p> <p>Any line of the commit message cannot be longer 100 characters.</p> <p>The footer could contain a closing reference to an issue.</p> <p>Samples:</p> <pre><code>docs(changelog): update changelog to beta.5\n\nfix(release): need to depend on latest rxjs and zone.js\n\nThe version in our package.json gets copied to the one we publish, and users need the latest of these.\n\ndocs(router): fix typo 'containa' to 'contains' (#36764)\n\nCloses #36763\n\nPR Close #36764\n</code></pre>"}, {"location": "semantic_versioning/#change-types", "title": "Change types", "text": "<p>Must be one of the following:</p> <ul> <li><code>feat</code>: A new feature.</li> <li><code>fix</code>: A bug fix.</li> <li><code>test</code>: Adding missing tests or correcting existing tests.</li> <li><code>docs</code>: Documentation changes.</li> <li><code>chore</code>: A package maintenance change such as updating the requirements.</li> <li><code>bump</code>: A commit to mark the increase of the version number.</li> <li><code>style</code>: Changes that do not affect the meaning of the code (white-space,     formatting, missing semi-colons, etc).</li> <li><code>ci</code>: Changes to our CI configuration files and scripts.</li> <li><code>perf</code>: A code change that improves performance.</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature.</li> <li><code>build</code>: Changes that affect the build system or external dependencies.</li> </ul>"}, {"location": "semantic_versioning/#subject", "title": "Subject", "text": "<p>The subject contains a succinct description of the change:</p> <ul> <li>Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\".</li> <li>Don't capitalize the first letter.</li> <li>No dot (.) at the end.</li> </ul>"}, {"location": "semantic_versioning/#body", "title": "Body", "text": "<p>Same as in the subject, use the imperative present tense. The body should include the motivation for the change and contrast this with previous behavior.</p>"}, {"location": "semantic_versioning/#footer", "title": "Footer", "text": "<p>The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes.</p> <p>Breaking Changes should start with the word <code>BREAKING CHANGE:</code> with a space or two newlines. The rest of the commit message is then used for this.</p>"}, {"location": "semantic_versioning/#revert", "title": "Revert", "text": "<p>If the commit reverts a previous commit, it should begin with <code>revert:</code> , followed by the header of the reverted commit. In the body it should say: <code>This reverts commit &lt;hash&gt;.</code>, where the hash is the SHA of the commit to revert.</p>"}, {"location": "semantic_versioning/#helpers", "title": "Helpers", "text": ""}, {"location": "semantic_versioning/#use-tool-to-bump-your-program-version", "title": "Use tool to bump your program version", "text": "<p>You can use the commitizen tool to:</p> <ul> <li>Automatically detect which type of change you're introducing and decide     which should be the next version number.</li> <li>Update the changelog</li> </ul> <p>By running <code>cz bump --changelog --no-verify</code>.</p> <p>The <code>--no-verify</code> part is required if you use pre-commit hooks.</p> <p>Whenever you want to release <code>1.0.0</code>, use <code>cz bump --changelog --no-verify --increment MAJOR</code>. If you are on a version <code>0.X.Y</code>, and you introduced a breaking change but don't want to upgrade to <code>1.0.0</code>, use the <code>--increment MINOR</code> flag.</p>"}, {"location": "semantic_versioning/#use-tool-to-create-the-commit-messages", "title": "Use tool to create the commit messages", "text": "<p>To get used to make correct commit messages, you can use the commitizen tool, that guides you through the steps of making a good commit message. Once you're used to the system though, it makes more sense to ditch the tool and write the messages yourself.</p> <p>In Vim, if you're using Vim fugitive you can change the configuration to:</p> <pre><code>nnoremap &lt;leader&gt;gc :terminal cz c&lt;CR&gt;\nnnoremap &lt;leader&gt;gr :terminal cz c --retry&lt;CR&gt;\n\n\" Open terminal mode in insert mode\nif has('nvim')\n    autocmd TermOpen term://* startinsert\nendif\nautocmd BufLeave term://* stopinsert\n</code></pre> <p>If some pre-commit hook fails, make the changes and then use <code>&lt;leader&gt;gr</code> to repeat the same commit message.</p>"}, {"location": "semantic_versioning/#pre-commit", "title": "Pre-commit", "text": "<p>To ensure that your project follows these guidelines, add the following to your pre-commit configuration:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>- repo: https://github.com/commitizen-tools/commitizen\n  rev: master\n  hooks:\n    - id: commitizen\n      stages: [commit-msg]\n</code></pre>"}, {"location": "semantic_versioning/#when-to-do-a-major-release", "title": "When to do a major release", "text": "<p>Following the Semantic Versioning idea of a major update is problematic because:</p> <ul> <li>You can quickly get into the high version number problem.</li> <li>The fact that any change may break the users     code makes the definition of when a change should be major     blurry.</li> <li>Often the change that triggered the major change only affects a low percentage     of your users (usually those using that one feature you changed in an     incompatible fashion).</li> </ul> <p>Does dropping Python 2 require a major release? Many (most) packages did this, but the general answer is ironically no, it is not an addition or a breaking change, the version solver will ensure the correct version is used (unless the <code>Requires-Python</code> metadata slot is empty or not updated).</p> <p>If you mark a feature as deprecated (almost always in a minor release), you can remove that feature in a future minor release. You have to define in your library documentation what the deprecation period is. For example, NumPy and Python use three minor releases. Sometimes is useful to implement deprecations based on a period of time. SemVer purists argue that this makes minor releases into major releases, but as we've seen it\u2019s not that simple. The deprecation period ensures the \u201cnext\u201d version works, which is really useful, and usually gives you time to adjust before the removal happens. It\u2019s a great balance for projects that are well kept up using libraries that move forward at a reasonable pace. If you make sure you can see deprecations, you will almost always work with the next several versions.</p>"}, {"location": "semantic_versioning/#semantic-versioning-system-problems", "title": "Semantic versioning system problems", "text": "<p>On paper, semantic versioning seems to be addressing all we need to encode the evolution and state of our library. When implementation time comes some issues are raised though.</p> <p>!!! note \"The pitfalls mentioned below don't invalidate the Semantic Versioning system, you just need to be aware of them.\"</p>"}, {"location": "semantic_versioning/#maintaining-different-versions", "title": "Maintaining different versions", "text": "<p>Version numbers are just a mapping of a sequence of digits to our branching strategy in source control. For instance, if you are doing SemVer then your <code>X.Y.Z</code> version maps a branch to <code>X.Y</code> branch where you're doing your current feature work, an <code>X.Y.Z+1</code> branch for any bugfixes, and potentially an <code>X+1.0.0</code> branch where you doing some crazy new stuff. So you got your next branch, main branch, and bugfix branch. And all three of those branches are alive and receiving updates.</p> <p>For projects that have those 3 kinds of branches going, the concept of SemVer makes much more sense, but how many projects are doing that? You have to be a pretty substantial project typically to have the throughput to justify that much project overhead.</p> <p>There are a lot more projects that have a single <code>bugfix</code> branch and a <code>main</code> branch which has all feature work, whether it be massively backwards-incompatible or not. In that case why carry around two version numbers? This is how you end up with ZeroVer. If you're doing that why not just drop a digit and have your version be <code>X.Y</code>? PEP 440 supports it, and it would more truthfully represent your branching strategy appropriately in your version number. However, most library maintainers/developers out there don\u2019t have enough resources to maintain even two branches.</p> <p>Maintaining a library is very time-consuming, and most libraries have just a few active maintainers available that maintain other many libraries. To complicate matters even further, for most maintainers this is not a full-time job, but something on the side, part of their free time.</p> <p>Given the scarce human resources to maintain a library, in practice, there\u2019s a single supported version for any library at any given point in time: the latest one. Any version before that (be that major, minor, patch) is in essence abandoned:</p> <ul> <li>If you want security updates, you need to move to the latest version.</li> <li>If you want a bug to be fixed, you need to move to the newest version.</li> <li>If you want a new feature, it is only going to be available in the latest     version.</li> </ul> <p>If the only maintained version is the latest, you really just have an <code>X</code> version number that is monotonically increasing. Once again PEP 440 supports it, so why not! It still communicates your branch strategy of there being only a single branch at any one time. Now I know this is a bit too unconventional for some people, and you may get into the high version number problem, then maybe it makes sense to use calendar versioning to use the version number to indicate the release date to signify just how old of a version you\u2019re using, but if stuff is working does that really matter?</p>"}, {"location": "semantic_versioning/#high-version-numbers", "title": "High version numbers", "text": "<p>Another major argument is that people inherently judge a project based on what it\u2019s version number is. They\u2019ll implicitly assume that <code>foo 2.0</code> is better than <code>bar 1.0</code> (and <code>frob 3.0</code> is better still) because the version numbers are higher. However, there is a limit to this, if you go too high too quickly, people assume your project is unstable and shouldn\u2019t really be used, even if the reason that your project is so high is because you removed some tiny edge cases that nobody actually used and didn\u2019t actually impact many people, if any, at all.</p> <p>These are two different expressions of the same thing. The first is that people will look down on a project for not having a high enough version compared to its competitors. While it\u2019s true that some people will do this, it's not a significant reason to throw away the communication benefits of your version number. Ultimately, no matter what you do, people who judge a project as inferior because of something as shallow as \u201csmaller version number\u201d will find some other, equally shallow, reason to pick between projects.</p> <p>The other side of this is a bit different. When you have a large major version, like <code>42.0.0</code>, people assume that your library is not stable and that you regularly break compatibility and if you follow SemVer strictly, it does actually mean that you regularly break compatibility.</p> <p>There are two general cases:</p> <ul> <li>The true positives: where a project that does routinely break it\u2019s public API in meaningful ways.</li> <li>The false positives: Projects that strictly follow semantic versioning were     each change which is not backwards compatible requires bumping a major     version. This means that if you remove some function that nobody actually     uses you need to increase your major version. Do it again and you need to     increase your major version again. Do this enough times, for even very small     changes and you can quickly get into a large version number <code>6</code>. This case     is a false positive for the \u201cstability\u201d test, because the reality is that     your project is actually quite stable.</li> </ul>"}, {"location": "semantic_versioning/#difference-in-change-categorization", "title": "Difference in change categorization", "text": "<p>Here's a thought experiment: you need to add a new warning to your Python package that tries to follow SemVer. Would that single change cause you to increase the major, minor, or patch version number? You might think a patch number bump since it isn't a new feature or breaking anything. You might think it's a minor version bump because it isn't exactly a bugfix. And you might think it's a major version bump because if you ran your Python code with <code>-W</code> error you suddenly introduced a new exception which could break people's code. Brett Cannon did a poll, answered by 231 people with the results:</p> <ul> <li>Patch/Bugfix: 47.2%</li> <li>Minor/enhancement: 44.2%</li> <li>Major/breaking: 8.7%</li> </ul> <p>That speaks volumes to why SemVer does not inherently work: someone's bugfix may be someone else's breaking change. Because in Python we can't statically define what an API change is there will always be a disagreement between you and your dependencies as to what a \"feature\" or \"bugfix\" truly is.</p> <p>That builds one of the arguments for CalVer. Because SemVer is imperfect at describing if a particular change will break someone upgrading the software, that we should instead throw it out and replace it with something that doesn\u2019t purport to tell us that information.</p>"}, {"location": "semantic_versioning/#unintended-changes", "title": "Unintended changes", "text": "<p>A major version bump must happen not only when you rewrite an entire library with its complete API, but also when you\u2019re just renaming a single rarely used function (which some may erroneously view as a minor change). Or even worse, it\u2019s not always clear what\u2019s part of the public API and what\u2019s not.</p> <p>You have a library with some incidental, undocumented, and unspecified behavior that you consider to be obviously not part of the public interface. You change it to solve what seems like a bug to you, and make a patch release, only to find that you have angry hordes at the gate who, thanks to Hyrum\u2019s Law, depend on the old behavior.</p> <p>With a sufficient number of users of an API, it does not matter what you promise in the contract. All observable behaviors of your system will be depended on by somebody.</p> <p>Which has been represented perfectly by the people behind xkcd.</p> <p></p> <p>While every maintainer would like to believe they\u2019ve thought of every use case up-front and created the best API for everything. In practice it's impossible to think on every impact your changes will make.</p> <p>Even if you were very diligent/broad with your interpretation to avoid accidentally breaking people with a bugfix release, bugs can still happen in a bugfix release. It obviously isn't intentional, but it does happen which means SemVer can't protect you from having to test your code to see if a patch version is compatible with your code.</p> <p>This makes \u201ctrue\u201d SemVer pointless. Minor releases are impossible, and patch releases are nearly impossible. If you fix a bug, someone could be depending on the buggy behaviour.</p>"}, {"location": "semantic_versioning/#using-zerover", "title": "Using ZeroVer", "text": "<p>ZeroVer is a joke versioning system similar to Semantic Versioning with the sole difference that <code>MAJOR</code> is always <code>0</code>. From the specification, as long as you are in the <code>0.X.Y</code> versions, you can introduce incompatible changes at any point. It intended to make fun of people who use \u201csemantic versioning\u201d but never make a <code>1.0</code> release, thus defeating the purpose of semver.</p> <p>This one of the consequences of trying to strictly follow Semantic Versioning, because once you give the leap to <code>1.0</code> you need to increase the <code>major</code> on each change quickly leading to the problem of high version numbers. The best way to fight this behaviour is to remember the often overlooked SemVer 2.0 FAQ guideline:</p> <p>If your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you\u2019re worrying a lot about backwards compatibility, you should probably already be 1.0.0.</p>"}, {"location": "semantic_versioning/#when-to-use-it", "title": "When to use it", "text": "<p>Check the Deciding what version system to use for your programs article section.</p>"}, {"location": "semantic_versioning/#references", "title": "References", "text": "<ul> <li> <p>Home</p> </li> <li> <p>Bernat post on versioning</p> </li> <li>Why I don't like SemVer anymore by Snarky</li> <li>Versioning Software by donald stufft</li> </ul>"}, {"location": "semantic_versioning/#libraries", "title": "Libraries", "text": "<p>These libraries can be used to interact with a git history of commits that follow the semantic versioning commit guidelines.</p> <ul> <li>python-semantic-release</li> </ul>"}, {"location": "signal/", "title": "Signal", "text": "<p>Signal is a cross-platform centralized encrypted messaging service developed by the Signal Technology Foundation and Signal Messenger LLC. It uses the Internet to send one-to-one and group messages, which can include files, voice notes, images and videos. It can also be used to make one-to-one and group voice and video calls.</p> <p>Signal uses standard cellular telephone numbers as identifiers and secures all communications to other Signal users with end-to-end encryption. The apps include mechanisms by which users can independently verify the identity of their contacts and the integrity of the data channel.</p> <p>Signal's software is free and open-source. Its clients are published under the GPLv3 license, while the server code is published under the AGPLv3 license. The official Android app generally uses the proprietary Google Play Services (installed on most Android devices), though it is designed to still work without them installed. Signal also has an official client app for iOS and desktop apps for Windows, MacOS and Linux.</p>"}, {"location": "signal/#pros-and-cons", "title": "Pros and cons", "text": "<p>Pros:</p> <ul> <li>Good security by default.</li> <li>Easy to use for non technical users.</li> <li>Good multi-device support.</li> </ul> <p>Cons:</p> <ul> <li>Uses phones to identify users.</li> <li>Centralized.</li> <li>Not available in     F-droid.</li> </ul>"}, {"location": "signal/#backup-extraction", "title": "Backup extraction", "text": "<p>I'd first try to use signal-black.</p>"}, {"location": "signal/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "sleep/", "title": "Sleep", "text": "<p>Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, reduced muscle activity and inhibition of nearly all voluntary muscles during rapid eye movement (REM) sleep,and reduced interactions with surroundings. Distinguished from wakefulness by a decreased ability to react to stimuli.</p> <p>Most of the content of this article is extracted from the Why we sleep book by Matthew Walker</p>"}, {"location": "sleep/#consequences-of-lack-of-sleep", "title": "Consequences of lack of sleep", "text": "<p>Sleeping less than six or seven hours a night can produce these consequences:</p> <ul> <li>Demolishing of the immune system.</li> <li>Doubling your risk of cancer.</li> <li>Is a key lifestyle factor determining and worsening the development of the Alzheimer's     disease.</li> <li>Disruption of blood sugar levels so profoundly that you would be classified as     pre-diabetic.</li> <li>Increase the likelihood of block and brittle of your coronary arteries.     Setting you on a path toward cardiovascular disease, stroke, and congestive     heart failure.</li> <li>Contributes to all major psychiatric conditions, including depression,     anxiety, and suicidality.</li> <li>Swelling concentrations of a hormone that makes you feel hungry while     suppressing a companion hormone that otherwise signals food satisfaction.</li> <li>Thwart the ability to learn and memorize.</li> </ul> <p>A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise.</p> <p>Therefore, the shorter you sleep, the shorter your life span.</p>"}, {"location": "sleep/#sleep-benefits", "title": "Sleep benefits", "text": "<p>We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep.</p> <p>Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture.</p> <p>Downstairs in the body, sleep:</p> <ul> <li>Restocks the armory of our immune system: helping fight malignancy, preventing     infection, and warding off sickness.</li> <li>Reforms the body's metabolic state by fine-tuning the balance of insulin and     circulating glucose.</li> <li>Regulates our appetite, helping control body weight through healthy food     selection rather than rash impulsivity.</li> <li>Maintains a flourishing microbiome within your gut essential to our     nutritional health being.</li> <li>Is tied to the fitness of our cardiovascular system, lowering blood pressure     while keeping our hearts in fine condition.</li> </ul> <p>Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity.</p> <p>Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day.</p>"}, {"location": "sleep/#sleep-physiological-effects", "title": "Sleep physiological effects", "text": "<p>There are two main factors that determine when you want to sleep or stay awake:</p> <ul> <li>The signal sent by the suprachiasmatic nucleus following the circadian rhythm.</li> <li>Sleep pressure: The brain builds up a chemical substance that creates     the \"sleep pressure\". The longer you've been awake, the more that chemical     sleep pressure accumulates, and consequentially, the sleepier you feel.</li> </ul>"}, {"location": "sleep/#the-circadian-rhythm", "title": "The circadian rhythm", "text": "<p>We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines:</p> <ul> <li>When you want to be awake or asleep.</li> <li>Your timed preferences for eating and drinking.</li> <li>Your moods and emotions</li> <li>The amount of urine you produce.</li> <li>Your core body temperature.</li> <li>Your metabolic rate.</li> <li>The release of numerous hormones.</li> </ul> <p>Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938:</p> <ul> <li>When cut off from the daily cycle of light and dark, the body keeps on     maintaining the rhythm.</li> <li>The period of the circadian rhythm is different for each person, but has an     average of 24 hours and 15 minutes.</li> </ul> <p>Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days.</p> <p>That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered:</p> <ul> <li>The part of the brains related to learning and memory had physically shrunk,     suggesting the destruction of brain cells caused by the biological stress of     timezone travel.</li> <li>Their short term memory was significantly impaired.</li> <li>They had far higher rates of cancer and type 2 diabetes than the general     population.</li> </ul> <p>The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people:</p> <ul> <li>Morning types: They have their peak of wakefulness early in the day and the     sleepiness early at night. They prefer to wake at or around dawn, and     function optimally at this time of day.</li> <li>Evening types: They prefer going to bed late and subsequently wake up late     the following morning, or even in the afternoon.</li> <li>In between: The remaining people fall somewhere in between, with a slight     leaning towards eveningness.</li> </ul>"}, {"location": "sleep/#melatonin", "title": "Melatonin", "text": "<p>The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself.</p> <p>Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day</p>"}, {"location": "sleep/#sleep-pressure", "title": "Sleep pressure", "text": "<p>While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake.</p>"}, {"location": "sleep/#caffeine", "title": "Caffeine", "text": "<p>You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent.</p> <p>Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body.</p> <p>An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it.</p> <p>When your liver evicts the caffeine from your system, you encounter the caffeine crash. Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again.</p> <p>For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between.</p>"}, {"location": "sleep/#relationship-between-the-circadian-rhythm-and-the-sleep-pressure", "title": "Relationship between the circadian rhythm and the sleep pressure", "text": "<p>The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned.</p> <p></p> <p>Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks.</p> <p>Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake.</p> <p>The distance between the curved lines above will be a direct reflection of your desire to sleep.</p> <p>By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep.</p> <p>During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up.</p>"}, {"location": "sleep/#all-nighters", "title": "All-nighters", "text": "<p>Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep.</p> <p>When you skip one night's sleep and remain awake throughout the following day,</p> <p></p> <p>By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake.  This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake.</p>"}, {"location": "sleep/#am-i-getting-enough-sleep", "title": "Am I getting enough sleep?", "text": "<p>When you don't sleep enough, one consequence among many is that adenosine concentrations remain too high, so the next morning you continue to accumulate sleep debt</p> <p>If after waking up you could fall asleep at ten or eleven in the morning, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation.</p> <p>Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence.</p> <p>Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder.</p>"}, {"location": "sleep/#the-sleep-cycle", "title": "The sleep cycle", "text": "<p>Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). Later, NREM sleep was further subdivided into four separate stages, named from 1 to 4 (all awful names <code>(\u0482\u2323\u0300_\u2323\u0301)</code>). Stages 3 and 4 are the deepest stages of NREM sleep, meaning that it's more difficult to wake you up in comparison with stages 1 and 2.</p> <p>In REM sleep, your eyes rapidly move from side to side underneath the lids. This movement are accompanied by active brainwaves, almost identical to those observed when you are awake. On the other hand, eyes remain still and the brainwaves also calm down in the NREM phases.</p> <p></p> <p>Even though we switch from sleep phases each 90 minutes, the ratio of NREM to REM sleep throughout the night changes across the night. In the first half of the night, the vast majority of time is spent in deep NREM and very little REM. But as we transition through the second half of the night, REM starts dominating.</p> <p>Although there is no scientific consensus, the need to remodel and update our neural circuits at night can explain this repeatable but asymmetric pattern. Throughout the day, the new memories are stored in the RAM of your brain, when you start to sleep, the brain needs to move the important ones to the hard drive, for long term retrieval. The brain needs to solve an optimization problem:</p> <ul> <li>The hard drive and the RAM have limited capacity.</li> <li>The RAM needs to be cleaned to be able to register the next day's memories.</li> <li>The brain needs RAM to do the analysis of which memories to keep and     which to remove.</li> </ul> <p>A key function of NREM sleep is to remove unnecessary neural connections, while REM sleep plays a role in strengthening those connections. The different roles and the capacity limits explains why the brain needs to switch between them. The asymmetry can be explained with the simile of creating a sculpture from a block of clay. At the beginning of the night, the long phases of NREM extensively removes unneeded material, with short REM phases to define the basic form. With each cycle, less material needs to be strongly removed and more enhancing of the details is required, thus the increase of REM sleep.</p> <p>A danger resides in this sleep profile. Since your brain desires most of its REM sleep in the last part of the night, if you wake up early, sleeping 6 hours instead of 8, you can be losing between 60 to 90% of all your REM sleep, even though you are losing 25% of your total sleep time. It works both ways, if you instead go to sleep two hours late, you'll loose a significant amount of deep NREM sleep. Preventing the brain to have the required REM or NREM daily rations results in many physical and mental issues.</p>"}, {"location": "sleep/#sleeping-time-and-sense-distortions", "title": "Sleeping time and sense distortions", "text": "<p>When you're asleep, you loose awareness of the outside world. Your ears are still hearing, your eyes, though closed, are still seeing, and the rest of the organs keep on working too.</p> <p>All these signals still flood into the center of your brain, but it's there, in the sensory convergence zone, where they end. The thalamus is the sensory gate to the brain that blocks them. If it lets them pass, they travel to the cortex at the top of your brain, where they are consciously perceived. By locking its gates shut when you're asleep, the thalamus imposes a sensory blackout in the brain. As a result, you are no longer consciously aware of the information transmitted from your sense organs.</p> <p>Another consequence of sleeping is a sense of time distortion experienced in two contradictory ways. While you loose your conscious mapping during sleep, at a non-conscious level, the brain keeps track of time with incredible precision. To distort it even more, you sense a time dilation in dreams. The signature patterns of brain-cell activity that occurs as you learn, gets recurrently repeated during sleep. That is, memories are being replayed at the level of brain-cell activity as you sleep. During REM sleep, the memories are replayed at half or quarter the speed in comparison of the activity when you're awake. This slow neural recounting may be the reason why we have that time dilation.</p>"}, {"location": "sleep/#how-your-brain-generates-sleep", "title": "How your brain generates sleep", "text": "<p>Brainwave activity of REM sleep looks similar to the one you have when you're awake. They cycle (going up and down) at a fast frequency of thirty or forty times per second in an unreliable pattern. This behaviour is explained by the fact that different parts of your waking brain are processing different pieces of information at different moments in time and in different ways.</p> <p></p>"}, {"location": "sleep/#references", "title": "References", "text": "<ul> <li>Why we sleep book by Matthew Walker</li> </ul>"}, {"location": "sponsor/", "title": "Sponsor", "text": "<p>It may arrive the moment in your life where someone wants to sponsor you. There are many sponsoring platforms you can use, each has their advantages and disadvantages.</p> <ul> <li>Liberapay.</li> <li>Ko-fi.</li> <li>Buy me a coffee.</li> <li>Github Sponsor.</li> </ul> Liberapay Ko-fi Buy Me a Coffee Github Sponsor Non-profit Yes No No No! (Microsoft!) Monthly fee No No No No Donation Commission 0% 0% 5% Not clear Paid plan No Yes No No Payment Processors Stripe, Paypal Stripe, Paypal Stripe, Standard Payout Stripe One time donations Possible but not user friendly Yes Yes Yes Membership Yes Yes Yes Yes Shop/Sales No Yes No No Based in France ? United States United States? Pay delay Instant Instant Instant Until 100$ User friendliness OK Good Good Good <p>Liberapay is the only non-profit recurrent donations platform. It's been the most recommended platform from the people I know from the open-source, activist environment.</p> <p>Ko-fi would be my next choice, as they don't do commissions on the donations and they support more features (that I don't need right now) than Liberapay.</p> <p>Each of these platforms use different payment processors such as:</p> <ul> <li>Stripe</li> <li>Paypal</li> </ul> <p>Usually Stripe takes less commissions than Paypal, also Paypal is known for closing user accounts and keeping their money.</p>"}, {"location": "sponsor/#conclusion", "title": "Conclusion", "text": "<p>If you just want something simple and don't mind the difficulties of doing one time donations of Liberapay, go with it, it's also the most ethical. If you want something more powerful, then Ko-fi is the best solution. You can even have both.</p> <p>Try to avoid Paypal and use Stripe for both platforms.</p>"}, {"location": "sponsor/#github-integration", "title": "Github integration", "text": ""}, {"location": "sponsor/#ko-fi-github-integration", "title": "Ko-fi Github integration", "text": "<ul> <li>Add <code>ko_fi: your_account_id</code> to the <code>.github/FUNDING.yml</code> file.</li> <li>Add a widget to your   <code>README.md</code>.</li> </ul>"}, {"location": "sponsor/#liberapay-github-integration", "title": "Liberapay Github integration", "text": "<ul> <li>Add <code>liberapay: your_user</code> to the <code>.github/FUNDING.yml</code> file.</li> <li>Add a widget to your <code>README.md</code>. You can get them on the Widgets section of   your settings.</li> </ul>"}, {"location": "sqlite/", "title": "SQLite", "text": "<p>SQLite is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client\u2013server database engine. Rather, it is embedded into the end program.</p> <p>SQLite is ACID-compliant and implements most of the SQL standard, generally following PostgreSQL syntax. However, SQLite uses a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity.[7] This means that one can, for example, insert a string into a column defined as an integer. SQLite will attempt to convert data between formats where appropriate, the string \"123\" into an integer in this case, but does not guarantee such conversions and will store the data as-is if such a conversion is not possible.</p> <p>SQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others.</p>"}, {"location": "sqlite/#operators-and-statements", "title": "Operators and statements", "text": ""}, {"location": "sqlite/#upsert-statements", "title": "Upsert statements", "text": "<p>UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL.</p> <p>The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT.</p> <p>If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert.</p> <p>The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers.</p> <p>Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name.</p> <pre><code>CREATE TABLE phonebook2(\n  name TEXT PRIMARY KEY,\n  phonenumber TEXT,\n  validDate DATE\n);\nINSERT INTO phonebook2(name,phonenumber,validDate)\n  VALUES('Alice','704-555-1212','2018-05-08')\n  ON CONFLICT(name) DO UPDATE SET\n    phonenumber=excluded.phonenumber,\n    validDate=excluded.validDate\n</code></pre>"}, {"location": "sqlite/#regexp", "title": "REGEXP", "text": "<p>The REGEXP operator is a special syntax for the <code>regexp()</code> user function. No <code>regexp()</code> user function is defined by default and so use of the REGEXP operator will normally result in an error message. If an application-defined SQL function named <code>regexp</code> is added at run-time, then the <code>X REGEXP Y</code> operator will be implemented as a call to <code>regexp(Y,X)</code>. If you're using sqlite3, you can check how to create the regexp function.</p>"}, {"location": "sqlite/#snippets", "title": "Snippets", "text": ""}, {"location": "sqlite/#get-the-columns-of-a-database", "title": "Get the columns of a database", "text": "<pre><code>PRAGMA table_info(table_name);\n</code></pre>"}, {"location": "sqlite/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "sqlite/#integer-autoincrement-not", "title": "[Integer autoincrement not", "text": "<p>working](https://stackoverflow.com/questions/16832401/sqlite-auto-increment-not-working)</p> <p>Rename the column type from <code>INT</code> to <code>INTEGER</code> and it starts working.</p> <p>From this:</p> <pre><code>CREATE TABLE IF NOT EXISTS foo (id INT PRIMARY KEY, bar INT)\n</code></pre> <p>to this:</p> <pre><code>CREATE TABLE IF NOT EXISTS foo (id INTEGER PRIMARY KEY, bar INT)\n</code></pre>"}, {"location": "sqlite/#references", "title": "References", "text": "<ul> <li>Home</li> <li>rqlite: is a lightweight, distributed     relational database, which uses SQLite as its storage engine. Forming     a cluster is very straightforward, it gracefully handles leader elections,     and tolerates failures of machines, including the leader.</li> </ul>"}, {"location": "sqlite3/", "title": "SQLite3", "text": "<p>SQLite3 is a python library that provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249.</p>"}, {"location": "sqlite3/#usage", "title": "Usage", "text": "<p>To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the <code>example.db</code> file:</p> <pre><code>import sqlite3\n\nconn = sqlite3.connect('example.db')\ncursor = conn.cursor()\n</code></pre> <p>Once we have a cursor we can <code>execute</code> the different SQL statements and the save them with the <code>commit</code> method of the Connection object. Finally we can close the connection with <code>close</code>.</p> <pre><code># Create table\ncursor.execute('''CREATE TABLE stocks\n             (date text, trans text, symbol text, qty real, price real)''')\n\n# Insert a row of data\ncursor.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n\n# Save (commit) the changes\nconn.commit()\n\n# We can also close the connection if we are done with it.\n# Just be sure any changes have been committed or they will be lost.\nconn.close()\n</code></pre>"}, {"location": "sqlite3/#get-columns-of-a-query", "title": "Get columns of a query", "text": "<pre><code>cursor = connection.execute('select * from bar')\nnames = [description[0] for description in cursor.description]\n</code></pre>"}, {"location": "sqlite3/#get-a-list-of-the-tables", "title": "Get a list of the tables", "text": "<pre><code>sql_query = \"\"\"SELECT name FROM sqlite_master\n  WHERE type='table';\"\"\"\ncursor = sqliteConnection.cursor()\ncursor.execute(sql_query)\nprint(cursor.fetchall())\n</code></pre>"}, {"location": "sqlite3/#regexp", "title": "Regexp", "text": "<p>SQLite needs the user to define a regexp function to be able to use the filter.</p> <pre><code>import sqlite3\nimport re\n\ndef regexp(expr, item):\n    reg = re.compile(expr)\n    return reg.search(item) is not None\n\nconn = sqlite3.connect(':memory:')\nconn.create_function(\"REGEXP\", 2, regexp)\ncursor = conn.cursor()\n</code></pre>"}, {"location": "sqlite3/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "storage/", "title": "Storage", "text": "<p>I have a server at home to host some services for my closest ones. The server is an Intel NUC which is super awesome in terms of electric consumption, CPU and RAM versus cost. The downside is that it has no hard drive to store the services data. It does have some USB ports to connect external hard drives though. As the data kept growing I started buying bigger drives. While it was affordable I purchased two so as to have one to store the backup of the data. The problem came when it became unaffordable for me. Then I took the good idea to assume that I could only have one drive of 16TB with my data. Obviously the inevitable happened. The hard drive died and those 10TB of data that were not stored in any backup were lost.</p> <p>Luckily enough, it was not unique data like personal photos. The data could be regenerated by manual processes at the cost of precious time (I'm still suffering this <code>:(</code>). But every cloud has a silver lining, this failure gave me the energy and motivation to improve my home architecture. To prevent this from happening again, the solution needs to be:</p> <ul> <li>Robust: If disks die I will have time to replace them before data is lost.</li> <li>Flexible: It needs to expand as the data grows.</li> <li>Not very expensive.</li> <li>Easy to maintain.</li> </ul> <p>There are two types of solutions to store data:</p> <ul> <li>On one host: All disks are attached to a server and the storage capacity is     shared to other devices by the local network.</li> <li>Distributed: The disks are attached to many servers and they work together to     provide the storage through the local network.</li> </ul> <p>A NAS server represents the first solution, while systems like Ceph or GlusterFS over Odroid HC4 fall into the second.</p> <p>Both are robust and flexible but I'm more inclined towards building a NAS because it can hold the amount of data that I need, it's easier to maintain and the underlying technology has been more battle proven throughout the years.</p>"}, {"location": "strategy/", "title": "Strategy", "text": "<p>Strategy is a general plan to achieve one or more long-term or overall goals under conditions of uncertainty.</p> <p>Strategy is important because the resources available to achieve goals are usually limited. Strategy generally involves setting goals and priorities, determining actions to achieve the goals, and mobilizing resources to execute the actions. A strategy describes how the ends (goals) will be achieved by the means (resources). Strategy can be intended or can emerge as a pattern of activity as the person or organization adapts to its environment.</p> <p>It typically involves two major processes:</p> <ul> <li> <p>Formulation: Involves analyzing the environment or situation, making     a diagnosis, and developing guiding policies. It includes such activities as     strategic planning and strategic     thinking.</p> </li> <li> <p>Implementation: Refers to the action plans taken to achieve the goals     established by the guiding policy.</p> </li> </ul>"}, {"location": "strategy/#strategic-planning", "title": "Strategic planning", "text": "<p>Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity.</p>"}, {"location": "strategy/#strategic-thinking", "title": "Strategic thinking", "text": "<p>Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought.</p> <p>Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today.</p> <p>The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.</p>"}, {"location": "sudokus/", "title": "Sudoku", "text": "<p>Sudoku is a logic-based, combinatorial number-placement puzzle. In classic Sudoku, the objective is to fill a 9 \u00d7 9 grid with digits so that each column, each row, and each of the nine 3 \u00d7 3 subgrids that compose the grid (also called \"boxes\", \"blocks\", or \"regions\") contain all of the digits from 1 to 9. The puzzle setter provides a partially completed grid, which for a well-posed puzzle has a single solution.</p>"}, {"location": "sudokus/#sudoku-strategies", "title": "Sudoku strategies", "text": "<ul> <li>Hidden pairs.</li> <li>Hidden triplets.</li> <li>Naked triplets.</li> </ul>"}, {"location": "tahoe/", "title": "Tahoe-LAFS", "text": "<p>Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system.</p> <p>Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result.</p> <p>The client creates more pieces (or \u201cshares\u201d) than it will eventually need, so even if some of the servers fail, you can still get your data back. Corrupt shares are detected and ignored, so the system can tolerate server-side hard-drive errors. All files are encrypted (with a unique key) before uploading, so even a malicious server operator cannot read your data. The only thing you ask of the servers is that they can (usually) provide the shares when you ask for them: you aren\u2019t relying upon them for confidentiality, integrity, or absolute availability.</p> <p>Tahoe does not provide locking of mutable files and directories. If there is more than one simultaneous attempt to change a mutable file or directory, then an <code>UncoordinatedWriteError</code> may result. This might, in rare cases, cause the file or directory contents to be accidentally deleted. The user is expected to ensure that there is at most one outstanding write or update request for a given file or directory at a time. One convenient way to accomplish this is to make a different file or directory for each person or process that wants to write.</p> <p>If mutable parts of a file store are accessed via sshfs, only a single sshfs mount should be used. There may be data loss if mutable files or directories are accessed via two sshfs mounts, or written both via sshfs and from other clients.</p>"}, {"location": "tahoe/#installation", "title": "Installation", "text": "<pre><code>apt-get install tahoe-lafs\n</code></pre> <p>Or if you want the latest version</p> <pre><code>pip install tahoe-lafs\n</code></pre> <p>If you plan to connect to servers protected through Tor, use <code>pip install tahoe-lafs[tor]</code> instead.</p>"}, {"location": "tahoe/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "tahoe/#pkg_resourcesdistributionnotfound-the-idna-distribution-was-not-found-and-is-required-by-twisted", "title": "pkg_resources.DistributionNotFound: The idna  distribution was not found and is required by Twisted", "text": "<pre><code>apt-get install python-idna\n</code></pre>"}, {"location": "tahoe/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Issues</li> </ul>"}, {"location": "talkey/", "title": "Talkey", "text": "<p>Talkey is a Simple Text-To-Speech (TTS) interface library with multi-language and multi-engine support.</p>"}, {"location": "talkey/#installation", "title": "Installation", "text": "<pre><code>pip install talkey\n</code></pre> <p>You need to install the TTS engines by yourself. Talkey supports:</p> <ul> <li>Flite</li> <li>SVOX Pico</li> <li>Festival</li> <li>eSpeak</li> <li>mbrola via eSpeak</li> </ul> <p>I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute:</p> <pre><code>sudo apt-get install libttspico-utils\n</code></pre> <p>It also supports the following networked TTS Engines:</p> <ul> <li>MaryTTS (needs hosting).</li> <li>Google TTS (cloud hosted)</li> </ul> <p>I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality.</p>"}, {"location": "talkey/#usage", "title": "Usage", "text": "<p>At its simplest use case:</p> <pre><code>import talkey\ntts = talkey.Talkey()\ntts.say(\"I've been really busy being dead. You know, after you murdered me.\")\n</code></pre> <p>It automatically detects languages without any further configuration:</p> <pre><code>tts.say(\"La cabra siempre tira al monte\")\n</code></pre>"}, {"location": "talkey/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "task_management/", "title": "Task Management", "text": "<p>Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals.</p> <p>You can address task management at different levels. High level management ensures that you choose your tasks in order to accomplish a goal, low level management helps you get things done.</p> <p>When you do task management well, you benefit from:</p> <ul> <li>Reducing your mental load, so you can use those resources doing productive     work.</li> <li>Improving your efficiency.</li> <li>Making more realistic estimations, thus meeting the commited deadlines.</li> <li>Finishing what you start.</li> <li>Knowing you're working towards your ultimate goals</li> <li>Stop feeling lost or overburdened.</li> <li>Make context switches cheaper.</li> </ul> <p>On the other side, task management done wrong can consume your willpower in the exchange of lost time and a confused mind.</p> <p>The tricky reality is that the factors that decide if you do it right or wrong are different for every person, and even for a person it may change over the time or mood states. That's why I follow the thought that task management is a tool that is meant to help you. If it's not, you need to change your system until it does.</p> <p>A side effect is that you have to tailor your task management system yourself. It doesn't matter how good the systems you find in the internet are, until you start getting your hands dirty, you won't know if they works for you. So instead of trying to discover the perfect solution, start with one that introduces the least friction in your current workflow, and evolve from that point guided by the faults you find. Forget about instant solutions, this is a never ending marathon. Make sure that each step is small and easy, otherwise you will get tired too soon.</p> <p>I haven't written a guide yet on how to give your first steps, but you could start by following a simple workflow with simple tools.</p>"}, {"location": "task_tools/", "title": "Task Management tools", "text": "<p>I currently use two tools to manage my tasks: the inbox and the task manager.</p>"}, {"location": "task_tools/#inbox", "title": "Inbox", "text": "<p>The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026</p> <p>To achieve a stress-free productivity, emptying the inbox should be a daily activity. Note that this does not mean doing things, it just means identifying things and deciding what to do with them, when you get it done, your situation is as follows:</p> <ul> <li>You have eliminated every thing you do not need.</li> <li>You have completed small actions that require no more than two minutes.</li> <li>You have delegated some actions that you do not have to do.</li> <li>You have sorted in your task manager the actions you will do     when appropriate, because they require more than 2 minutes.</li> <li>You have sorted in your task manager or calendar the tasks     that have a due date.</li> <li>There have been only a few minutes, but you feel pretty good. Everything is     where it should be.</li> </ul> <p>I've developed <code>pynbox</code> to automate the management of the inbox. Help out if you like it!</p>"}, {"location": "task_tools/#task-manager", "title": "Task manager", "text": "<p>If you've never used a task manager, start with the simplest one and see what do you feel its lacking. Choose then a better task manager based on your needs.</p> <p>In the past I've used taskwarrior, but its limitations led me to start creating pydo although it's still not a usable project :(.</p>"}, {"location": "task_tools/#the-simplest-task-manager", "title": "The simplest task manager", "text": "<p>The simplest task manager is a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable.</p> <p>When you add a new item, choose it's location relative to the existent one based on its priority. Being the top tasks are the ones that need to be done first.</p> <pre><code>* Task with a high priority\n* Task with low priority\n</code></pre> <p>The advantages of using a plaintext file over a physical notebook is that you can use your editor skills to manage the elements more efficiently. For example by reordering them or changing the description.</p>"}, {"location": "task_tools/#add-task-state-sections", "title": "Add task state sections", "text": "<p>You'll soon encounter tasks that become blocked but need your monitoring. You can add a <code># Blocked</code> section and move those tasks under it. You can optionally add the reasons why it's blocked indented below the element.</p> <pre><code>* Unblocked task\n\n# Blocked\n\n* Blocked task\n  * Waiting for Y to happen\n</code></pre>"}, {"location": "task_tools/#divide-a-task-in-small-steps", "title": "Divide a task in small steps", "text": "<p>One of the main benefits of a task manager is that you free your mind of what you need to do next, so you can focus on the task at hand. When a task is big split it in smaller doable steps that drive to its completion. If the steps are also big split them further with more indentation levels.</p> <pre><code>* Complex task\n  * Do X\n  * Do Y\n    * Do Z\n    * Do W\n</code></pre>"}, {"location": "task_tools/#web-based-task-manager", "title": "Web based task manager", "text": "<p>Life happened and the development of pydo has fallen behind in my priority list. I've also reached a point where simplest one is no longer suitable for my workflow because:</p> <ul> <li>I loose a lot of time in the reviews.</li> <li>I loose a lot of time when doing the different plannings (year, trimester,     month, week, day).</li> <li>I find it hard to organize and refine the backlog.</li> </ul> <p>As <code>pydo</code> is not ready yet and I need a solution that works today better than the simplest task manager, I've done an analysis of the state of the art of self-hosted applications of all of them the two that were more promising were Taiga and OpenProject.</p>"}, {"location": "task_tools/#taiga", "title": "Taiga", "text": "<p>An Open source project with a lot of functionality. If you want to try it, you can create an account at Disroot (an awesome collective by the way). They have set up an instance where you can check if you like it.</p> <p>Some facts made me finally not choose it, for example:</p> <ul> <li>Subtasks can't have subtasks. Something I've found myself having quite often.     Specially if you refine your tasks in great detail.</li> <li>When browsing the backlog or the boards, you can't edit a task in that window,     you need to open it in another tab.</li> <li>I don't understand very well the different components, the difference between     tasks and issues for example.</li> </ul>"}, {"location": "task_tools/#openproject", "title": "OpenProject", "text": "<p>Check the OpenProject page to see the analysis of the tool.</p> <p>In the end I went with this option.</p>"}, {"location": "task_tools/#references", "title": "References", "text": "<ul> <li>GTD time management     framework.</li> </ul>"}, {"location": "task_workflows/", "title": "Task management workflows", "text": ""}, {"location": "task_workflows/#hype-flow-versus-a-defined-plan", "title": "Hype flow versus a defined plan", "text": "<p>I've found two ways to work on my tasks: following a plan and following the hype flow.</p> <p>The first one helps you finish what you started, and directs your efforts towards big goals. The side effect is that it achieves it by setting constrains on what to do, so you sometimes end up in the position of doing tasks that you don't want to at the moment, and suppressing yourself not to do the ones that you want.</p> <p>The second one takes advantage of letting you work on wherever you want at the moment, which boosts your creativity and productivity. This way imposes less constrains on you and is more pleasant because surfing the hype is awesome. The side effect is that if you have many interests, you can move forward very quickly on many directions leaving a lot of projects half done, instead of pushing in the direction of your big goals.</p> <p>The art here is to combine both at need, if you have a good plan, you may be able to start surfing the hype, and when the time constrains start to press you, switch to a stricter plan to be able to deliver value in time. This makes more sense in work environments, at personal level I usually just surf the hype unless I have a clear objective with a due date to reach.</p>"}, {"location": "task_workflows/#planning-workflows", "title": "Planning workflows", "text": "<p>Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have:</p> <ul> <li>Task plan.</li> <li>Pomodoro.</li> <li>Day plan.</li> <li>Week plan.</li> <li>Fortnight plan.</li> <li>Month plan.</li> <li>Trimester plan.</li> <li>Year plan.</li> </ul> <p>If you're starting your task management career, start with the first level. Once you're comfortable, move one step up until you reach the sweet spot between time invested in management and the profit it returns.</p> <p>Each of the plans defined below describe the most complete process, use them as a starting point to define the plan that works for you depending on your needs and how much time you want to invest at that particular moment. Even I don't follow them strictly. As they change over time, it's useful to find a way to be able to follow them without thinking too much on what are the specific steps, for example having a checklist or a script.</p>"}, {"location": "task_workflows/#task-plan", "title": "Task plan", "text": "<p>The task plan defines the steps required to finish a task. It's your most basic roadmap to address a task, and a good starting point if you feel overwhelmed when faced with an assignment.</p> <p>When done well, you'll better understand what you need to do, it will prevent you from wasting time at dead ends as you'll think before acting, and you'll develop the invaluable skill of breaking big problems into smaller ones.</p> <p>To define a task plan, follow the next steps:</p> <ul> <li>Decide what do you want to achieve when the task is finished.</li> <li>Analyze the possible ways to arrive to that goal. Try to assess different     solutions before choosing one.</li> <li>Once you have it, split it into steps small enough to be comfortable     following them without further analysis.</li> </ul> <p>Some people define the task plan whenever they add the task to their task manager. Others prefer to save some time each month to refine the plans of the tasks to be done the next one.</p> <p>The plan is an alive document that changes each Pomodoro cycle and that you'll need to check often. It has to be accessible and it should be easy for you to edit. If you don't know where to start, use the simplest task manager.</p> <p>Try not to overplan though, if at the middle of a task you realize that the rest of the steps don't make sense, all the time invested in their definition will be lost. That's why it's a good idea to have a great detail for the first steps and gradually move to rougher definitions on later ones.</p>"}, {"location": "task_workflows/#pomodoro", "title": "Pomodoro", "text": "<p>Pomodoro is a technique used to ensure that for short periods of time, you invest all your mental resources in doing the work needed to finish a task. It's your main unit of work and a good starting point if you have concentration issues.</p> <p>When done well, you'll start moving faster on your tasks, because uninterrupted work is the most efficient. You'll also begin to know if you're drifting from your day's plan, and will have space to adapt it or the task plan to time constrains or unexpected events.</p> <p>If you don't yet have a task plan or day plan, don't worry! Ignore the steps that involve them until you do.</p> <p>The next steps define a Pomodoro cycle:</p> <ul> <li>Select the cycle time span. Either 20 minutes or until the next interruption,     whichever is shortest.</li> <li>Decide what are you going to do.</li> <li>Analyze yourself to see if you're state of mind is ready to only do that for     the chosen time span. If it's not, maybe you need to take a \"Pomodoro     break\", take 20 minutes off doing something that replenish your willpower or     the personal attribute that is preventing you to be able to work.</li> <li>Start the timer.</li> <li>Work uninterruptedly on what you've decided until the timer goes off.</li> <li>Take 20s to look away from the screen (this is good for your ejes).</li> <li>Update your task and day plans:<ul> <li>Tick off the done task steps.</li> <li>Refine the task steps that can be addressed in the next cycle.</li> <li>Check if you can still meet the day's plan.</li> </ul> </li> <li>Check the interruption channels that need to be checked each 20     minutes.</li> </ul> <p>At the fourth Pomodoro cycle, you'll have finished a Pomodoro iteration. At the end of the iteration:</p> <ul> <li>Check if you're going to meet the day plan, if you're not, change     change it or the task plan to make the time constrain.</li> <li>Get a small rest, you've earned it! Get off the chair, stretch or give a small     walk. What's important is that you take your mind off the task at hand and     let your body rest. Remember, this is a marathon, you need to take care of     yourself.</li> <li>Start a new Pomodoro iteration.</li> </ul> <p>If you're super focused at the end of a Pomodoro cycle, you can skip the task plan update until the end of the iteration.</p> <p>To make it easy to follow the pomodoro plan I use a script that:</p> <ul> <li>Uses timer to show the countdown.</li> <li>Uses safeeyes to track the eye rests.</li> <li>Asks me to follow the list of steps I've previously defined.</li> </ul>"}, {"location": "task_workflows/#day-plan", "title": "Day plan", "text": "<p>This plan defines at day level which tasks are you going to work on and schedules when are you going to address them. It's the most basic roadmap to address a group of tasks. The goal is to survive the day. It's a good starting point if you forget to do tasks that need to be done in the day or if you miss appointments.</p> <p>It's also the next step of advance awareness, if you have a day plan, on each Pomodoro iteration you'll get the feeling whether you're going to finish what you proposed yourself.</p> <p>You can make your plan at the start of the day, start by getting an idea of:</p> <ul> <li>What do you need to do by checking:<ul> <li>The last day's plan.</li> <li>Calendar events.</li> <li>The week's plan if you have it, or the prioritized list of     tasks to do.</li> </ul> </li> <li>How much uninterrupted time you have between calendar events.</li> <li>Your state of mind.</li> </ul> <p>Then create the day schedule:</p> <ul> <li>Add the calendar events.</li> <li>Add the interruption     events.</li> <li>Setup an alert for the closest calendar event.</li> </ul> <p>And the day tasks plan:</p> <ul> <li>Decide the tasks to be worked on and think when you want to do them.</li> </ul> <p>To follow it throughout the day and when it's coming to an end:</p> <ul> <li>Update your week or/and task plans to meet the time     constrains.</li> <li>Optionally sketch the next day's plan.</li> </ul> <p>When doing the plan keep in mind to minimize the number of tasks and calendar events so as not to get overwhelmed, and not to schedule a new task before you finish what you've already started. It's better to eventually fall short on tasks, than never reaching your goal.</p> <p>To make it easy to follow I use a script that:</p> <ul> <li>Asks me to check the weather forecast.</li> <li>Uses timer to show the countdown.</li> <li>Uses safeeyes to track the eye rests.</li> <li>Asks me to follow the list of steps I've previously defined.</li> </ul>"}, {"location": "task_workflows/#week-plan", "title": "Week plan", "text": "<p>The plan defines at week level which tasks are you going to work on and schedules when are you going to address them. It's the next roadmap level to address a group of tasks. The goal changes from surviving the day to start planning your life. It's a good starting point if you are comfortable working with the pomodoro, task and day plans, and want to start deciding where you're heading to.</p> <p>It's also the next step of advance awareness, if you have a week plan, each day you'll get the feeling whether you're going to finish what you proposed yourself.</p> <p>You can make your plan at the start of the week, similar to the day plan, start by getting an idea of:</p> <ul> <li>What do you need to do by:<ul> <li>Closing the last week's plan.</li> <li>Checking upcoming calendar events.</li> <li>The month's plan if you have it, or the prioritized list of     tasks to do, identifying the task dependencies that may block the task     development.</li> </ul> </li> <li>How much uninterrupted time you have between calendar events.</li> <li>Your state of mind.</li> </ul> <p>To close the last week's plan:</p> <ul> <li>Mark the plan items as done</li> <li>Get an idea of what percent of objectives you actually met. With the mindset     of seeing how much you can commit on the next one, not to think how bad you     performed, you did the best you could, and nothing else could be done.</li> <li> <p>Clean the active tasks. Throughout the week, there will be tasks that you left     unfinished. For each of them:</p> <ul> <li>Decide if the task still makes sense and if it's actionable</li> <li>Check if you can merge it with other tasks</li> <li>Check if it belongs to an active objective</li> <li>Remove the rest.</li> </ul> </li> </ul> <p>Then create the week schedule:</p> <ul> <li>Arrange or move calendar events to maximize the uninterrupted periods, then     add them to the plan.</li> <li>Add the interruption     events.</li> <li>Decide the tasks to be worked on and roughly assign them to the week days.</li> </ul> <p>Follow it throughout the week, and when it's coming to an end:</p> <ul> <li>Update your month or/and task plans to meet the time     constrains.</li> <li>Update the people that may depend on you of possible plan drifts.</li> </ul> <p>To make it easy to follow I use a script that:</p> <ul> <li>Asks me to follow the list of steps I've previously defined.</li> </ul>"}, {"location": "task_workflows/#fortnight-month-trimester-plan", "title": "Fortnight, month, trimester plan", "text": "<p>From the week plan you can increasingly think your roadmap, start with a fortnight plan, when you're comfortable go up to month and trimester plans. The idea is similar to the week plan but with less definition. You deal with bigger tasks that help shape your life in the long run.</p> <p>The process of planning and reviewing each of these should be as short as possible, otherwise you'll soon get tired of it. For example, for the month plan you can:</p> <ul> <li>Do the week plan review: Where you transfer the non planned tasks to the     month plan.</li> <li>Do the fortnight plan review</li> <li>Do the month plan review:<ul> <li>Check the objectives you had and how many you meet, adding notes on your     progress.</li> <li>Analyze what to do with the new objectives, adding them to the trimester plan</li> <li>Transfer the non planned elements to the semester plan.</li> </ul> </li> <li>Do the month's planning:<ul> <li>Review the semester plan if you have it.</li> <li>Decide which big tasks you want to work on</li> </ul> </li> <li>Do the fortnight plan</li> <li>Do the week plan</li> </ul>"}, {"location": "task_workflows/#references", "title": "References", "text": "<ul> <li>Pomodoro article.</li> </ul>"}, {"location": "teeth/", "title": "Teeth", "text": "<p>Taking good care of your teeth can be easier if you remember that each visit to the dentist is both super expensive and painful. So those 10 minutes each day are really worth it.</p>"}, {"location": "teeth/#how-to-take-care-of-your-teeth", "title": "How to take care of your teeth", "text": "TL;DR: Daily actions to keep your teeth healty <ul> <li>Brush your teeth after every meal for at least two minutes.</li> <li>Floss each day before the last teeth brush.</li> <li>Use an electric toothbrush.</li> <li>Replace the brush each three months or at first     sign of wear and tear.</li> <li>Don't eat or drink anything but water after your nightly brush.</li> <li>Do not rinse after you brush your teeth.</li> <li>Use floss instead of a toothpick.</li> <li>Use mouthwash daily but not after brushing.</li> </ul> <p>Oral hygiene is the practice of keeping one's mouth clean and free of disease and other problems (e.g. bad breath) by regular brushing of the teeth (dental hygiene) and cleaning between the teeth. It is important that oral hygiene be carried out on a regular basis to enable prevention of dental disease and bad breath. The most common types of dental disease are tooth decay (cavities, dental caries) and gum diseases, including gingivitis, and periodontitis.</p> <p>General guidelines suggest brushing twice a day: after breakfast and before going to bed, but ideally the mouth would be cleaned after every meal. Cleaning between the teeth is called interdental cleaning and is as important as tooth brushing. This is because a toothbrush cannot reach between the teeth and therefore only removes about 50% of plaque from the surface of the teeth. There are many tools to clean between the teeth, including floss and interdental brushes; it is up to each individual to choose which tool they prefer to use.</p> <p>Over 80% of cavities occur inside fissures in teeth where brushing cannot reach food left trapped after eating and saliva and fluoride have no access to neutralize acid and remineralize demineralized teeth, unlike easy-to-clean parts of the tooth, where fewer cavities occur.</p>"}, {"location": "teeth/#teeth-brushing", "title": "Teeth brushing", "text": "<p>Routine tooth brushing is the principal method of preventing many oral diseases, and perhaps the most important activity an individual can practice to reduce dental plaque and tartar.</p> <p>The dental plaque contains a mixture of bacteria, their acids and sticky byproducts and food remnants. It forms naturally on teeth immediately after you've eaten but doesn't get nasty and start to cause damage to the teeth until it reaches a certain stage of maturity. The exact amount of time this takes isn't known but is at least more than 12 hours.</p> <p>Bacteria consume sugar and, as a byproduct, produce acids which dissolve mineral out of the teeth, leaving microscopic holes we can't see. If the process isn't stopped and they aren't repaired, these can become big, visible cavities.</p> <p>So controlling plaque reduces the risk of the individual suffering from plaque-associated diseases such as gingivitis, periodontitis, and caries.</p> <p>Many oral health care professionals agree that tooth brushing should be done for a minimum of two minutes, and be practiced at least twice a day, but ideally after each meal.</p> <p>Toothbrushing can only clean to a depth of about 1.5 mm inside the gingival pockets, but a sustained regime of plaque removal above the gum line can affect the ecology of the microbes below the gums and may reduce the number of pathogens in pockets up to 5 mm in depth.</p> <p>Toothpaste (dentifrice) with fluoride, or alternatives such as nano-hydroxyapatite, is an important tool to readily use when tooth brushing. The fluoride (or alternatives) in the dentifrice is an important protective factor against caries, and an important supplement needed to remineralize already affected enamel. However, in terms of preventing gum disease, the use of toothpaste does not increase the effectiveness of the activity with respect to the amount of plaque removed. People use toothpaste with nano-hydroxyapatite instead of fluoride as it performs the same function, and some people believe fluoride in toothpaste is a neurotoxin.</p> <p>For maximum benefit, use toothpaste with 1350-1500 ppmF (that's concentration of fluoride in parts per million) to prevent tooth decay.</p> <p>At night, you produce less saliva than during the day. Because of this, your teeth have less protection from saliva and are more vulnerable to acid attacks. That's why it's important to remove food from your teeth before bed so plaque bacteria can't feast overnight. Don't eat or drink anything except water after brushing at night. This also gives fluoride the longest opportunity to work.</p>"}, {"location": "teeth/#how-to-brush-your-teeth", "title": "How to brush your teeth", "text": "<p>The procedure I'm using right now is:</p> <ul> <li>Wet the brush but don't add any toothpaste.</li> <li>Slowly and systematically guide the bristle of the electric brush from tooth     to tooth, following the contour of the gums and their crowns, remembering to     massage the gums. For example, start with the upper left part on the     outside, reach the other side of your mouth, clean the bottom of your right     side teeth, then go back on the inside of each teeth until you arrive to     your left side. Try to avoid brushing with too much force as this can     damage the surface of your teeth.</li> <li>Rinse with water.</li> <li>Place a pea sized amount of toothpaste on the brush and repeat the cycle.</li> <li>Brush your tongue.</li> <li>Spit the extra toothpaste but don't rinse or drink anything in the next 30     minutes.</li> </ul> <p>The whole process should take at least two minutes.</p>"}, {"location": "teeth/#manual-versus-electric-tooth-brush", "title": "Manual versus electric tooth brush", "text": "<p>If you want to use a manual one, Oral health professionals recommend the use of a tooth brush with a small head and soft bristles as they are most effective in removing plaque without damaging the gums.</p> <p>The technique is crucial to the effectiveness of tooth brushing and disease prevention. Back and forth brushing is not effective in removing plaque at the gum line. Tooth brushing should employ a systematic approach, angle the bristles at a 45-degree angle towards the gums, and make small circular motions at that angle.</p> <p>When using an electric one, the bristle head should be guided from tooth to tooth slowly, following the contour of the gums and crowns of the tooth. The motion of the toothbrush head removes the need to manually oscillate the brush or make circles.</p> <p>Another study suggest that the effectiveness of electric toothbrushes at reducing plaque formation and gingivitis is superior to conventional manual toothbrushes.</p> <p>Regardless of the type, you are always best using a soft-bristled toothbrush with a small head and a flexible neck because this will most effectively remove plaque and debris from your teeth, without damaging your teeth and gums and drawing blood.</p>"}, {"location": "teeth/#toothbrush-replacement", "title": "Toothbrush replacement", "text": "<p>Remember to replace your brush at the first sign of wear-and-tear or every three months, whichever comes first. Frayed or broken bristles won't clean your mouth properly. Change your toothbrush once the bristles lose their flexibility. 1</p> <p>Also, after a couple of months of daily use, bacteria and food particles begin to accumulate on the toothbrush.</p>"}, {"location": "teeth/#to-rinse-or-not-to-rinse", "title": "To rinse or not to rinse", "text": "<p>There is a lot of controversy on the topic on whether you should rinse or not your mouth after brushing your teeth. (1, 2)</p> <p>People in favor of rinsing your mouth argue that:</p> <ul> <li>You\u2019ll get rid of the excess toothpaste along with any food or bacteria that     could have been stuck in your teeth or released by the brushing itself.</li> <li>You\u2019ll also be removing the fluoride from your mouth, which if swallowed,     might upset your stomach.</li> </ul> <p>People against rinsing your mouth argue that:</p> <ul> <li>When you rinse with water, you\u2019re potentially washing away any remnants of     toothpaste, including the fluoride that makes it work. That could mean that     even though you are brushing your teeth, it might not be as effective as it     should be.</li> </ul> <p>Whilst there have been studies on the effectiveness of rinsing, the results only indicate that there COULD be an advantage of one over the other. So it's up to you to evaluate the advantages and disadvantages of each method.</p> <p>Some people are prone to cavities, or might have poor dental health. If your teeth chip, crack or break easily, it\u2019s strongly recommended that you don't rinse after you brush.  Similarly, if you consume a lot of sugar, you should probably avoid rinsing. If you don't fit into these categories, then it\u2019s really based on your own preference.</p>"}, {"location": "teeth/#keep-your-brush-away-from-your-feces", "title": "Keep your brush away from your feces", "text": "<p>As the Mythbusters showed, Fecal coliform were found on toothbrushes stored at the bathroom. And even though none were of a level high enough to be dangerous, and experts confirm that such bacteria are impossible to completely avoid, you can reduce the risk by:</p> <ul> <li>Storing the brush in a cupboard or in other room.</li> <li>Putting a lid on your toothbrush</li> <li>Closing the lid on your toilet seat before flushing.</li> </ul>"}, {"location": "teeth/#how-to-floss", "title": "How to floss", "text": "<p>Tooth brushing alone will not remove plaque from all surfaces of the tooth as 40% of the surfaces are interdental. One technique that can be used to access these areas is dental floss. When the proper technique is used, flossing can remove plaque and food particles from between the teeth and below the gums. The American Dental Association (ADA) reports that up to 80% of plaque may be removed by this method. The ADA recommends cleaning between the teeth as part of one's daily oral hygiene regime, with a different piece of floss at each flossing session.</p> <p>The correct technique to ensure maximum plaque removal is as follows: (1, 2)</p> <ul> <li>Floss length: 15\u201325 cm wrapped around middle fingers.</li> <li>For upper teeth grasp the floss with thumb and index finger, for lower teeth     with both index fingers. Ensure that a length of roughly 2.5cm is left     between the fingers.</li> <li>Ease the floss gently between the teeth using a back and forth motion. Do not     snap the floss into the gums.</li> <li>When the floss reaches your gumline, curve it into a C-shape against a tooth     until you feel resistance.</li> <li>Hold the floss against the tooth. Gently scrape the side of the tooth, moving     the floss away from the gum. Repeat on the other side of the gap, along the     side of the next tooth.</li> <li>Do not forget the back of your last tooth.</li> <li>Ensure that the floss is taken below the gum margins using a back and forth up     and down motion.</li> </ul> <p>You should floss before brushing your teeth because any food, plaque, and bacteria released by flossing are removed by the afterwards brushing.</p> <p>Another tips regarding flossing are:</p> <ul> <li>Skip the toothpick: Use floss instead of a toothpick to remove food stuck in between your teeth. Using a toothpick can damage your gums and lead to an infection.</li> <li>Be gentle: Don't be too aggressive when flossing to avoid bleeding gums.</li> </ul> <p>When you first start flossing, your gums may be tender and bleed a little. Carry on flossing your teeth and the bleeding should stop as your gums become healthier.  If you're still getting regular bleeding after a few days, see your dental team. They can check if you're flossing correctly.</p>"}, {"location": "teeth/#mouth-washing", "title": "Mouth washing", "text": "<p>Using a mouthwash that contains fluoride can help prevent tooth decay, but don't use mouthwash (even a fluoride one) straight after brushing your teeth or it'll wash away the concentrated fluoride in the toothpaste left on your teeth. [1]</p> <p>Choose a different time to use mouthwash, such as after lunch. And remember not to eat or drink for 30 minutes after using a fluoride mouthwash.</p>"}, {"location": "teeth/#do-yearly-check-ups", "title": "Do yearly check ups", "text": "<p>First find a dentist that you trust, until you do, search for second and third options before diving into anything you may regret.</p> <p>Once you have it, yearly go to their dreaded places so they can:</p> <ul> <li>Check that everything is alright.</li> <li>Do a regular clean, but beware of unnecessary deep cleaning.</li> </ul>"}, {"location": "teeth/#references", "title": "References", "text": "<ul> <li>Wikipedia oral hygiene article</li> <li>CNN health article on oral     hygiene</li> </ul>"}, {"location": "teeth_deep_cleaning/", "title": "Teeth deep cleaning", "text": "<p>TL;DR: Ask the opinion of two or three independent dentists before doing a deep clean.</p> <p>Scaling and root planing, also known as conventional periodontal therapy, non-surgical periodontal therapy or deep cleaning, is a procedure involving removal of dental plaque and calculus (scaling or debridement) and then smoothing, or planing, of the (exposed) surfaces of the roots, removing cementum or dentine that is impregnated with calculus, toxins, or microorganisms, the etiologic agents that cause inflammation. It is a part of non-surgical periodontal therapy. This helps to establish a periodontium that is in remission of periodontal disease.</p> <p>As to the frequency of cleaning, research on this matter is inconclusive. That is, it has neither been shown that more frequent cleaning leads to better outcomes nor that it does not. Thus, any general recommendation for a frequency of routine cleaning (e.g. every six months, every year) has no empirical basis. (1)</p>"}, {"location": "teeth_deep_cleaning/#why-do-we-need-deep-cleaning", "title": "Why do we need deep cleaning", "text": "<p>We all have a plethora of bacteria in our mouths. Those bacteria mix with other substances to form sticky plaque on teeth, which is mostly banished by regular brushing and flossing.</p> <p></p> <p>Plaques that don't get brushed away can harden and form a substance known as tartar, which can only be removed with a dental cleaning. When tartar remains on the teeth, it can cause inflammation of the gums, a condition called gingivitis, characterized by red swollen gums that can bleed easily. A mild form of gum disease, gingivitis can usually be reversed through regular brushing and flossing along with cleanings by a dentist or hygienist.</p> <p></p> <p>If gingivitis isn't cured, it can advance to a more severe form of gum disease called periodontitis, in which the inflamed tissue begins to pull away from the teeth, forming spaces, or pockets. As the pockets become deeper, more of the tooth below the gum line is exposed to bacteria, which can damage the bone holding teeth in place.</p> <p>Eventually, if the pockets become deep enough, teeth can become loose and may even be lost. Dentists measure the depth of the pockets with a probe that has a tiny ruler on the end. Healthy gums have pockets that measure no more than 3 mm \u2014 or a little less than a tenth of an inch \u2014 deep. More than that and you\u2019re getting into trouble.</p> <p>One way to slow or halt the process is through deep cleaning, which removes the plaque below the gum line and smooths rough spots on the tooth root, making it harder for bacteria to accumulate there.</p>"}, {"location": "teeth_deep_cleaning/#signs-of-periodontitis", "title": "Signs of periodontitis", "text": "<ul> <li>Red or swollen gums</li> <li>Tender or bleeding gums</li> <li>Persistent bad breath</li> <li>Your teeth look like they\u2019ve been getting longer as gums recede.</li> <li>Teeth that are sensitive</li> <li>Loose teeth</li> <li>Pain when chewing</li> </ul>"}, {"location": "teeth_deep_cleaning/#evidence-based-dentistry", "title": "Evidence-based dentistry", "text": "<p>Several systematic reviews have been made of the effectiveness of scaling and root planing as evidence-based dentistry. A Cochrane review by Worthington et al. in 2013 considered only scaling and polishing of the teeth, but not root planing. After examining 88 papers they found only three studies that met all their requirements, remarking that \"the quality of the evidence was generally low.\"</p> <p>An extensive review that did involve root planing was published by the Canadian Agency for Drugs and Technologies in Health in 2016. It made a number of findings, including (1) In five randomized controlled trials, scaling and root planing \"was associated with a decrease in plaque from baseline at one month, three months, or six months;\" and (2) Four studies analyzed changes in the gingival index (GI) from the baseline and \"found a significant improvement from baseline in the scaling and root planing group at three months and six months.\" This study also discussed evidence-based guidelines for frequency of scaling with and without root planing for patients both with and without chronic periodontitis. The group that produced one of the main systematic reviews used in the 2016 Canadian review has published guidelines based on its findings. They recommend that scaling and root planing (SRP) should be considered as the initial treatment for patients with chronic periodontitis. They note that \"the strength of the recommendation is limited because SRP is considered the reference standard and thus used as an active control for periodontal trials and there are few studies in which investigators compare SRP with no treatment.\" They add however that \"root planing ... carries the risk of damaging the root surface and potentially causing tooth or root sensitivity. Generally expected post-SRP procedural adverse effects include discomfort.\"</p> <p>Enamel cracks, early caries and resin restorations can be damaged during scaling.</p>"}, {"location": "teeth_deep_cleaning/#effectiveness-of-the-procedure", "title": "Effectiveness of the procedure", "text": "<p>A scaling and root planing procedure is to be considered effective if the patient is subsequently able to maintain their periodontal health without further bone or attachment loss and if it prevents recurrent infection with periodontal pathogens.</p> <p>The long term effectiveness of scaling and root planing depends upon a number of factors. These factors include patient compliance, disease progress at the time of intervention, probing depth, and anatomical factors like grooves in the roots of teeth, concavities, and furcation involvement which may limit visibility of underlying deep calculus and debris.</p> <p>First and foremost, periodontal scaling and root planing is a procedure that must be done thoroughly and with attention to detail in order to ensure complete removal of all calculus and plaque from involved sites. If these causative agents are not removed, the disease will continue to progress and further damage will result. In cases of mild to moderate periodontitis, scaling and root planing can achieve excellent results if the procedure is thorough. As periodontitis increases in severity, a greater amount of supporting bone is destroyed by the infection. This is illustrated clinically by the deepening of the periodontal pockets targeted for cleaning and disinfection during the procedure. Once the periodontal pockets exceed 6 mm in depth, the effectiveness of deposit removal begins to decrease, and the likelihood of complete healing after one procedure begins to decline as well. The more severe the infection prior to intervention, the greater the effort required to arrest its progress and return the patient to health. Diseased pockets over 6 mm can be resolved through periodontal flap surgery.</p> <p>Although healing of the soft tissues will begin immediately following removal of the microbial biofilm and calculus that cause the disease, scaling and root planing is only the first step in arresting the disease process. Following initial cleaning and disinfection of all affected sites, it is necessary to prevent the infection from recurring. Therefore, patient compliance is, by far, the most important factor, having the greatest influence on the success or failure of periodontal intervention. Immediately following treatment, the patient will need to maintain excellent oral care at home. With proper homecare, which includes but is by no means limited to brushing twice daily for 2\u20133 minutes, flossing daily and use of mouthrinse, the potential for effective healing following scaling and root planing increases. Commitment to and diligence in the thorough completion of daily oral hygiene practices are essential to this success.</p> <p>The process which allows for the formation of deep periodontal pockets does not occur overnight. Therefore, it is unrealistic to expect the tissue to heal completely in a similarly short time period. Gains in gingival attachment may occur slowly over time, and ongoing periodontal maintenance visits are usually recommended (by some sources).</p>"}, {"location": "teeth_deep_cleaning/#side-effects", "title": "Side effects", "text": "<p>The process carries it's risks, such as:</p> <ul> <li>Pop out a filling</li> <li>Gums damage in an irreversible way.</li> <li>End up with an abscess if a tiny piece of tartar is knocked loose and becomes     trapped.</li> <li>Have more sensitivity after the procedure.</li> </ul>"}, {"location": "teeth_deep_cleaning/#conclusion", "title": "Conclusion", "text": "<ul> <li>Deep cleaning is an invasive procedure</li> <li>There is a lack of scientific studies supporting the frequency of it's     application, specially for people that doesn't suffer from periodontitis.</li> <li>It's an expensive procedure.</li> </ul> <p>So, ask the opinion of two or three independent dentists before doing a deep clean.</p>"}, {"location": "tenacity/", "title": "Tenacity", "text": "<p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.</p>"}, {"location": "tenacity/#installation", "title": "Installation", "text": "<pre><code>pip install tenacity\n</code></pre>"}, {"location": "tenacity/#usage", "title": "Usage", "text": "<p>Tenacity isn't api compatible with retrying but adds significant new functionality and fixes a number of longstanding bugs.</p> <p>The simplest use case is retrying a flaky function whenever an Exception occurs until a value is returned.</p> <pre><code>import random\nfrom tenacity import retry\n\n@retry\ndef do_something_unreliable():\n    if random.randint(0, 10) &gt; 1:\n        raise IOError(\"Broken sauce, everything is hosed!!!111one\")\n    else:\n        return \"Awesome sauce!\"\n\nprint(do_something_unreliable())\n</code></pre>"}, {"location": "tenacity/#basic-retry", "title": "Basic Retry", "text": "<p>As you saw above, the default behavior is to retry forever without waiting when an exception is raised.</p> <pre><code>@retry\ndef never_gonna_give_you_up():\n    print(\"Retry forever ignoring Exceptions, don't wait between retries\")\n    raise Exception\n</code></pre>"}, {"location": "tenacity/#stopping", "title": "Stopping", "text": "<p>Let\u2019s be a little less persistent and set some boundaries, such as the number of attempts before giving up.</p> <pre><code>@retry(stop=stop_after_attempt(7))\ndef stop_after_7_attempts():\n    print(\"Stopping after 7 attempts\")\n    raise Exception\n</code></pre> <p>We don\u2019t have all day, so let\u2019s set a boundary for how long we should be retrying stuff.</p> <pre><code>@retry(stop=stop_after_delay(10))\ndef stop_after_10_s():\n    print(\"Stopping after 10 seconds\")\n    raise Exception\n</code></pre> <p>You can combine several stop conditions by using the <code>|</code> operator:</p> <pre><code>@retry(stop=(stop_after_delay(10) | stop_after_attempt(5)))\ndef stop_after_10_s_or_5_retries():\n    print(\"Stopping after 10 seconds or 5 retries\")\n    raise Exception\n</code></pre>"}, {"location": "tenacity/#waiting-before-retrying", "title": "Waiting before retrying", "text": "<p>Most things don\u2019t like to be polled as fast as possible, so let\u2019s just wait 2 seconds between retries.</p> <pre><code>@retry(wait=wait_fixed(2))\ndef wait_2_s():\n    print(\"Wait 2 second between retries\")\n    raise Exception\n</code></pre> <p>Some things perform best with a bit of randomness injected.</p> <pre><code>@retry(wait=wait_random(min=1, max=2))\ndef wait_random_1_to_2_s():\n    print(\"Randomly wait 1 to 2 seconds between retries\")\n    raise Exception\n</code></pre> <p>Then again, it\u2019s hard to beat exponential backoff when retrying distributed services and other remote endpoints.</p> <pre><code>@retry(wait=wait_exponential(multiplier=1, min=4, max=10))\ndef wait_exponential_1():\n    print(\"Wait 2^x * 1 second between each retry starting with 4 seconds, then up to 10 seconds, then 10 seconds afterwards\")\n    raise Exception\n</code></pre> <p>Then again, it\u2019s also hard to beat combining fixed waits and jitter (to help avoid thundering herds) when retrying distributed services and other remote endpoints.</p> <pre><code>@retry(wait=wait_fixed(3) + wait_random(0, 2))\ndef wait_fixed_jitter():\n    print(\"Wait at least 3 seconds, and add up to 2 seconds of random delay\")\n    raise Exception\n</code></pre> <p>When multiple processes are in contention for a shared resource, exponentially increasing jitter helps minimise collisions.</p> <pre><code>@retry(wait=wait_random_exponential(multiplier=1, max=60))\ndef wait_exponential_jitter():\n    print(\"Randomly wait up to 2^x * 1 seconds between each retry until the range reaches 60 seconds, then randomly up to 60 seconds afterwards\")\n    raise Exception\n</code></pre>"}, {"location": "tenacity/#whether-to-retry", "title": "Whether to retry", "text": "<p>We have a few options for dealing with retries that raise specific or general exceptions, as in the cases here.</p> <pre><code>@retry(retry=retry_if_exception_type(IOError))\ndef might_io_error():\n    print(\"Retry forever with no wait if an IOError occurs, raise any other errors\")\n    raise Exception\n</code></pre> <p>We can also use the result of the function to alter the behavior of retrying.</p> <pre><code>def is_none_p(value):\n    \"\"\"Return True if value is None\"\"\"\n    return value is None\n\n@retry(retry=retry_if_result(is_none_p))\ndef might_return_none():\n    print(\"Retry with no wait if return value is None\")\n</code></pre> <p>We can also combine several conditions:</p> <pre><code>def is_none_p(value):\n    \"\"\"Return True if value is None\"\"\"\n    return value is None\n\n@retry(retry=(retry_if_result(is_none_p) | retry_if_exception_type()))\ndef might_return_none():\n    print(\"Retry forever ignoring Exceptions with no wait if return value is None\")\n</code></pre> <p>Any combination of stop, wait, etc. is also supported to give you the freedom to mix and match.</p> <p>It\u2019s also possible to retry explicitly at any time by raising the <code>TryAgain</code> exception:</p> <pre><code>@retry\ndef do_something():\n    result = something_else()\n    if result == 23:\n       raise TryAgain\n</code></pre>"}, {"location": "tenacity/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "terraform/", "title": "Terraform", "text": "<p>Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack.</p>"}, {"location": "terraform/#tools", "title": "Tools", "text": "<ul> <li>tfschema: A binary that allows you   to see the attributes of the resources of the different providers. There are   some times that there are complex attributes that aren't shown on the docs   with an example. Here you'll see them clearly.</li> </ul> <pre><code>tfschema resource list aws | grep aws_iam_user\n&gt; aws_iam_user\n&gt; aws_iam_user_group_membership\n&gt; aws_iam_user_login_profile\n&gt; aws_iam_user_policy\n&gt; aws_iam_user_policy_attachment\n&gt; aws_iam_user_ssh_key\n\ntfschema resource show aws_iam_user\n+----------------------+-------------+----------+----------+----------+-----------+\n| ATTRIBUTE            | TYPE        | REQUIRED | OPTIONAL | COMPUTED | SENSITIVE |\n+----------------------+-------------+----------+----------+----------+-----------+\n| arn                  | string      | false    | false    | true     | false     |\n| force_destroy        | bool        | false    | true     | false    | false     |\n| id                   | string      | false    | true     | true     | false     |\n| name                 | string      | true     | false    | false    | false     |\n| path                 | string      | false    | true     | false    | false     |\n| permissions_boundary | string      | false    | true     | false    | false     |\n| tags                 | map(string) | false    | true     | false    | false     |\n| unique_id            | string      | false    | false    | true     | false     |\n+----------------------+-------------+----------+----------+----------+-----------+\n\n# Open the documentation of the resource in the browser\n\ntfschema resource browse aws_iam_user\n</code></pre> <ul> <li> <p>terraforming: Tool to export existing   resources to terraform</p> </li> <li> <p>terraboard: Web dashboard to   visualize and query terraform tfstate, you can search, compare and see the   most active ones. There are deployments for k8s.</p> </li> </ul> <pre><code>export AWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXXXXXX\nexport AWS_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nexport AWS_DEFAULT_REGION=eu-west-1\nexport AWS_BUCKET=terraform-tfstate-20180119\nexport TERRABOARD_LOG_LEVEL=debug\ndocker network create terranet\ndocker run -ti --rm --name db -e POSTGRES_USER=gorm -e POSTGRES_DB=gorm -e POSTGRES_PASSWORD=\"mypassword\" --net terranet postgres\ndocker run -ti --rm -p 8080:8080 -e AWS_REGION=\"$AWS_DEFAULT_REGION\" -e AWS_ACCESS_KEY_ID=\"${AWS_ACCESS_KEY_ID}\" -e AWS_SECRET_ACCESS_KEY=\"${AWS_SECRET_ACCESS_KEY}\" -e AWS_BUCKET=\"$AWS_BUCKET\" -e DB_PASSWORD=\"mypassword\" --net terranet camptocamp/terraboard:latest\n</code></pre> <ul> <li> <p>tfenv: Install different versions of terraform   <pre><code>git clone https://github.com/tfutils/tfenv.git ~/.tfenv\necho 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' &gt;&gt; ~/.zshrc\ntfenv list-remote\ntfenv install 0.12.8\nterraform version\ntfenv install 0.11.15\nterraform version\ntfenv use 0.12.8\nterraform version\n</code></pre></p> </li> <li> <p>https://github.com/eerkunt/terraform-compliance</p> </li> <li> <p>landscape: A program to   modify the <code>plan</code> and show a nicer version, really useful when it's shown as   json. Right now   it only works for terraform 11.</p> </li> </ul> <pre><code>terraform plan | landscape\n</code></pre> <ul> <li>k2tf: Program to convert k8s yaml   manifestos to HCL.</li> </ul>"}, {"location": "terraform/#editor-plugins", "title": "Editor Plugins", "text": "<p>For Vim:</p> <ul> <li>vim-terraform: Execute tf from     vim and autoformat when saving.</li> <li>vim-terraform-completion:     linter and autocomplete.</li> </ul>"}, {"location": "terraform/#good-practices-and-maintaining", "title": "Good practices and maintaining", "text": "<ul> <li>fmt: Formats the code   following hashicorp best practices.</li> </ul> <pre><code>terraform fmt\n</code></pre> <ul> <li> <p>Validate: Tests that   the syntax is correct.   <pre><code>terraform validate\n</code></pre></p> </li> <li> <p>Documentaci\u00f3n: Generates   a table in markdown with the inputs and outputs.</p> </li> </ul> <pre><code>terraform-docs markdown table *.tf &gt; README.md\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|:----:|:-----:|:-----:|\n| broker_numbers | Number of brokers | number | `\"3\"` | no |\n| broker_size | AWS instance type for the brokers | string | `\"kafka.m5.large\"` | no |\n| ebs_size | Size of the brokers disks | string | `\"300\"` | no |\n| kafka_version | Kafka version | string | `\"2.1.0\"` | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| brokers_masked_endpoints | Zookeeper masked endpoints |\n| brokers_real_endpoints | Zookeeper real endpoints |\n| zookeeper_masked_endpoints | Zookeeper masked endpoints |\n| zookeeper_real_endpoints | Zookeeper real endpoints |\n</code></pre> <ul> <li>Terraform lint (tflint): Only works with   some AWS resources. It allows the validation against a third party API. For   example:   <pre><code>  resource \"aws_instance\" \"foo\" {\n    ami           = \"ami-0ff8a91507f77f867\"\n    instance_type = \"t1.2xlarge\" # invalid type!\n  }\n</code></pre></li> </ul> <p>The code is valid, but in AWS there doesn't exist the type <code>t1.2xlarge</code>. This   test avoids this kind of issues.</p> <pre><code>wget https://github.com/wata727/tflint/releases/download/v0.11.1/tflint_darwin_amd64.zip\nunzip tflint_darwin_amd64.zip\nsudo install tflint /usr/local/bin/\ntflint -v\n</code></pre> <p>We can automate all the above to be executed before we do a commit using the pre-commit framework.</p> <pre><code>sudo pip install pre-commit\ncd $proyectoConTerraform\necho \"\"\"repos:\n- repo: git://github.com/antonbabenko/pre-commit-terraform\n  rev: v1.19.0\n  hooks:\n    - id: terraform_fmt\n    - id: terraform_validate\n    - id: terraform_docs\n    - id: terraform_tflint\n\"\"\" &gt; .pre-commit-config.yaml\npre-commit install\npre-commit run terraform_fmt\npre-commit run terraform_validate --file dynamo.tf\npre-commit run -a\n</code></pre>"}, {"location": "terraform/#tests", "title": "Tests", "text": "<p>Motivation</p>"}, {"location": "terraform/#static-analysis", "title": "Static analysis", "text": ""}, {"location": "terraform/#linters", "title": "Linters", "text": "<ul> <li>conftest</li> <li>tflint</li> <li><code>terraform validate</code></li> </ul>"}, {"location": "terraform/#dry-run", "title": "Dry run", "text": "<ul> <li><code>terraform plan</code></li> <li>hashicorp sentinel</li> <li>terraform-compliance</li> </ul>"}, {"location": "terraform/#unit-tests", "title": "Unit tests", "text": "<p>There is no real unit testing in infrastructure code as you need to deploy it in a real environment</p> <ul> <li> <p><code>terratest</code> (works for k8s and terraform)</p> <p>Some sample code in:</p> <ul> <li>github.com/gruntwork-io/infrastructure-as-code-testing-talk</li> <li>gruntwork.io</li> </ul> </li> </ul>"}, {"location": "terraform/#e2e-test", "title": "E2E test", "text": "<ul> <li>Too slow and too brittle to be worth it</li> <li>Use incremental e2e testing</li> </ul>"}, {"location": "terraform/#variables", "title": "Variables", "text": "<p>It's a good practice to name the resource before the particularization of the resource, so you can search all the elements of that resource, for example, instead of <code>client_cidr</code> and <code>operations_cidr</code> use <code>cidr_operations</code> and <code>cidr_client</code></p> <pre><code>variable \"list_example\"{\n  description = \"An example of a list\"\n  type = \"list\"\n  default = [1, 2, 3]\n}\n\nvariable \"map_example\"{\n  description = \"An example of a dictionary\"\n  type = \"map\"\n  default = {\n    key1 = \"value1\"\n    key2 = \"value2\"\n  }\n}\n</code></pre> <p>For the use of maps inside maps or lists investigate <code>zipmap</code></p> <p>To access you have to use <code>\"${var.list_example}\"</code></p> <p>For secret variables we use: <pre><code>variable \"db_password\" {\n  description = \"The password for the database\"\n}\n</code></pre></p> <p>Which has no default value, we save that password in our keystore and pass it as environmental variable <pre><code>export TF_VAR_db_password=\"{{ your password }}\"\nterragrunt plan\n</code></pre></p> <p>As a reminder, Terraform stores all variables in its state file in plain text, including this database password, which is why your terragrunt config should always enable encryption for remote state storage in S3</p>"}, {"location": "terraform/#interpolation-of-variables", "title": "Interpolation of variables", "text": "<p>You can't interpolate in variables, so instead of <pre><code>variable \"sistemas_gpg\" {\n  description = \"Sistemas public GPG key for Zena\"\n  type = \"string\"\n  default = \"${file(\"sistemas_zena.pub\")}\"\n}\n</code></pre> You have to use locals</p> <pre><code>locals {\n  sistemas_gpg = \"${file(\"sistemas_zena.pub\")}\"\n}\n\n\"${local.sistemas_gpg}\"\n</code></pre>"}, {"location": "terraform/#show-information-of-the-resources", "title": "Show information of the resources", "text": "<p>Get information of the infrastructure. Output variables show up in the console after you run <code>terraform apply</code>, you can also use <code>terraform output [{{ output_name }}]</code> to see the value of a specific output without applying any changes</p> <pre><code>output \"public_ip\" {\n  value = \"${aws_instance.example.public_ip}\"\n}\n</code></pre> <pre><code>&gt; terraform apply\naws_security_group.instance: Refreshing state... (ID: sg-db91dba1)\naws_instance.example: Refreshing state... (ID: i-61744350)\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\npublic_ip = 54.174.13.5\n</code></pre>"}, {"location": "terraform/#data-source", "title": "Data source", "text": "<p>A data source represents a piece of read-only information that is fetched from the provider every time you run Terraform. It does not create anything new</p> <pre><code>data \"aws_availability_zones\" \"all\" {}\n</code></pre> <p>And you reference it with <code>\"${data.aws_availability_zones.all.names}\"</code></p>"}, {"location": "terraform/#read-only-state-source", "title": "Read-only state source", "text": "<p>With <code>terraform_remote_state</code> you an fetch the Terraform state file stored by another set of templates in a completely read-only manner.</p> <p>From an app template we can read the info of the ddbb with <pre><code>data \"terraform_remote_state\" \"db\" {\n  backend = \"s3\"\n  config {\n    bucket = \"(YOUR_BUCKET_NAME)\"\n    key = \"stage/data-stores/mysql/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre></p> <p>And you would access the variables inside the database terraform file with <code>data.terraform_remote_state.db.outputs.port</code></p> <p>To share variables from state, you need to to set them in the <code>outputs.tf</code> file.</p>"}, {"location": "terraform/#template_file-source", "title": "Template_file source", "text": "<p>It is used to load templates, it has two parameters, <code>template</code> which is a string and <code>vars</code> which is a map of variables. it has one output attribute called <code>rendered</code>, which is the result of rendering template. For example</p> <pre><code># File: user-data.sh\n#!/bin/bash\ncat &gt; index.html &lt;&lt;EOF\n&lt;h1&gt;Hello, World&lt;/h1&gt;\n&lt;p&gt;DB address: ${db_address}&lt;/p&gt;\n&lt;p&gt;DB port: ${db_port}&lt;/p&gt;\nEOF\nnohup busybox httpd -f -p \"${server_port}\" &amp;\n</code></pre> <pre><code>data \"template_file\" \"user_data\" {\n  template = \"${file(\"user-data.sh\")}\"\n  vars {\n    server_port = \"${var.server_port}\"\n    db_address = \"${data.terraform_remote_state.db.address}\"\n    db_port = \"${data.terraform_remote_state.db.port}\"\n  }\n}\n</code></pre>"}, {"location": "terraform/#resource-lifecycle", "title": "Resource lifecycle", "text": "<p>The <code>lifecycle</code> parameter is a meta-parameter, it exist on about every resource in Terraform. You can add a <code>lifecycle</code> block to any resource to configure how that resource should be created, updated or destroyed.</p> <p>The available options are: * <code>create_before_destroy</code>: Which if set to true will create a replacement   resource before destroying hte original resource * <code>prevent_destroy</code>: If set to true, any attempt to delete that resource   (<code>terraform destroy</code>), will fail, to delete it you have to first remove the   <code>prevent_destroy</code></p> <pre><code>resource \"aws_launch_configuration\" \"example\" {\n  image_id = \"ami-40d28157\"\n  instance_type = \"t2.micro\"\n  security_groups = [\"${aws_security_group.instance.id}\"]\n  user_data = &lt;&lt;-EOF\n              #!/bin/bash\n              echo \"Hello, World\" &gt; index.html\n              nohup busybox httpd -f -p \"${var.server_port}\" &amp;\n              EOF\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n</code></pre> <p>If you set the <code>create_before_destroy</code> on a resource, you also have to set it on every resource that X depends on (if you forget, you'll get errors about cyclical dependencies). In the case of the launch configuration, that means you need to set <code>create_before_destroy</code> to true on the security group:</p> <pre><code>resource \"aws_security_group\" \"instance\" {\n  name = \"terraform-example-instance\"\n  ingress {\n    from_port = \"${var.server_port}\"\n    to_port = \"${var.server_port}\"\n    protocol = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n</code></pre>"}, {"location": "terraform/#use-collaboratively", "title": "Use collaboratively", "text": ""}, {"location": "terraform/#share-state", "title": "Share state", "text": "<p>The best option is to use S3 as bucket of the config.</p> <p>First create it <pre><code>resource \"aws_s3_bucket\" \"terraform_state\" {\n  bucket = \"terraform-up-and-running-state\"\n  versioning {\n    enabled = true\n  }\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n</code></pre></p> <p>And then configure terraform <pre><code>terraform remote config \\\n          -backend=s3 \\\n          -backend-config=\"bucket=(YOUR_BUCKET_NAME)\" \\\n          -backend-config=\"key=global/s3/terraform.tfstate\" \\\n          -backend-config=\"region=us-east-1\" \\\n          -backend-config=\"encrypt=true\"\n</code></pre></p> <p>In this way terraform will automatically pull the latest state from this bucked and push the latest state after running a command</p>"}, {"location": "terraform/#lock-terraform", "title": "Lock terraform", "text": "<p>To avoid several people running terraform at the same time, we'd use <code>terragrunt</code> a wrapper for terraform that manages remote state for you automatically and provies locking by using DynamoDB (in the free tier)</p> <p>Inside the <code>terraform_config.tf</code> you create the dynamodb table and then configure your <code>s3</code> backend to use it</p> <pre><code>resource \"aws_dynamodb_table\" \"terraform_statelock\" {\n  name           = \"global-s3\"\n  read_capacity  = 20\n  write_capacity = 20\n  hash_key       = \"LockID\"\n\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n}\n\nterraform {\n  backend \"s3\" {\n    bucket = \"grupo-zena-tfstate\"\n    key    = \"global/s3/terraform.tfstate\"\n    region = \"eu-west-1\"\n    encrypt = \"true\"\n    dynamodb_table = \"global-s3\"\n  }\n}\n</code></pre> <p>You'll probably need to execute an <code>terraform apply</code> with the <code>dynamodb_table</code> line commented</p> <p>If you want to unforce a lock, execute:</p> <pre><code>terraform force-unlock {{ unlock_id }}\n</code></pre> <p>You get the <code>unlock_id</code> from an error trying to execute any <code>terraform</code> command</p>"}, {"location": "terraform/#modules", "title": "Modules", "text": "<p>In terraform you can put code inside of a <code>module</code> and reuse in multiple places throughout your code.</p> <p>The provider resource should be specified by the user and not in the modules</p> <p>Whenever you add a module to your terraform template or modify its source parameter you need to run a get command before you run <code>plan</code> or <code>apply</code></p> <pre><code>terraform get\n</code></pre> <p>To extract output variables of a module to the parent tf file you should use</p> <p><code>${module.{{module.name}}.{{output_name}}}</code></p>"}, {"location": "terraform/#basics", "title": "Basics", "text": "<p>Any set of Terraform templates in a directory is a module.</p> <p>The good practice is to have a directory called <code>modules</code> in your parent project directory. There you git clone the desired modules. and for example inside <code>pro/services/bastion/main.tf</code> you'd call it with:</p> <pre><code>provider \"aws\" {\n  region = \"eu-west-1\"\n}\n\nmodule \"bastion\" {\n  source = \"../../../modules/services/bastion/\"\n}\n</code></pre>"}, {"location": "terraform/#outputs", "title": "Outputs", "text": "<p>Modules encapsulate their resources. A resource in one module cannot directly depend on resources or attributes in other modules, unless those are exported through outputs. These outputs can be referenced in other places in your configuration, for example:</p> <pre><code>resource \"aws_instance\" \"client\" {\n  ami               = \"ami-408c7f28\"\n  instance_type     = \"t1.micro\"\n  availability_zone = \"${module.consul.server_availability_zone}\"\n}\n</code></pre>"}, {"location": "terraform/#import", "title": "Import", "text": "<p>You can import the different parts with <code>terraform import {{resource_type}}.{{resource_name}} {{ resource_id }}</code></p> <p>For examples see the documentation of the desired resource.</p>"}, {"location": "terraform/#bulk-import", "title": "Bulk import", "text": "<p>But if you want to bulk import sources, I suggest using <code>terraforming</code>.</p>"}, {"location": "terraform/#bad-points", "title": "Bad points", "text": "<ul> <li>Manually added resources wont be managed by terraform, therefore you can't use   it to enforce as shown in this   bug.</li> <li>If you modify the LC of an ASG, the instances don't get rolling updated, you   have to do it manually.</li> <li>They call the dictionaries <code>map</code>... (/\uff9f\u0414\uff9f)/</li> <li>The conditionals are really ugly. You need to use <code>count</code>.</li> <li>You can't split long strings xD</li> </ul>"}, {"location": "terraform/#best-practices", "title": "Best practices", "text": "<p>Name the resources with <code>_</code> instead of <code>-</code> so the editor's completion work :)</p>"}, {"location": "terraform/#vpc", "title": "VPC", "text": "<p>Don't use the default vpc</p>"}, {"location": "terraform/#security-groups", "title": "Security groups", "text": "<p>Instead of using <code>aws_security_group</code> to define the ingress and egress rules, use it only to create the empty security group and use <code>aws_security_group_rule</code> to add the rules, otherwise you'll get into a cycle loop</p> <p>The sintaxis of an egress security group must be <code>egress_from_{{source}}_to_destination</code>. The sintaxis of an ingress security group must be <code>ingress_to_{{destination}}_from_{{source}}</code></p> <p>Also set the order of the arguments, so they look like the name.</p> <p>For ingress rule:</p> <pre><code>security_group_id = ...\ncidr_blocks = ...\n</code></pre> <p>And in egress should look like:</p> <pre><code>security_group_id = ...\ncidr_blocks = ...\n</code></pre> <p>Imagine you want to filter the traffic from A -&gt; B, the egress rule from A to B should go besides the ingress rule from B to A.</p>"}, {"location": "terraform/#default-security-group", "title": "Default security group", "text": "<p>You can't manage the default security group of an vpc, therefore you have to adopt it and set it to no rules at all with <code>aws_default_security_group</code> resource</p>"}, {"location": "terraform/#iam", "title": "IAM", "text": "<p>You have to generate an gpg key and export it in base64</p> <pre><code>gpg --export {{ gpg_id }} | base64\n</code></pre> <p>To see the secrets you have to decrypt it <pre><code>terraform output secret | base64 --decode | gpg -d\n</code></pre></p>"}, {"location": "terraform/#sensitive-information", "title": "Sensitive information", "text": "<p>There are several approaches here.</p> <p>First rely on the S3 encryption to protect the information in your state file</p> <p>Second use Vault provider to protect the state file.</p> <p>Third (but I won't use it) would be to use terrahelp</p>"}, {"location": "terraform/#rds-credentials", "title": "RDS credentials", "text": "<p>The RDS credentials are saved in plaintext both in the definition and in the state file, see this bug for more information. The value of <code>password</code> is not compared against the value of the password in the cloud, so as long as the string in the code and the state file remains the same, it won't try to change it.</p> <p>As a workaround, you can create the RDS with a fake password <code>changeme</code>, and once the resource is created, run an <code>aws</code> command to change it. That way, the value in your code and the state is not the real one, but it won't try to change it.</p> <p>Inspired in this gist and the <code>local-exec</code> docs, you could do:</p> <pre><code>resource \"aws_db_instance\" \"main\" {\n    username = \"postgres\"\n    password = \"changeme\"\n    ...\n}\n\nresource \"null_resource\" \"master_password\" {\n    triggers {\n        db_host = aws_db_instance.main.address\n    }\n    provisioner \"local-exec\" {\n        command = \"pass generate rds_main_password; aws rds modify-db-instance --db-instance-identifier $INSTANCE --master-user-password $(pass show rds_main_password)\"\n        environment = {\n            INSTANCE = aws_db_instance.main.identifier\n        }\n    }\n}\n</code></pre> <p>Where the password is stored in your <code>pass</code> repository that can be shared with the team.</p> <p>If you're wondering why I added such a long line, well it's because of HCL! as you can't split long strings, marvelous isn't it? xD</p>"}, {"location": "terraform/#loops", "title": "Loops", "text": "<p>You can't use nested lists or dictionaries, see this 2015 bug</p>"}, {"location": "terraform/#loop-over-a-variable", "title": "Loop over a variable", "text": "<pre><code>variable \"vpn_egress_tcp_ports\" {\n  description = \"VPN egress tcp ports \"\n  type = \"list\"\n  default = [50, 51, 500, 4500]\n}\n\nresource \"aws_security_group_rule\" \"ingress_tcp_from_ops_to_vpn_instance\"{\n  count = \"${length(var.vpn_egress_tcp_ports)}\"\n  type = \"ingress\"\n  from_port   = \"${element(var.vpn_egress_tcp_ports, count.index)}\"\n  to_port     = \"${element(var.vpn_egress_tcp_ports, count.index)}\"\n  protocol    = \"tcp\"\n  cidr_blocks = [ \"${var.cidr}\"]\n  security_group_id = \"${aws_security_group.pro_ins_vpn.id}\"\n}\n</code></pre>"}, {"location": "terraform/#refactoring", "title": "Refactoring", "text": "<p>Refactoring in terraform is ugly business</p>"}, {"location": "terraform/#refactoring-in-modules", "title": "Refactoring in modules", "text": "<p>If you try to refactor your terraform state into modules it will try to destroy and recreate all the elements of the module...</p>"}, {"location": "terraform/#refactoring-the-state-file", "title": "Refactoring the state file", "text": "<pre><code>terraform state mv -state-out=other.tfstate module.web module.web\n</code></pre>"}, {"location": "terraform/#google-cloud-integration", "title": "Google cloud integration", "text": "<p>You configure it in the terraform directory <pre><code>// Configure the Google Cloud provider\nprovider \"google\" {\n  credentials = \"${file(\"account.json\")}\"\n  project     = \"my-gce-project\"\n  region      = \"us-central1\"\n}\n</code></pre></p> <p>To download the json go to the Google Developers Console. Go to <code>Credentials</code> then <code>Create credentials</code> and finally <code>Service account key</code>.</p> <p>Select <code>Compute engine default service account</code> and select <code>JSON</code> as the key type.</p>"}, {"location": "terraform/#ignore-the-change-of-an-attribute", "title": "Ignore the change of an attribute", "text": "<p>Sometimes you don't care whether some attributes of a resource change, if that's the case use the <code>lifecycle</code> statement:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  # ...\n\n  lifecycle {\n    ignore_changes = [\n      # Ignore changes to tags, e.g. because a management agent\n      # updates these based on some ruleset managed elsewhere.\n      tags,\n    ]\n  }\n}\n</code></pre>"}, {"location": "terraform/#define-the-default-value-of-an-variable-that-contains-an-object-as-empty", "title": "Define the default value of an variable that contains an object as empty", "text": "<pre><code>variable \"database\" {\n  type = object({\n    size                 = number\n    instance_type        = string\n    storage_type         = string\n    engine               = string\n    engine_version       = string\n    parameter_group_name = string\n    multi_az             = bool\n  })\n  default     = null\n</code></pre>"}, {"location": "terraform/#conditionals", "title": "Conditionals", "text": ""}, {"location": "terraform/#elif", "title": "Elif", "text": "<pre><code>locals {\n  test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\"\n}\n</code></pre>"}, {"location": "terraform/#do-a-conditional-if-a-variable-is-not-null", "title": "Do a conditional if a variable is not null", "text": "<pre><code>resource \"aws_db_instance\" \"instance\" {\n  count                = var.database == null ? 0 : 1\n  ...\n</code></pre>"}, {"location": "terraform/#debugging", "title": "Debugging", "text": "<p>You can set the <code>TF_LOG</code> environmental variable to one of the log levels <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code> to change the verbosity of the logs.</p> <p>To remove the debug traces run <code>unset TF_LOG</code>.</p>"}, {"location": "terraform/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Modules registry</li> <li>terraform-aws-modules</li> <li>AWS providers</li> <li>AWS examples</li> <li>GCloud examples</li> <li>Good and bad sides of terraform</li> <li>Awesome Terraform</li> </ul>"}, {"location": "time_management/", "title": "Time management", "text": "<p>Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency.</p> <p>To be able to do time management, you first need to define how do you want to increase your effectiveness, efficiency, and productivity. For me, it means increasing the amount and quality of work per unit of time or effort. Understanding work as any task that gets me closer to a goal. It doesn't necessarily be related with professional work, it can be applied to a personal project, cleaning or hanging out with friends.</p> <p>The rest of the article describes the approaches I use to maximize this idea of efficiency. If you have a different understanding, goal or your brain works in a different way than mine, most of the guidelines may not apply to you, but they could spark some ideas that you can implement on your daily life.</p> <p>To increase the productivity we can:</p> <ul> <li>Reduce the time spent doing unproductive tasks.</li> <li>Improve the way you do the tasks.</li> <li>Improve how you manage your tools.</li> <li>Improve your state and environment to be more efficient.</li> </ul>"}, {"location": "time_management/#reduce-the-time-spent-doing-unproductive-tasks", "title": "Reduce the time spent doing unproductive tasks", "text": "<p>Sadly, the day has only 24 hours you can use. There's nothing to do about it, we can however reduce the amount of wasted time to make a better use of the time that we have.</p>"}, {"location": "time_management/#minimize-the-context-switches", "title": "Minimize the context switches", "text": "<p>Each time we switch from one task to another, the brain needs to load all the necessary information to be able to address the new task. Dumping the old task information and loading the new is both time consuming and exhausting, so do it consciously and sparingly.</p> <p>One way of improving this behaviour is by using the Pomodoro technique.</p>"}, {"location": "time_management/#interruption-management", "title": "Interruption management", "text": "<p>We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning.</p> <p>Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels.</p> <p>Fill up your own interruption analysis report and define your workflow to manage them.</p>"}, {"location": "time_management/#avoid-lost-time-doing-nothing", "title": "Avoid lost time doing nothing", "text": "<p>Sometimes I catch myself watching at the screen with zero mental activity and drooling. Other times I endlessly switch between browser tabs or the email client and the chat clients for no reason, it's just a reflex act. You probably have similar behaviours that lead you nowhere. Some should be an alert that you need a break (don't drool the keyboard please), but others are bad uncontrolled behaviours that could be identified and got rid of.</p>"}, {"location": "time_management/#fix-your-environment", "title": "Fix your environment", "text": "<p>When we loose time, we don't do it consciously, that's why it's difficult for us to stay alert and actively try to change those behaviours.  It's much easier to fix your environment so that the reasons that trigger the time loss don't happen at all.</p> <p>For example, if you keep on going back to the email client regularly even though you decided only to check it three times a day, instead of mentally punishing yourself when you check it, close the client or move it to another workspace so it's not where you're used to see it.</p>"}, {"location": "time_management/#dont-wait-switch-task", "title": "Don't wait, switch task", "text": "<p>Even though we want to minimize the context switches, staring at the screen for a long process to end makes no sense. If you do task management well, the context switch toll gets smaller enough that whenever you hit a block in the task you're working on, you can switch to another one. A block can be caused by a long running process or waiting for someone to do something.</p> <p>If you find concentrating difficult, don't do this, it's a hard skill to master.</p> <p>When a block comes, I first try to switch back to processes that I was already working on. Try to have as less processes as possible, less than three if possible. If there is only one active process, look at the task plan for the next step that could be done in parallel. As both processes work on the same task, they share most of the context, so the switch is cheap. If there is none, go to the day plan to start the first step of the next task in the plan.</p>"}, {"location": "time_management/#improve-the-way-you-do-the-tasks", "title": "Improve the way you do the tasks", "text": "<p>Improve how you manage your tasks to:</p> <ul> <li>Reduce your mental load, so you can use those resources doing productive     work.</li> <li>Improve your efficiency.</li> <li>Make more realistic estimations, thus meeting the commited deadlines.</li> <li>Finish what you start.</li> <li>Know you're working towards your ultimate goals</li> <li>Stop feeling lost or overburdened.</li> <li>Make context switches cheaper.</li> </ul>"}, {"location": "time_management/#improve-how-you-manage-your-tools", "title": "Improve how you manage your tools", "text": "<p>Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it.</p> <p>Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.</p> <ul> <li>Email management.</li> <li>Instant messages management.</li> <li>Meetings.</li> </ul>"}, {"location": "time_management/#meetings", "title": "Meetings", "text": "<p>Calls, video calls, group calls or physical meetings are the best communication channel to transmit non trivial short messages. Even if they are the most efficient, they will break your working workflow, as you'll need to prepare yourself to know what to say and how, go to the meeting location, and then process all the information gathered. That's why if not used wisely, it can be a sink of productivity.</p> <p>Try to minimize and group the meetings, thus having less interruptions. Maximize the continuous uninterrupted time, so schedule them at the start or end of the morning or afternoon.</p> <p>Once you agreed to attend, make each of them count. Define an agenda and a time limit per section. That'll keep the conversation on track, and will give enough information to the attendees to decide if they need to be there. Likewise, whenever you're invited to a meeting, value if you need to go. If you don't, politely decline the offer. Sometimes assigning someone the role to conduct the meeting, or taking turns to talk can help.</p> <p>There are more informal meetings where you don't need all these constrains and formality. For example in a coffee break. You know that they are going to be unproductive but that's ok too. Master your tools and apply them where you think they are needed.</p>"}, {"location": "time_management/#improve-your-state", "title": "Improve your state", "text": "<p>To be able to work efficiently, manage your tasks and change your habits you need to have the appropriate state of mind. This last factor is often overlooked, but one of the most important.</p> <p>To be efficient you need to take care of yourself. Analyze how are you to detect what physical or mental attributes aren't at the optimum level and act accordingly by fixing them and adjusting your plans.</p> <p>This will be difficult to most of us, as we are disconnected from our bodies, and don't know how to study ourselves. If it's your case, you could start by meditating or to quantifying yourself.</p> <p>Some of the vectors you can work on to improve your state are:</p> <ul> <li>Sleep better.</li> <li>Work out.</li> <li>Hang out.</li> <li>Isolate your personal life from your work life.</li> <li>Procrastinate mindfully.</li> <li>Don't be a slave of the interruptions.</li> <li>Improve your working environment.</li> <li>Prevent illnesses through hygiene and exercise.</li> </ul>"}, {"location": "tool_management/", "title": "Tool management", "text": "<p>Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it.</p> <p>Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.</p>"}, {"location": "typer/", "title": "Typer", "text": "<p>Typer is a library for building CLI applications that users will love using and developers will love creating. Based on Python 3.6+ type hints.</p> <p>The key features are:</p> <ul> <li>Intuitive to write: Great editor support. Completion everywhere. Less time   debugging. Designed to be easy to use and learn. Less time reading docs.</li> <li>Easy to use: It's easy to use for the final users. Automatic help, and   automatic completion for all shells.</li> <li>Short: Minimize code duplication. Multiple features from each parameter   declaration. Fewer bugs.</li> <li>Start simple: The simplest example adds only 2 lines of code to your app: 1   import, 1 function call.</li> <li>Grow large: Grow in complexity as much as you want, create arbitrarily   complex trees of commands and groups of subcommands, with options and   arguments.</li> </ul>"}, {"location": "typer/#installation", "title": "Installation", "text": "<pre><code>pip install 'typer[all]'\n</code></pre>"}, {"location": "typer/#minimal-usage", "title": "Minimal usage", "text": "<pre><code>import typer\n\n\ndef main(name: str):\n    print(f\"Hello {name}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#usage", "title": "Usage", "text": "<p>Create a <code>typer.Typer()</code> app, and create two subcommands with their parameters.</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef hello(name: str):\n    print(f\"Hello {name}\")\n\n\n@app.command()\ndef goodbye(name: str, formal: bool = False):\n    if formal:\n        print(f\"Goodbye Ms. {name}. Have a good day.\")\n    else:\n        print(f\"Bye {name}!\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"}, {"location": "typer/#using-subcommands", "title": "Using subcommands", "text": "<p>In some cases, it's possible that your application code needs to live on a single file.</p> <pre><code>import typer\n\napp = typer.Typer()\nitems_app = typer.Typer()\napp.add_typer(items_app, name=\"items\")\nusers_app = typer.Typer()\napp.add_typer(users_app, name=\"users\")\n\n\n@items_app.command(\"create\")\ndef items_create(item: str):\n    print(f\"Creating item: {item}\")\n\n\n@items_app.command(\"delete\")\ndef items_delete(item: str):\n    print(f\"Deleting item: {item}\")\n\n\n@items_app.command(\"sell\")\ndef items_sell(item: str):\n    print(f\"Selling item: {item}\")\n\n\n@users_app.command(\"create\")\ndef users_create(user_name: str):\n    print(f\"Creating user: {user_name}\")\n\n\n@users_app.command(\"delete\")\ndef users_delete(user_name: str):\n    print(f\"Deleting user: {user_name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>Then you'll be able to call each subcommand with:</p> <pre><code>python main.py items create\n</code></pre> <p>For more complex code use nested subcommands</p>"}, {"location": "typer/#nested-subcommands", "title": "Nested Subcommands", "text": "<p>You can split the commands in different files for clarity once the code starts to grow:</p> <p>File: <code>reigns.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef conquer(name: str):\n    print(f\"Conquering reign: {name}\")\n\n\n@app.command()\ndef destroy(name: str):\n    print(f\"Destroying reign: {name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>towns.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef found(name: str):\n    print(f\"Founding town: {name}\")\n\n\n@app.command()\ndef burn(name: str):\n    print(f\"Burning town: {name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>lands.py</code>:</p> <pre><code>import typer\n\nimport reigns\nimport towns\n\napp = typer.Typer()\napp.add_typer(reigns.app, name=\"reigns\")\napp.add_typer(towns.app, name=\"towns\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>users.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef create(user_name: str):\n    print(f\"Creating user: {user_name}\")\n\n\n@app.command()\ndef delete(user_name: str):\n    print(f\"Deleting user: {user_name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>items.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef create(item: str):\n    print(f\"Creating item: {item}\")\n\n\n@app.command()\ndef delete(item: str):\n    print(f\"Deleting item: {item}\")\n\n\n@app.command()\ndef sell(item: str):\n    print(f\"Selling item: {item}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>main.py</code>:</p> <pre><code>import typer\n\nimport items\nimport lands\nimport users\n\napp = typer.Typer()\napp.add_typer(users.app, name=\"users\")\napp.add_typer(items.app, name=\"items\")\napp.add_typer(lands.app, name=\"lands\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"}, {"location": "typer/#using-the-context", "title": "Using the context", "text": "<p>When you create a Typer application it uses <code>Click</code> underneath. And every Click application has a special object called a \"Context\" that is normally hidden.</p> <p>But you can access the context by declaring a function parameter of type <code>typer.Context</code>.</p> <p>The context is also used to store objects that you may need for all the commands, for example a repository.</p> <p>Tiangolo (<code>typer</code>s main developer)suggests to use global variables or a function with <code>lru_cache</code>.</p>"}, {"location": "typer/#using-short-option-names", "title": "Using short option names", "text": "<pre><code>import typer\n\n\ndef main(user_name: str = typer.Option(..., \"--name\", \"-n\")):\n    print(f\"Hello {user_name}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <p>The <code>...</code> as the first argument is to make the option required</p>"}, {"location": "typer/#create-vvv", "title": "Create <code>-vvv</code>", "text": "<p>You can make a CLI option work as a counter with the <code>counter</code> parameter:</p> <pre><code>import typer\n\n\ndef main(verbose: int = typer.Option(0, \"--verbose\", \"-v\", count=True)):\n    print(f\"Verbose level is {verbose}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#get-the-command-line-application-directory", "title": "Get the command line application directory", "text": "<p>You can get the application directory where you can, for example, save configuration files with <code>typer.get_app_dir()</code>:</p> <pre><code>from pathlib import Path\n\nimport typer\n\nAPP_NAME = \"my-super-cli-app\"\n\n\ndef main() -&gt; None:\n    \"\"\"Define the main command line interface.\"\"\"\n    app_dir = typer.get_app_dir(APP_NAME)\n    config_path: Path = Path(app_dir) / \"config.json\"\n    if not config_path.is_file():\n        print(\"Config file doesn't exist yet\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <p>It will give you a directory for storing configurations appropriate for your CLI program for the current user in each operating system.</p>"}, {"location": "typer/#exiting-with-an-error-code", "title": "Exiting with an error code", "text": "<p><code>typer.Exit()</code> takes an optional code parameter. By default, code is <code>0</code>, meaning there was no error.</p> <p>You can pass a code with a number other than <code>0</code> to tell the terminal that there was an error in the execution of the program:</p> <pre><code>import typer\n\n\ndef main(username: str):\n    if username == \"root\":\n        print(\"The root user is reserved\")\n        raise typer.Exit(code=1)\n    print(f\"New user created: {username}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#create-a-version-command", "title": "Create a <code>--version</code> command", "text": "<p>You could use a callback to implement a <code>--version</code> CLI option.</p> <p>It would show the version of your CLI program and then it would terminate it. Even before any other CLI parameter is processed.</p> <pre><code>from typing import Optional\n\nimport typer\n\n__version__ = \"0.1.0\"\n\n\ndef version_callback(value: bool) -&gt; None:\n    \"\"\"Print the version of the program.\"\"\"\n    if value:\n        print(f\"Awesome CLI Version: {__version__}\")\n        raise typer.Exit()\n\n\ndef main(\n    version: Optional[bool] = typer.Option(\n        None, \"--version\", callback=version_callback, is_eager=True\n    ),\n) -&gt; None:\n    ...\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#testing", "title": "Testing", "text": "<p>Testing is similar to <code>click</code> testing, but you import the <code>CliRunner</code> directly from <code>typer</code>:</p> <pre><code>from typer.testing import CliRunner\n</code></pre>"}, {"location": "typer/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Issues</li> </ul>"}, {"location": "use_warnings/", "title": "Using warnings to evolve your package", "text": "<p>Regardless of the versioning system you're using, once you reach your first stable version, the commitment to your end users must be that you give them time to adapt to the changes in your program. So whenever you want to introduce a breaking change release it under a new interface, and in parallel, start emitting <code>DeprecationWarning</code> or <code>UserWarning</code> messages whenever someone invokes the old one. Maintain this state for a defined period (for example six months), and communicate explicitly in the warning message the timeline for when users have to migrate.</p> <p>This gives everyone time to move to the new interface without breaking their system, and then the library may remove the change and get rid of the old design chains forever. As an added benefit, only people using the old interface will ever see the warning, as opposed to affecting everyone (as seen with the semantic versioning major version bump).</p> <p>If you're following semantic versioning you'd do this change in a <code>minor</code> release, and you'll finally remove the functionality in another <code>minor</code> release. As you've given your users enough time to adequate to the new version of the code, it's not understood as a breaking change.</p> <p>This allows too for your users to be less afraid and stop upper-pinning you in their dependencies.</p> <p>Another benefit of using warnings is that if you configure your test runner to capture the warnings (which you should!) you can use your test suite to see the real impact of the deprecation, you may even realize why was that feature there and that you can't deprecate it at all.</p>"}, {"location": "use_warnings/#using-warnings", "title": "Using warnings", "text": "<p>Even though there are many warnings, I usually use <code>UserWarning</code> or <code>DeprecationWarning</code>. The full list is:</p> Class Description Warning This is the base class of all warning category classes. UserWarning The default category for warn(). DeprecationWarning Warn other developers about deprecated features. FutureWarning Warn other end users of applications about deprecated features. SyntaxWarning Warn about dubious syntactic features. RuntimeWarning Warn about dubious runtime features. PendingDeprecationWarning Warn about features that will be deprecated in the future (ignored by default). ImportWarning Warn triggered during the process of importing a module (ignored by default). UnicodeWarning Warn related to Unicode. BytesWarning Warn related to bytes and bytearray. ResourceWarning Warn related to resource usage (ignored by default)."}, {"location": "use_warnings/#how-to-raise-a-warning", "title": "How to raise a warning", "text": "<p>Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program, where that condition doesn\u2019t warrant raising an exception and terminating the program.</p> <pre><code>import warnings\n\ndef f():\n    warnings.warn('Message', DeprecationWarning)\n</code></pre>"}, {"location": "use_warnings/#suppressing-a-warning", "title": "Suppressing a warning", "text": "<p>To disable in the whole file, add to the top:</p> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\")\n</code></pre> <p>If you want this to apply to only one section of code, then use the warnings context manager:</p> <pre><code>import warnings\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\")\n    # .. your divide-by-zero code ..\n</code></pre> <p>And if you want to disable it for the whole code base configure pytest accordingly.</p>"}, {"location": "use_warnings/#how-to-evolve-your-code", "title": "How to evolve your code", "text": "<p>To ensure that the transition is smooth you need to tweak your code so that the user can switch a flag and make sure that their code keeps on working with the new changes. For example imagine that we have a class <code>MyClass</code> with a method <code>my_method</code>.</p> <pre><code>class MyClass:\n    def my_method(self, argument):\n        # my_method code goes here\n</code></pre> <p>You can add an argument <code>deprecate_my_method</code> that defaults to <code>False</code>, or you can take the chance to change the signature of the function, so that if the user is using the old argument, it uses the old behaviour and gets the warning, and if it's using the new argument, it uses the new. The advantage of changing the signature is that you don't need to do another deprecation for the temporal argument flag.</p> <p>!!! note \"Or you can use environmental variables\"</p> <pre><code>class MyClass:\n    def __init__(self, deprecate_my_method = False):\n        self.deprecate_my_method = deprecate_my_method\n\n    def my_method(self, argument):\n        if self.deprecate_my_method:\n            # my_method new functionality\n        else:\n            warnings.warn(\"Use my_new_method instead\", UserWarning)\n            # my_method old code goes here\n</code></pre> <p>That way when users get the new version of your code, if they are not using <code>my_method</code> they won't get the exception, and if they are, they can change how they initialize their classes with <code>MyClass(deprecate_my_method=True)</code>, run their tests tweaking their code to meet the new functionality and make sure that they are ready for the method to be deprecated. Once removed, another UserWarning will be raised to stop using <code>deprecate_my_method</code> as an argument to initialize the class as it is no longer needed.</p> <p>Until you remove the old code, you need to keep both functionalities and make sure all your test suite works with both cases. To do that, create the warning, run the tests and see what tests are raising the exception. For each of them you need to think if this test will make sense with the new code:</p> <ul> <li>If it doesn't, make sure that the warning is raised.</li> <li>If it is, make sure that the warning is raised and create     another test with the <code>deprecate_my_method</code> enabled.</li> </ul> <p>Once the deprecation date arrives you'll need to search for the date in your code to see where the warning is raised and used, remove the old functionality and update the tests. If you used a temporal argument to let the users try the new behaviour, issue the warning to deprecate it.</p>"}, {"location": "use_warnings/#use-environmental-variables", "title": "Use environmental variables", "text": "<p>A cleaner way to handle it is with environmental variables, that way you don't need to change the signature of the function twice. I've learned this from boto where they informed their users this way:</p> <ul> <li>If you wish to test the new feature we have created a new environment variable     <code>BOTO_DISABLE_COMMONNAME</code>. Setting this to <code>true</code> will suppress the warning and     use the new functionality.</li> <li>If you are concerned about this change causing disruptions, you can pin your     version of <code>botocore</code> to <code>&lt;1.28.0</code> until you are ready to migrate.</li> <li> <p>If you are only concerned about silencing the warning in your logs, use     <code>warnings.filterwarnings</code> when instantiating a new service client.</p> <pre><code>import warnings\nwarnings.filterwarnings('ignore', category=FutureWarning, module='botocore.client')\n</code></pre> </li> </ul>"}, {"location": "use_warnings/#testing-warnings", "title": "Testing warnings", "text": "<p>To test the function with pytest you can use <code>pytest.warns</code>:</p> <pre><code>import warnings\nimport pytest\n\n\ndef test_warning():\n    with pytest.warns(UserWarning, match='my warning'):\n        warnings.warn(\"my warning\", UserWarning)\n</code></pre> <p>For the <code>DeprecationWarnings</code> you can use <code>deprecated_call</code>:</p> <p>Or you can use <code>deprecated</code>:</p> <pre><code>def test_myfunction_deprecated():\n    with pytest.deprecated_call():\n        f()\n</code></pre> <pre><code>@deprecated(version='1.2.0', reason=\"You should use another function\")\ndef some_old_function(x, y):\n    return x + y\n</code></pre> <p>But it adds a dependency to your program, although they don't have any downstream dependencies.</p>"}, {"location": "use_warnings/#references", "title": "References", "text": "<ul> <li>Bernat post on versioning</li> </ul>"}, {"location": "vdirsyncer/", "title": "vdirsyncer", "text": "<p>vdirsyncer is a Python command-line tool for synchronizing calendars and addressbooks between a variety of servers and the local filesystem. The most popular usecase is to synchronize a server with a local folder and use a set of other programs such as <code>khal</code> to change the local events and contacts. Vdirsyncer can then synchronize those changes back to the server.</p> <p>However, <code>vdirsyncer</code> is not limited to synchronizing between clients and servers. It can also be used to synchronize calendars and/or addressbooks between two servers directly.</p> <p>It aims to be for calendars and contacts what OfflineIMAP is for emails.</p>"}, {"location": "vdirsyncer/#installation", "title": "Installation", "text": "<p>Although it's available in the major package managers, you can get a more bleeding edge version with <code>pip</code>.</p> <pre><code>pipx install vdirsyncer\n</code></pre> <p>If you don't have <code>pipx</code> you can use <code>pip</code>.</p> <p>You also need to install some dependencies for it to work:</p> <pre><code>sudo apt-get install libxml2 libxslt1.1 zlib1g\n</code></pre>"}, {"location": "vdirsyncer/#configuration", "title": "Configuration", "text": "<p>In this example we set up contacts synchronization, but calendar sync works almost the same. Just swap <code>type = \"carddav\"</code> for <code>type = \"caldav\"</code> and <code>fileext = \".vcf\"</code> for <code>fileext = \".ics\"</code>.</p> <p>By default, <code>vdirsyncer</code> looks for its configuration file in the following locations:</p> <ul> <li>The file pointed to by the <code>VDIRSYNCER_CONFIG</code> environment variable.</li> <li><code>~/.vdirsyncer/config</code>.</li> <li><code>$XDG_CONFIG_HOME/vdirsyncer/config</code>, which is normally     <code>~/.config/vdirsyncer/config</code>.</li> </ul> <p>You need to create the directory as it's not created by default and the base config file.</p> <p>The config file should start with a general section, where the only required parameter is status_path. The following is a minimal example:</p> <pre><code>[general]\nstatus_path = \"~/.vdirsyncer/status/\"\n</code></pre> <p>After the <code>general</code> section, an arbitrary amount of pair and storage sections might come.</p> <p>In vdirsyncer, synchronization is always done between two storages. Such storages are defined in storage sections, and which pairs of storages should actually be synchronized is defined in pair section. This format is copied from OfflineIMAP, where storages are called repositories and pairs are called accounts.</p>"}, {"location": "vdirsyncer/#syncing-a-calendar", "title": "Syncing a calendar", "text": "<p>To sync to a nextcloud calendar:</p> <pre><code>[pair my_calendars]\na = \"my_calendars_local\"\nb = \"my_calendars_remote\"\ncollections = [\"from a\", \"from b\"]\nmetadata = [\"color\"]\n\n[storage my_calendars_local]\ntype = \"filesystem\"\npath = \"~/.calendars/\"\nfileext = \".ics\"\n\n[storage my_calendars_remote]\ntype = \"caldav\"\n#Can be obtained from nextcloud\nurl = \"https://yournextcloud.example.lcl/remote.php/dav/calendars/USERNAME/personal/\"\nusername = \"&lt;USERNAME&gt;\"\n#Instead of inserting my plaintext password I fetch it using pass\npassword.fetch = [\"command\", \"pass\", \"nextcloud\"]\n#SSL certificate fingerprint\nverify_fingerprint = \"FINGERPRINT\"\n#Verify ssl certificate. Set to false if it is self signed and not installed on local machine\nverify = true\n</code></pre> <p>Read the SSl and certificate validation section to see how to create the <code>verify_fingerprint</code>.</p>"}, {"location": "vdirsyncer/#syncing-an-address-book", "title": "Syncing an address book", "text": "<p>The following example synchronizes ownCloud\u2019s addressbooks to <code>~/.contacts/</code>:</p> <pre><code>[pair my_contacts]\na = \"my_contacts_local\"\nb = \"my_contacts_remote\"\ncollections = [\"from a\", \"from b\"]\n\n[storage my_contacts_local]\ntype = \"filesystem\"\npath = \"~/.contacts/\"\nfileext = \".vcf\"\n\n[storage my_contacts_remote]\ntype = \"carddav\"\n\n# We can simplify this URL here as well. In theory it shouldn't matter.\nurl = \"https://owncloud.example.com/remote.php/carddav/\"\nusername = \"bob\"\npassword = \"asdf\"\n</code></pre> <p>Note</p> <pre><code>Configuration for other servers can be found at\n[Servers](https://vdirsyncer.pimutils.org/en/stable/tutorials/index.html#supported-servers).\n</code></pre> <p>After running <code>vdirsyncer discover</code> and <code>vdirsyncer sync</code>, <code>~/.contacts/</code> will contain subdirectories for each addressbook, which in turn will contain a bunch of <code>.vcf</code> files which all contain a contact in VCARD format each. You can modify their contents, add new ones and delete some, and your changes will be synchronized to the CalDAV server after you run <code>vdirsyncer sync</code> again.</p>"}, {"location": "vdirsyncer/#conflict-resolution", "title": "Conflict resolution", "text": "<p>If the same item is changed on both sides <code>vdirsyncer</code> can manage the conflict in three ways:</p> <ul> <li>Displaying an error message (the default).</li> <li>Choosing one alternative version over the other.</li> <li>Starts a command of your choice that is supposed to merge the two alternative     versions.</li> </ul> <p>Options 2 and 3 require adding a <code>conflict_resolution</code> parameter to the pair section. Option 2 requires giving either <code>a wins</code> or <code>b wins</code> as value to the parameter:</p> <pre><code>[pair my_contacts]\n...\nconflict_resolution = \"b wins\"\n</code></pre> <p>Earlier we wrote that <code>b = \"my_contacts_remote\"</code>, so when <code>vdirsyncer</code> encounters the situation where an item changed on both sides, it will simply overwrite the local item with the one from the server.</p> <p>Option 3 requires specifying as value of <code>conflict_resolution</code> an array starting with <code>command</code> and containing paths and arguments to a command. For example:</p> <pre><code>[pair my_contacts]\n...\nconflict_resolution = [\"command\", \"vimdiff\"]\n</code></pre> <p>In this example, <code>vimdiff &lt;a&gt; &lt;b&gt;</code> will be called with <code>&lt;a&gt;</code> and <code>&lt;b&gt;</code> being two temporary files containing the conflicting files. The files need to be exactly the same when the command returns. More arguments can be passed to the command by adding more elements to the array.</p>"}, {"location": "vdirsyncer/#ssl-and-certificate-validation", "title": "SSL and certificate validation", "text": "<p>To pin the certificate by fingerprint:</p> <pre><code>[storage foo]\ntype = \"caldav\"\n...\nverify_fingerprint = \"94:FD:7A:CB:50:75:A4:69:82:0A:F8:23:DF:07:FC:69:3E:CD:90:CA\"\n#verify = false  # Optional: Disable CA validation, useful for self-signed certs\n</code></pre> <p>SHA1-, SHA256- or MD5-Fingerprints can be used.</p> <p>You can use the following command for obtaining a SHA-1 fingerprint:</p> <pre><code>echo -n | openssl s_client -connect unterwaditzer.net:443 | openssl x509 -noout -fingerprint\n</code></pre> <p>Note that <code>verify_fingerprint</code> doesn't suffice for <code>vdirsyncer</code> to work with self-signed certificates (or certificates that are not in your trust store). You most likely need to set <code>verify = false</code> as well. This disables verification of the SSL certificate\u2019s expiration time and the existence of it in your trust store, all that\u2019s verified now is the fingerprint.</p> <p>However, please consider using Let\u2019s Encrypt such that you can forget about all of that. It is easier to deploy a free certificate from them than configuring all of your clients to accept the self-signed certificate.</p>"}, {"location": "vdirsyncer/#storing-passwords", "title": "Storing passwords", "text": "<p><code>vdirsyncer</code> can fetch passwords from several sources other than the config file.</p> <p>Say you have the following configuration:</p> <pre><code>[storage foo]\ntype = \"caldav\"\nurl = ...\nusername = \"foo\"\npassword = \"bar\"\n</code></pre> <p>But it bugs you that the password is stored in cleartext in the config file. You can do this:</p> <pre><code>[storage foo]\ntype = \"caldav\"\nurl = ...\nusername = \"foo\"\npassword.fetch = [\"command\", \"~/get-password.sh\", \"more\", \"args\"]\n</code></pre> <p>You can fetch the username as well:</p> <pre><code>[storage foo]\ntype = \"caldav\"\nurl = ...\nusername.fetch = [\"command\", \"~/get-username.sh\"]\npassword.fetch = [\"command\", \"~/get-password.sh\"]\n</code></pre> <p>Or really any kind of parameter in a storage section.</p> <p>With <code>pass</code> for example, you might find yourself writing something like this in your configuration file:</p> <pre><code>password.fetch = [\"command\", \"pass\", \"caldav\"]\n</code></pre>"}, {"location": "vdirsyncer/#google", "title": "Google", "text": "<p><code>vdirsyncer</code> supports synchronization with Google calendars with the restriction that VTODO files are rejected by the server.</p> <p>Synchronization with Google contacts is less reliable due to negligence of Google\u2019s CardDAV API. Google\u2019s CardDAV implementation is allegedly a disaster in terms of data safety. Always back up your data.</p> <p>At first run you will be asked to authorize application for Google account access.</p> <p>To use this storage type, you need to install some additional dependencies:</p> <pre><code>pip install vdirsyncer[google]\n</code></pre>"}, {"location": "vdirsyncer/#official-steps", "title": "Official steps", "text": "<p>As of 2022-10-13 these didn't work for me, see the next section</p> <p>Furthermore you need to register <code>vdirsyncer</code> as an application yourself to obtain <code>client_id</code> and <code>client_secret</code>, as it is against Google\u2019s Terms of Service to hardcode those into open source software:</p> <ul> <li>Go to the Google API Manager and     create a new project under any name.</li> <li>Within that project, enable the <code>CalDAV</code> and <code>CardDAV</code> APIs (not the Calendar     and Contacts APIs, those are different and won\u2019t work). There should be     a searchbox where you can just enter those terms.</li> <li>In the sidebar, select <code>Credentials</code> and create a new <code>OAuth Client ID</code>. The application type is <code>Other</code>.</li> <li>You\u2019ll be prompted to create a OAuth consent screen first. Fill out that form     however you like.</li> <li>Finally you should have a Client ID and a Client secret. Provide these in your     storage config.</li> </ul> <p>The <code>token_file</code> parameter should be a <code>filepath</code> where vdirsyncer can later store authentication-related data. You do not need to create the file itself or write anything to it.</p> <pre><code>[storage example_for_google_calendar]\ntype = \"google_calendar\"\ntoken_file = \"...\"\nclient_id = \"...\"\nclient_secret = \"...\"\n#start_date = null\n#end_date = null\n#item_types = []\n</code></pre>"}, {"location": "vdirsyncer/#use-nekr0z-patch-solution", "title": "Use Nekr0z patch solution", "text": "<p>look the previous section if you have doubts on any of the steps</p> <p>If the official steps failed for you, try these ones:</p> <ul> <li>Go to the Google API Manager and     create a new project under any name.</li> <li>Selected the vdirsyncer project</li> <li>Went to Credentials -&gt; Create Credentials -&gt; OAuth Client ID</li> <li>Select \"Web Application\"</li> <li>Under \"Authorised redirect URIs\" added <code>http://127.0.0.1:8088</code> pressed \"Create\".</li> <li>Edit your <code>vdirsyncer</code> <code>config</code> [storage google] section to have the new     client_id and client_secret ().</li> <li> <p>Find the location of the <code>vdirsyncer/storage/google.py</code> in your environment     (mine was in     <code>~/.local/pipx/venvs/vdirsyncer/lib/python3.10/site-packages/vdirsyncer/storage</code>) and changed line 65 from</p> <pre><code>redirect_uri=\"urn:ietf:wg:oauth:2.0:oob\",\n</code></pre> <p>to</p> <pre><code>redirect_uri=\"http://127.0.0.1:8088\",\n</code></pre> </li> <li> <p>Run <code>vdirsyncer discover my_calendar</code>.</p> </li> <li>Opened the link in my browser (on my desktop machine).</li> <li> <p>Proceeded with Google authentication until \"Firefox can not connect to 127.0.0.1:8088.\" was displayed.     from the browser's address bar that looked like:</p> <p>http://127.0.0.1:8088/?state=SOMETHING&amp;code=HERECOMESTHECODE&amp;scope=https://www.googleapis.com/auth/calendar * Copy the <code>HERECOMESTHECODE</code> part. * Paste the code into the session where <code>vdirsyncer</code> was running</p> </li> </ul>"}, {"location": "vdirsyncer/#see-differences-between-syncs", "title": "See differences between syncs", "text": "<p>If you create a git repository where you have your calendars you can do a <code>git diff</code> and see the files that have changed. If you do a commit after each sync you can have all the history.</p>"}, {"location": "vdirsyncer/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "versioning/", "title": "Program versioning", "text": "<p>The Don't Repeat Yourself principle encourages developers to abstract code into a separate components and reuse them rather than write it over and over again. If this happens across the system, the best practice is to put it inside a package that lives on its own (a library) and then pull it in from the applications when required.</p> <p>!!! note \"This article is heavily based on the posts in the references sections, the main credit goes to them, I've just refactored all together under my personal opinion.\"</p> <p>As most of us can\u2019t think of every feature that the library might offer or what bugs it might contain, these packages tend to evolve. Therefore, we need some mechanism to encode these evolutions of the library, so that downstream users can understand how big the change is. Most commonly, developers use three methods:</p> <ul> <li>A version number.</li> <li>A changelog.</li> <li>The git history.</li> </ul> <p>The version change is used as a concise way for the project to communicate this evolution, and it's what we're going to analyze in this article. However, encoding all the information of a change into a number switch has proven to be far from perfect.</p> <p>That's why keeping a good and detailed changelog makes a lot of sense, as it will better transmit that intent, and what will be the impact of upgrading. Once again, this falls into the same problem as before, while a change log is more descriptive, it still only tells you what changes (or breakages) a project intended to make, it doesn\u2019t go into any detail about unintended consequences of changes made. Ultimately, a change log\u2019s accuracy is no different than that of the version itself, it's just (hopefully!) more detailed.</p> <p>Fundamentally, any indicator of change that isn\u2019t a full diff is just a lossy encoding of that change. You can't expect though to read all the diffs of the libraries that you use, that's why version numbers and changelogs make a lot of sense. We just need to be aware of the limits of each system.</p> <p>That being said, you'll use version numbers in two ways:</p> <ul> <li> <p>As a producer of applications and libraries where you\u2019ll have to decide what     versioning system to use.</p> </li> <li> <p>As a consumer of dependencies, you\u2019ll have to express what versions of a given library your     application/library is compatible.</p> </li> </ul>"}, {"location": "versioning/#deciding-what-version-system-to-use-for-your-programs", "title": "Deciding what version system to use for your programs", "text": "<p>The two most popular versioning systems are:</p> <ul> <li> <p>Semantic Versioning: A way to define your program's     version based on the type of changes you've introduced.</p> </li> <li> <p>Calendar Versioning: A versioning convention based     on your project's release calendar, instead of arbitrary numbers.</p> </li> </ul> <p>Each has it's advantages and disadvantages. From a consumer perspective, I think that projects should generally default to SemVer-ish, following the spirit of the documentation rather than the letter of the specification because:</p> <ul> <li>Your version number becomes a means of communicating your changes intents to     your end users.</li> <li>If you use the semantic versioning commit message     guidelines, you are more     likely to have a useful git history and can automatically maintain the     project's changelog.</li> </ul> <p>There are however, corner cases where CalVer makes more sense:</p> <ul> <li> <p>You\u2019re tracking something that is already versioned using dates or for which     the version number can only really be described as a point in time release.     The <code>pytz</code> is a good example of both of these cases, the Olson TZ database is     versioned using a date based scheme and the information that it is providing     is best represented as a snapshot of the state of what timezones were like     at a particular point in time.</p> </li> <li> <p>Your project is going to be breaking compatibility in every release and you do     not want to make any promises of compatibility. You should still document     this fact in your README, but if there\u2019s no promise of compatibility between     releases, then there\u2019s no information to be communicated in the version     number.</p> </li> <li> <p>Your project is never going to intentionally breaking compatibility in     a release, and you strive to always maintain compatibility. Projects can     always just use the latest version of your software. Your changes will     only ever be additive, and if you need to change your API, you\u2019ll do     something like leave the old API intact, and add a new API with the new     semantics. An example of this case would be the Ubuntu versions.</p> </li> </ul>"}, {"location": "versioning/#how-to-evolve-your-code-version", "title": "How to evolve your code version", "text": "<p>Assuming you're using Semantic Versioning you can improve your code evolution by:</p> <ul> <li>Avoid becoming a ZeroVer package.</li> <li>Use Warnings to avoid major changes</li> </ul>"}, {"location": "versioning/#avoid-becoming-a-zerover-package", "title": "Avoid becoming a ZeroVer package", "text": "<p>Once your project reach it's first level of maturity you should release <code>1.0.0</code> to avoid falling into ZeroVer. For example you can use one of the next indicators:</p> <ul> <li>If you're frequently using it and haven't done any breaking change in 3 months.</li> <li>If 30 users are depending on it. For example counting the project stars.</li> </ul>"}, {"location": "versioning/#use-warnings-to-avoid-major-changes", "title": "Use Warnings to avoid major changes", "text": "<p>Semantic versioning uses the major version to defend against breaking changes, and at the same offers maintainers the freedom to evolve the library without breaking users. Nevertheless, this does not seem to work that well.</p> <p>So it's better to use Warnings to avoid major changes.</p>"}, {"location": "versioning/#communicate-with-your-users", "title": "Communicate with your users", "text": "<p>You should warn your users not to blindly trust that any version change is not going to break their code and that you assume that they are actively testing the package updates.</p>"}, {"location": "versioning/#keep-the-requires-python-metadata-updated", "title": "Keep the <code>Requires-Python</code> metadata updated", "text": "<p>It's important not to upper cap the Python version and to maintain the <code>Requires-Python</code> package metadata updated. Dependency solvers will use this information to fetch the correct versions of the packages for the users.</p>"}, {"location": "versioning/#deciding-how-to-manage-the-versions-of-your-dependencies", "title": "Deciding how to manage the versions of your dependencies", "text": "<p>As a consumer of other dependencies, you need to specify in your package what versions does your code support. The traditional way to do it is by pinning those versions in your package definition. For example in python it lives either in the <code>setup.py</code> or in the <code>pyproject.toml</code>.</p>"}, {"location": "versioning/#lower-version-pinning", "title": "Lower version pinning", "text": "<p>When you're developing a program that uses a dependency, you usually don't know if a previous version of that dependency is compatible with your code, so in theory it makes sense to specify that you don't support any version smaller than the actual with something like <code>&gt;=1.2</code>. If you follow this train of thought, each time you update your dependencies, you should update your lower pins, because you're only running your test suite on those versions. If the libraries didn't do upper version pinning, then there would be no problem as you wouldn't be risking to get into version conflicts.</p> <p>A more relaxed approach would be not to update the pins when you update, in that case, you should run your tests both against the oldest possible values and the newest to ensure that everything works as expected. This way you'll be more kind to your users as you'll reduce possible version conflicts, but it'll add work to the maintainers.</p> <p>The most relaxed approach would be not to use pins at all, it will suppress most of the version conflicts but you won't be sure that the dependencies that your users are using are compatible with your code.</p> <p>Think about how much work you want to invest in maintaining your package and how much stability you want to offer before you choose one or the other method. Once you've made your choice, it would be nice if you communicate it to your users through your documentation.</p>"}, {"location": "versioning/#upper-version-pinning", "title": "Upper version pinning", "text": "<p>Program maintainers often rely on upper version pinning to guarantee that their code is not going to be broken due to a dependency update.</p> <p>We\u2019ll cover the valid use cases for capping after this section. But, just to be clear, if you know you do not support a new release of a library, then absolutely, go ahead and cap it as soon as you know this to be true. If something does not work, you should cap (or maybe restrict a single version if the upstream library has a temporary bug rather than a design direction that\u2019s causing the failure). You should also do as much as you can to quickly remove the cap, as all the downsides of capping in the next sections still apply.</p> <p>The following will assume you are capping before knowing that something does not work, but just out of general principle, like Poetry recommends and defaults to with <code>poetry add</code>. In most cases, the answer will be don\u2019t. For simplicity, I will also assume you are being tempted to cap to major releases (<code>^1.0.0</code> in Poetry or <code>~=1.0</code> in all other tooling that follows Python standards via PEP 440) following the false security that only <code>major</code> changes can to break your code. If you cap to minor versions <code>~=1.0.0</code>, this is much worse, and the arguments below apply even more strongly.</p>"}, {"location": "versioning/#version-limits-break-code-too", "title": "Version limits break code too", "text": "<p>Following this path will effectively opt you out of bug fixes and security updates, as most of the projects only maintain the latest version of their program and what worse, you'll be preventing everyone using your library not to use the latest version of those libraries. All in exchange to defend yourself against a change that in practice will rarely impact you. Sure, you can move on to the next version of each of your pins each time they increase a major via something like <code>Click&gt;=8, &lt;9</code>. However, this involves manual intervention on their code, and you might not have the time to do this for every one of your projects.</p> <p>If we add the fact that not only major but any other version change may break your code due to unintended changes and the difference in the change categorization, then you can treat all changes equally, so it makes no sense on pinning the major version either.</p> <p>This is specially useless when you add dependencies that follow CalVer. <code>poetry add</code> packaging will still do <code>^21</code> for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer.</p>"}, {"location": "versioning/#semver-never-promises-to-break-your-code", "title": "SemVer never promises to break your code", "text": "<p>A really easy but incorrect generalization of the SemVer rules is \u201ca major version will break my code\u201d. Even if the library follows true SemVer perfectly, a major version bump does not promise to break downstream code. It promises that some downstream code may break. If you use <code>pytest</code> to test your code, for example, the next major version will be very unlikely to break. If you write a <code>pytest</code> extension, however, then the chances of something breaking are much higher (but not 100%, maybe not even 50%). Quite ironically, the better a package follows SemVer, the smaller the change will trigger a major version, and therefore the less likely a major version will break a particular downstream code.</p> <p>As a general rule, if you have a reasonably stable dependency, and you only use the documented API, especially if your usage is pretty light/general, then a major update is extremely unlikely to break your code. It\u2019s quite rare for light usage of a library to break on a major update. It can happen, of course, but is unlikely. If you are using something very heavily, if you are working on a framework extension, or if you use internals that are not publicly documented, then your chances of breaking on a major release are much higher. Python has a culture of producing <code>FutureWarnings</code>, <code>DeprecationWarnings</code>, or <code>PendingDeprecationWarnings</code> (make sure they are on in your testing, and turn into errors), good libraries will use them.</p>"}, {"location": "versioning/#version-conflicts", "title": "Version conflicts", "text": "<p>And then there\u2019s another aspect version pinning will introduce: version conflicts.</p> <p>An application or library will have a set of libraries it depends on directly. These are libraries you\u2019re directly importing within the application/library you\u2019re maintaining, but then the libraries themselves may rely on other libraries. This is known as a transitive dependency. Very soon, you\u2019ll get to a point where two different components use the same library, and both of them might express version constraints on it.</p> <p>For example, consider the case of <code>tenacity</code>: a general-purpose retrying library. Imagine you were using this in your application, and being a religious follower of semantic versioning, you\u2019ve pinned it to the version that was out when you created the app in early 2018: <code>4.11</code>. The constraint would specify version <code>4.11</code> or later, but less than the next major version <code>5</code>.</p> <p>At the same time, you also connect to an HTTP service. This connection is handled by another library, and the maintainer of that decided to also use <code>tenacity</code> to offer automatic retry functionality. They pinned it similarly following the semantic versioning convention. Back in 2018, this caused no issues. But then August comes, and version <code>5.0</code> is released.</p> <p>The service and its library maintainers have a lot more time on their hands (perhaps because they are paid to do so), so they quickly move to version <code>5.0</code>. Or perhaps they want to use a feature from the new major version. Now they introduce the pin greater than five but less than six on tenacity. Their public interface does not change at all at this point, so they do not bump their major version. It\u2019s just a patch release.</p> <p>Python can only have one version of a library installed at a given time. At this point, there is a version conflict. You\u2019re requesting a version between four and five, while the service library is requesting a version between five and six. Both constraints cannot be satisfied.</p> <p>If you use a version of pip older than <code>20.2</code> (the release in which it added a dependency resolver) it will just install a version matching the first constraint it finds and ignore any subsequent constraints. Versions of pip after <code>20.2</code> would fail with an error indicating that the constraint cannot be satisfied.</p> <p>Either way, your application no longer works. The only way to make it work is to either pin the service library down to the last working patch number, or upgrade your version pinning of <code>tenacity</code>. This is generating extra work for you with minimal benefit. Often it might not be even possible to use two conflicting libraries until one of them relaxes their requirements. It also means you must support a wide version range; ironically. If you update to requiring `tenacity</p> <p>5<code>, your update can\u2019t be installed with another library still on</code>4.11<code>. So you have to support</code>tenacity&gt;=4.11,&lt;6` for a while until most libraries have similarly updated.</p> <p>And for those who might think this doesn\u2019t happen often, let me say that <code>tenacity</code> released another major version a year later in November 2019. Thus, the cycle starts all over again. In both cases, your code most likely did not need to change at all, as just a small part of their public API changed. In my experience, this happens a lot more often than when a major version bump breaks you. I've found myself investing most of my project maintenance time opening issues in third party dependencies to update their pins.</p>"}, {"location": "versioning/#it-doesnt-scale", "title": "It doesn\u2019t scale", "text": "<p>If you have a single library that doesn\u2019t play well, then you probably will get a working solve easily (this is one reason that this practice doesn\u2019t seem so bad at first). If more packages start following this tight capping, however, you end up with a situation where things simply cannot solve. A moderately sized application can have a hundred or more dependencies when expanded, so such issues in my experience start to appear every few months. You need only 5-6 of such cases for every 100 libraries for this issue to pop up every two months on your plate. And potentially for a multiple of your applications.</p> <p>The entire point of packaging is to allow you to get lots of packages that each do some job for you. We should be trying to make it easy to be able to add dependencies, not harder.</p> <p>The implication of this is you should be very careful when you see tight requirements in packages and you have any upper bound caps anywhere in the dependency chain. If something caps dependencies, there\u2019s a very good chance adding two such packages will break your solve, so you should pick just one, or just avoid them altogether, so you can add one in the future. This is a good rule, actually: Never add a library to your dependencies that has excessive upper bound capping. When I have failed to follow this rule for a larger package, I have usually come to regret it.</p> <p>If you are doing the capping and are providing a library, you now have a commitment to quickly release an update, ideally right before any capped dependency comes out with a new version. Though if you cap, how to you install development versions or even know when a major version is released? This makes it harder for downstream packages to update, because they have to wait for all the caps to be moved for all upstream.</p>"}, {"location": "versioning/#it-conflicts-with-tight-lower-bounds", "title": "It conflicts with tight lower bounds", "text": "<p>A tight lower bound is only bad if packages cap upper bounds. If you can avoid upper-cap packages, you can accept tight lower bound packages, which are much better; better features, better security, better compatibility with new hardware and OS\u2019s. A good packaging system should allow you to require modern packages; why develop for really old versions of things if the packaging system can upgrade them? But a upper bound cap breaks this. Hopefully anyone who is writing software and pushing versions will agree that tight lower limits are much better than tight upper limits, so if one has to go, it\u2019s the upper limits.</p> <p>It is also rather rare that packages solve for lower bounds in CI (I would love to see such a solver become an option, by the way!), so setting a tight lower bound is one way to avoid rare errors when old packages are cached that you don\u2019t actually support. CI almost never has a cache of old packages, but users do.</p>"}, {"location": "versioning/#capping-dependencies-hides-incompatibilities", "title": "Capping dependencies hides incompatibilities", "text": "<p>Another serious side effect of capping dependencies is that you are not notified properly of incoming incompatibilities, and you have to be extra proactive in monitoring your dependencies for updates. If you don\u2019t cap your dependencies, you are immediately notified when a dependency releases a new version, probably by your CI, the first time you build with that new version. If you are running your CI with the <code>--dev</code> flag on your <code>pip install</code> (uncommon, but probably a good idea), then you might even catch and fix the issue before a release is even made. If you don\u2019t do this, however, then you don\u2019t know about the incompatibility until (much) later.</p> <p>If you are not following all of your dependencies, you might not notice that you are out of date until it\u2019s both a serious problem for users and it\u2019s really hard for you to tell what change broke your usage because several versions have been released. While I\u2019m not a huge fan of Google\u2019s live-at-head philosophy (primarily because it has heavy requirements not applicable for most open-source projects), I appreciate and love catching a dependency incompatibility as soon as you possibly can; the smaller the change set, the easier it is to identify and fix the issue.</p>"}, {"location": "versioning/#capping-all-dependencies-hides-real-incompatibilities", "title": "Capping all dependencies hides real incompatibilities", "text": "<p>If you see <code>X&gt;=1.1</code>, that tells you that the package is using features from <code>1.1</code> and do not support <code>1.0</code>. If you see <code>X&lt;1.2</code>, this should tell you that there\u2019s a problem with <code>1.2</code> and the current software, specifically something they know the dependency will not fix/revert. Not that you just capped all your dependencies and have no idea if that will or won\u2019t work at all. A cap should be like a TODO; it\u2019s a known issue that needs to be worked on soon. As in yesterday.</p>"}, {"location": "versioning/#pinning-the-python-version-is-special", "title": "Pinning the Python version is special", "text": "<p>Another practice pushed by Poetry is adding an upper cap to the Python version. This is misusing a feature designed to help with dropping old Python versions to instead stop new Python versions from being used. \u201cScrolling back\u201d through older releases to find the newest version that does not restrict the version of Python being used is exactly the wrong behavior for an upper cap, and that is what the purpose of this field is. Current versions of pip do seem to fail when this is capped, rather than scrolling back to find an older uncapped version, but I haven\u2019t found many libraries that have \u201cadded\u201d this after releasing to be sure of that.</p> <p>To be clear, this is very different from a library: specifically, you can\u2019t downgrade your Python version if this is capped to something below your current version. You can only fail. So this does not \u201cfix\u201d something by getting an older, working version, it only causes hard failures if it works the way you might hope it does. This means instead of seeing the real failure and possibly helping to fix it, users just see a <code>Python doesn\u2019t match</code> error. And, most of the time, it\u2019s not even a real error; if you support Python <code>3.x</code> without warnings, you should support Python <code>3.x+1</code> (and <code>3.x+2</code>, too).</p> <p>Capping to <code>&lt;4</code> (something like <code>^3.6</code> in Poetry) is also directly in conflict with the Python developer\u2019s own statements; they promise the <code>3-&gt;4</code> transition will be more like the <code>1-&gt;2</code> transition than the <code>2-&gt;3</code> transition. When Python 4 does come out, it will be really hard to even run your CI on 4 until all your dependencies uncap. And you won\u2019t actually see the real failures, you\u2019ll just see incompatibility errors, so you won\u2019t even know what to report to those libraries. And this practice makes it hard to test development versions of Python.</p> <p>And, if you use Poetry, as soon as someone caps the Python version, every Poetry project that uses it must also cap, even if you believe it is a detestable practice and confusing to users. It is also wrong unless you fully pin the dependency that forced the cap. If the dependency drops it in a patch release or something else you support, you no longer would need the cap.</p>"}, {"location": "versioning/#applications-are-slightly-different", "title": "Applications are slightly different", "text": "<p>If you have a true application (that is, if you are not intending your package to be used as a library), upper version constraints are much less problematic, and some of the reasons above don't apply. This due to two reasons.</p> <p>First, if you are writing a library, your \u201cusers\u201d are specifying your package in their dependencies; if an update breaks them, they can always add the necessary exclusion or cap for you to help end users. It\u2019s a leaky abstraction, they shouldn\u2019t have to care about what your dependencies are, but when capping interferes with what they can use, that\u2019s also a leaky and unfixable abstraction. For an application, the \u201cusers\u201d are more likely to be installing your package directly, where the users are generally other developers adding to requirements for libraries.</p> <p>Second, for an app that is installed from PyPI, you are less likely to have to worry about what else is installed (the other issues are still true). Many (most?) users will not be using <code>pipx</code> or a fresh virtual environment each time, so in practice, you\u2019ll still run into problems with tight constraints, but there is a workaround (use <code>pipx</code>, for example). You still are still affected by most of the arguments above, though, so personally I\u2019d still not recommend adding untested caps.</p>"}, {"location": "versioning/#when-is-it-ok-to-set-an-upper-limit", "title": "When is it ok to set an upper limit?", "text": "<p>Valid reasons to add an upper limit are:</p> <ul> <li> <p>If a dependency is known to be broken, block out the broken version. Try very     hard to fix this problem quickly, then remove the block if it\u2019s fixable on     your end. If the fix happens upstream, excluding just the broken version is     fine (or they can \u201cyank\u201d the bad release to help everyone).</p> </li> <li> <p>If you know upstream is about to make a major change that is very likely to     break your usage, you can cap. But try to fix this as quickly as possible so     you can remove the cap by the time they release. Possibly add development     branch/release testing until this is resolved.</p> </li> <li> <p>If upstream asks users to cap, then I still don\u2019t like it, but it is okay if     you want to follow the upstream recommendation. You should ask yourself: do     you want to use a library that may intentionally break you and require     changes on your part without help via deprecation periods? A one-time major     rewrite might be an acceptable reason. Also, if you are upstream, it is very     un-Pythonic to break users without deprecation warnings first. Don\u2019t do it     if possible.</p> </li> <li> <p>If you are writing an extension for an ecosystem/framework (pytest extension,     Sphinx extension, Jupyter extension, etc), then capping on the major version     of that library is acceptable. Note this happens once - you have a single     library that can be capped. You must release as soon as you possibly can     after a new major release, and you should be closely following upstream</p> <ul> <li>probably using development releases for testing, etc. But doing this for one library is probably manageable.</li> </ul> </li> <li> <p>You are releasing two or more libraries in sync with each other. You control     the release cadence for both libraries. This is likely the \u201cbest\u201d reason to     cap. Some of the above issues don\u2019t apply in this case - since you control     the release cadence and can keep them in sync.</p> </li> <li> <p>You depend on private internal details of a library. You should also rethink     your choices - this can be broken in a minor or patch release, and often is.</p> </li> </ul> <p>If you cap in these situations, I wouldn\u2019t complain, but I wouldn\u2019t really recommend it either:</p> <ul> <li> <p>If you have a heavy dependency on a library, maybe cap. A really large API     surface is more likely to be hit by the possible breakage.</p> </li> <li> <p>If a library is very new, say on version 1 or a ZeroVer library, and has very     few users, maybe cap if it seems rather unstable. See if the library authors     recommend capping - they might plan to make a large change if it\u2019s early in     development. This is not blanket permission to cap ZeroVer libraries!</p> </li> <li> <p>If a library looks really unstable, such as having a history of making big     changes, then cap. Or use a different library. Even better, contact the     authors, and make sure that your usage is safe for the near future.</p> </li> </ul>"}, {"location": "versioning/#summary", "title": "Summary", "text": "<p>No more than 1-2 of your dependencies should fall into the categories of acceptable upper pinning. In every other case, do not cap your dependences, specially if you are writing a library! You could probably summarize it like this: if there\u2019s a high chance (say <code>75%+</code>) that a dependency will break for you when it updates, you can add a cap. But if there\u2019s no reason to believe it will break, do not add the cap; you will cause more severe (unfixable) pain than the breakage would.</p> <p>If you have an app instead of a library, you can be cautiously more relaxed, but not much. Apps do not have to live in shared environments, though they might.</p> <p>Notice many of the above instances are due to very close/special interaction with a small number of libraries (either a plugin for a framework, synchronized releases, or very heavy usage). Most libraries you use do not fall into this category. Remember, library authors don\u2019t want to break users who follow their public API and documentation. If they do, it\u2019s for a special and good reason (or it is a bad library to depend on). They will probably have a deprecation period, produce warnings, etc.</p> <p>If you do version cap anything, you are promising to closely follow that dependency, update the cap as soon as possible, follow beta or RC releases or the development branch, etc. When a new version of a library comes out, end users should be able to start trying it out. If they can\u2019t, your library\u2019s dependencies are a leaky abstraction (users shouldn\u2019t have to care about what dependencies libraries use).</p>"}, {"location": "versioning/#automatically-upgrade-and-test-your-dependencies", "title": "Automatically upgrade and test your dependencies", "text": "<p>Now that you have minimized the upper bound pins and defined the lower bound pins you need to ensure that your code works with the latest version of your dependencies.</p> <p>One way to do it is running a periodic cronjob (daily probably) that updates your requirements lock, optionally your lower bounds, and checks that the tests keep on passing.</p>"}, {"location": "versioning/#monitor-your-dependencies-evolution", "title": "Monitor your dependencies evolution", "text": "<p>You rely on your dependencies to fulfill critical parts of your package, therefore it makes sense to know how they are changing in order to:</p> <ul> <li>Change your package to use new features.</li> <li>Be aware of the new possibilities to solve future problems.</li> <li>Get an idea of the dependency stability and future.</li> </ul> <p>Depending on how much you rely on the dependency, different levels of monitorization can be used, ordered from least to most you could check:</p> <ul> <li> <p>Release messages: Some projects post them in their blogs, you can use     their RSS feed to keep updated. If the project uses Github to create the     release messages, you can get notifications on just those release messages.</p> <p>If the project uses Semantic Versioning, it can help you dismiss all changes that are <code>micro</code>, review without urgency the <code>minor</code> and prioritize the <code>major</code> ones. If all you're given is a CalVer style version then you're forced to dedicate the same time to each of the changes.</p> </li> <li> <p>Changelog: if you get a notification of a new release, head to the changelog     to get a better detail of what has changed.</p> </li> <li> <p>Pull requests: Depending on the project release workflow, it may     take some time from a change to be accepted until it's published under a new     release, if you monitor the pull requests, you get an early idea of what     will be included in the new version.</p> </li> <li> <p>Issues: Most of changes introduced in a project are created from the outcome     of a repository issue, where a user expresses their desire to introduce the     change. If you monitor them you'll get the idea of how the project will     evolve in the future.</p> </li> </ul>"}, {"location": "versioning/#summary_1", "title": "Summary", "text": "<p>Is semantic versioning irrevocably broken? Should it never be used? I don\u2019t think so. It still makes a lot of sense where there are ample resources to maintain multiple versions in parallel. A great example of this is Django. However, it feels less practical for projects that have just a few maintainers.</p> <p>In this case, it often leads to opting people out of bug fixes and security updates. It also encourages version conflicts in environments that can\u2019t have multiple versions of the same library, as is the case with Python. Furthermore, it makes it a lot harder for developers to learn from their mistakes and evolve the API to a better place. Rotten old design decisions will pull down the library for years to come.</p> <p>A better solution at hand can be using CalVer and a time-window based warning system to evolve the API and remove old interfaces. Does it solve all problems? Absolutely not.</p> <p>One thing it makes harder is library rewrites. For example, consider virtualenv's recent rewrite. Version 20 introduced a completely new API and changed some behaviours to new defaults. For such use cases in a CalVer world, you would likely need to release the rewritten project under a new name, such as virtualenv2. Then again, such complete rewrites are extremely rare (in the case of virtualenv, it involved twelve years passing).</p> <p>No version scheme will allow you to predict with any certainty how compatible your software will be with potential future versions of your dependencies. The only reasonable choices are for libraries to choose minimum versions/excluded versions only, never maximum versions. For applications, do the same thing, but also add in a lock file of known, good versions with exact pins (this is the fundamental difference between install_requires and requirements.txt).</p>"}, {"location": "versioning/#this-doesnt-necessarily-apply-to-other-ecosystems", "title": "This doesn't necessarily apply to other ecosystems", "text": "<p>All of  this advice coming from me does not necessarily apply to all other packaging ecosystems. Python's flat dependency management has its pros and cons, hence why some other ecosystems do things differently.</p>"}, {"location": "versioning/#references", "title": "References", "text": "<ul> <li>Bernat post on versioning</li> <li>Should You Use Upper Bound Version Constraints? by Henry Schreiner</li> <li>Why I don't like SemVer anymore by Snarky</li> <li>Versioning Software by donald stufft</li> </ul>"}, {"location": "vim/", "title": "Vim", "text": "<p>Vim is a lightweight keyboard driven editor. It's the road to fly over the keyboard as it increases productivity and usability.</p> <p>If you doubt between learning emacs or vim, go with emacs with spacemacs</p> <p>I am a power vim user for more than 10 years, and seeing what my friends do with emacs, I suggest you to learn it while keeping the vim movement.</p> <p>Spacemacs is a preconfigured Emacs with those bindings and a lot of more stuff, but it's a good way to start.</p>"}, {"location": "vim/#vi-vs-vim-vs-neovim", "title": "Vi vs Vim vs Neovim", "text": "<p>TL;DR: Use Neovim</p> <p>Small comparison:</p> <ul> <li>Vi</li> <li>Follows the Single Unix Specification and POSIX.</li> <li>Original code written by Bill Joy in 1976.</li> <li>BSD license.</li> <li> <p>Doesn't even have a git repository <code>-.-</code>.</p> </li> <li> <p>Vim</p> </li> <li>Written by Bram Moolenaar in 1991.</li> <li>Vim is free and open source software, license is compatible with the GNU General Public License.</li> <li>C: 47.6 %, Vim Script: 44.8%, Roff 1.9%, Makefile 1.7%, C++ 1.2%</li> <li>Commits: 7120, Branch: 1, Releases: 5639, Contributor: 1</li> <li> <p>Lines: 1.295.837</p> </li> <li> <p>Neovim</p> </li> <li>Written by the community from 2014</li> <li>Published under the Apache 2.0 license</li> <li>Commits: 7994, Branch 1, Releases: 9, Contributors: 303</li> <li>Vim script: 46.9%, C:38.9%, Lua 11.3%, Python 0.9%, C++ 0.6%</li> <li>Lines: 937.508 (27.65% less code than vim)</li> <li>Refactor: Simplify maintenance and encourage contributions</li> <li>Easy update, just symlinks</li> <li>Ahead of vim, new features inserted in Vim 8.0 (async)</li> </ul> <p>Neovim is a refactor of Vim to make it viable for another 30 years of hacking.</p> <p>Neovim very intentionally builds on the long history of Vim community knowledge and user habits. That means \u201cswitching\u201d from Vim to Neovim is just an \u201cupgrade\u201d.</p> <p>From the start, one of Neovim\u2019s explicit goals has been simplify maintenance and encourage contributions.  By building a codebase and community that enables experimentation and low-cost trials of new features..</p> <p>And there\u2019s evidence of real progress towards that ambition. We\u2019ve successfully executed non-trivial \u201coff-the-roadmap\u201d patches: features which are important to their authors, but not the highest priority for the project.</p> <p>These patches were included because they:</p> <ul> <li>Fit into existing conventions/design.</li> <li>Included robust test coverage (enabled by an advanced test framework and CI).</li> <li>Received thoughtful review by other contributors.</li> </ul>"}, {"location": "vim/#abbreviations", "title": "Abbreviations", "text": "<p>In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, <code>~/.vim/abbreviations.vim</code> for abbreviations that can be used in every type of format and <code>~/.vim/markdown-abbreviations.vim</code> for the ones that can interfere with programming typing.</p> <p>Those files are sourced in my <code>.vimrc</code></p> <pre><code>\" Abbreviations\nsource ~/.vim/abbreviations.vim\nautocmd BufNewFile,BufReadPost *.md source ~/.vim/markdown-abbreviations.vim\n</code></pre> <p>To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as <code>teh</code>.</p> <p>The process has it's inconveniences:</p> <ul> <li>You need different abbreviations for the capitalized versions, so you'd need     two abbreviations for <code>iab cant can't</code> and <code>iab Cant Can't</code></li> <li>It's not user friendly to add new words, as you need to open a file.</li> </ul> <p>The Vim Abolish plugin solves that. For example:</p> <pre><code>\" Typing the following:\nAbolish seperate separate\n\n\" Is equivalent to:\niabbrev seperate separate\niabbrev Seperate Separate\niabbrev SEPERATE SEPARATE\n</code></pre> <p>Or create more complex rules, were each <code>{}</code> gets captured and expanded with different caps</p> <pre><code>:Abolish {despa,sepe}rat{e,es,ed,ing,ely,ion,ions,or}  {despe,sepa}rat{}\n</code></pre> <p>With a bang (<code>:Abolish!</code>) the abbreviation is also appended to the file in <code>g:abolish_save_file</code>. By default <code>after/plugin/abolish.vim</code> which is loaded by default.</p> <p>Typing <code>:Abolish! im I'm</code> will append the following to the end of this file:</p> <pre><code>Abolish im I'm\n</code></pre> <p>To make it quicker I've added a mapping for <code>&lt;leader&gt;s</code>.</p> <pre><code>nnoremap &lt;leader&gt;s :Abolish!&lt;Space&gt;\n</code></pre> <p>Check the README for more details.</p>"}, {"location": "vim/#troubleshooting", "title": "Troubleshooting", "text": "<p>Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue.</p> <pre><code>Abolish knobas knowledge-based\nAbolish w what\n</code></pre> <p>Will yield <code>KnowledgeBased</code> if invoked with <code>Knobas</code>, and <code>WHAT</code> if invoked with <code>W</code>. Therefore the following definitions are preferred:</p> <pre><code>Abolish Knobas Knowledge-based\nAbolish W What\n</code></pre>"}, {"location": "vim/#auto-complete-prose-text", "title": "Auto complete prose text", "text": "<p>Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default.</p> <pre><code>let g:ycm_filetype_blacklist = {\n      \\ 'tagbar' : 1,\n      \\ 'qf' : 1,\n      \\ 'notes' : 1,\n      \\ 'unite' : 1,\n      \\ 'vimwiki' : 1,\n      \\ 'pandoc' : 1,\n      \\ 'infolog' : 1\n  \\}\n</code></pre> <p>When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions.</p> <pre><code>\" Limit the results for markdown files to 1\nau FileType markdown let g:ycm_max_num_candidates = 1\nau FileType markdown let g:ycm_max_num_identifier_candidates = 1\n</code></pre>"}, {"location": "vim/#find-synonyms", "title": "Find synonyms", "text": "<p>Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config.</p> <p>File: ~/.vimrc</p> <pre><code>Plugin 'ron89/thesaurus_query.vim'\n\n\" Thesaurus\nlet g:tq_enabled_backends=[\"mthesaur_txt\"]\nlet g:tq_mthesaur_file=\"~/.vim/thesaurus\"\nnnoremap &lt;leader&gt;r :ThesaurusQueryReplaceCurrentWord&lt;CR&gt;\ninoremap &lt;leader&gt;r &lt;esc&gt;:ThesaurusQueryReplaceCurrentWord&lt;CR&gt;\n</code></pre> <p>Run <code>:PluginInstall</code> and download the thesaurus text from gutenberg.org</p> <p>Next time you find a word like <code>therefore</code> you can press <code>:ThesaurusQueryReplaceCurrentWord</code> and you'll get a window with the following:</p> <pre><code>In line: ... therefore ...\nCandidates for therefore, found by backend: mthesaur_txt\nSynonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence\n          (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this\n          (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that\n          (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason\n          (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably\n          (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity\n          (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably\n          (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise\n          (48)under the circumstances (49)whence (50)wherefore (51)wherefrom\nType number and &lt;Enter&gt; (empty cancels; 'n': use next backend; 'p' use previous backend):\n</code></pre> <p>If for example you type <code>45</code> and hit enter, it will change it for <code>thus</code>.</p>"}, {"location": "vim/#keep-foldings", "title": "Keep foldings", "text": "<p>When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file</p> <pre><code>augroup remember_folds\n  autocmd!\n  autocmd BufLeave * mkview\n  autocmd BufEnter * silent! loadview\naugroup END\n</code></pre>"}, {"location": "vim/#python-folding-done-right", "title": "Python folding done right", "text": "<p>Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours...</p> <p>SimpylFold does the trick just fine.</p>"}, {"location": "vim/#delete-a-file-inside-vim", "title": "Delete a file inside vim", "text": "<pre><code>:call delete(expand('%')) | bdelete!\n</code></pre> <p>You can make a function so it's easier to remember</p> <pre><code>function! Rm()\n  call delete(expand('%')) | bdelete!\nendfunction\n</code></pre> <p>Now you need to run <code>:call Rm()</code>.</p>"}, {"location": "vim/#resources", "title": "Resources", "text": "<ul> <li>Nvim news</li> <li>spacevim</li> <li>awesome-vim: a list of vim       resources maintained by the community</li> </ul>"}, {"location": "vim/#vimrc-tweaking", "title": "Vimrc tweaking", "text": "<ul> <li>jessfraz vimrc</li> </ul>"}, {"location": "vim/#learning", "title": "Learning", "text": "<ul> <li>vim golf</li> <li>Vim game tutorial: very funny and challenging,       buuuuut at lvl 3 you have to pay :(.</li> <li>PacVim:       Pacman like vim game to learn.</li> <li>Vimgenius: Increase your speed and improve your       muscle memory with Vim Genius, a timed flashcard-style game designed to       make you faster in Vim. It\u2019s free and you don\u2019t need to sign up. What are       you waiting for?</li> <li>Openvim: Interactive tutorial for vim.</li> </ul>"}, {"location": "vim_tabs/", "title": "Vim tabs", "text": "<p>This article is almost a copy paste of joshldavis post</p> <p>First I have to admit, I was a heavy user of tabs in Vim.</p> <p>I was using tabs in Vim as you\u2019d use tabs in most other programs (Firefox, Terminal, Adium, etc.). I was used to the idea of a tab being the place where a document lives.</p> <p>When you want to edit a document, you open a new tab and edit away! That\u2019s how tabs work so that must be how they work in Vim right?</p> <p>Nope.</p>"}, {"location": "vim_tabs/#stop-the-tab-madness", "title": "Stop the Tab Madness", "text": "<p>If you are using tabs like this then you are really limiting yourself and using a feature of Vim that wasn't meant to work like this.</p> <p>Before I explain that, let\u2019s be sure we understand what a buffer is in Vim as well as a few other basic things.</p> <p>After that, I\u2019ll explain the correct way to use tabs within Vim.</p>"}, {"location": "vim_tabs/#buffers", "title": "Buffers", "text": "<p>A buffer is nothing more than text that you are editing. For example, when you open a file, the content of the file is loaded into a buffer. So when you issue this command:</p> <pre><code>vim .vimrc\n</code></pre> <p>You are actually launching Vim with a single buffer that is filled with the contents of the <code>.vimrc</code> file.</p> <p>Now let\u2019s look at what happens when you try to edit multiple files. Let\u2019s issue this command:</p> <pre><code>vim .vimrc .bashrc\n</code></pre> <p>In Vim run <code>:bnext</code></p> <p>Vim does what it did before, but instead of just 1 buffer, it opens another buffer that is filled with <code>.bashrc</code>. So now we have two buffers open.</p> <p>If you want to pause editing <code>.vimrc</code> and move to <code>.bashrc</code>, you could run this command in Vim <code>:bnext</code> which will show the <code>.bashrc</code> buffer. There are various other commands to manipulate buffers which you can see if you type <code>:h buffer-list</code>. Or you can use easymotion.</p>"}, {"location": "vim_tabs/#windows", "title": "Windows", "text": "<p>A window in Vim is just a way to view a buffer. Whenever you create a new vertical or horizontal split, that is a window. For example, if you were to type in <code>:help window</code>, it would launch a new window that shows the help documentation.</p> <p>The important thing to note is that a window can view any buffer it wishes; it isn't forced to look at the same buffer. When editing a file, if we type <code>:vsplit</code>, we will get a vertical split and in the other window, we will see the current buffer we are editing.</p> <p>That should no longer be confusing because a window lets us look at any buffer. It just so happens that when creating a new split: <code>:split</code> or <code>:vsplit</code>, the buffer that we view is just the current one.</p> <p>By running any of the buffer commands from <code>:h buffer-list</code>, we can modify which buffer a window is viewing.</p> <p>For an example of this, by running the following commands, we will start editing two files in Vim, open a new window by horizontally splitting, and then view the second buffer in the original window.</p> <pre><code>vim .vimrc .bashrc\n</code></pre> <p>In Vim run: <code>:split</code> and <code>:bnext</code></p>"}, {"location": "vim_tabs/#so-a-tab-is", "title": "So a Tab is\u2026?", "text": "<p>So now that we know what a buffer is and what a window is. Here is what Vim says in the Vim documentation regarding a buffer/window/tab:</p> <p>Summary:</p> <ul> <li>A buffer is the in-memory text of a file.</li> <li>A window is a viewport on a buffer.</li> <li>A tab page is a collection of windows.</li> </ul> <p>According to the documentation, a tab is just a collection of windows. This goes back to our earlier definition in that a tab is really just a layout.</p> <p>A tab is only designed to give you a different layout of windows.</p>"}, {"location": "vim_tabs/#the-tab-problem", "title": "The Tab Problem", "text": "<p>Tabs were only designed to let us have different layouts of windows. They aren't intended to be where an open file lives; an open file is instead loaded into a buffer.</p> <p>If you can view the same buffer across all tabs, how is this like a normal tab in most other editors?</p> <p>If you try to force a single tab to point to a single buffer, that is just futile. Vim just wasn't meant to work like this.</p>"}, {"location": "vim_tabs/#the-buffer-solution", "title": "The Buffer Solution", "text": "<p>To reconcile all of this and learn how to use Vim\u2019s buffers/windows effectively, it might be useful to stop using tabs altogether until you understand how to edit with just using buffers/windows.</p> <p>The first thing I did was install a plugin that allows me to visualize all the buffers open across the top. I use bufferline for this.</p> <p>Instead of replicating tabs across the top like we did in the previous solution, we are instead going to use the power of being able to open many buffers simultaneously without worrying about which ones are open.</p> <p>In my experience, CtrlP gives a powerful fuzzy finder to navigate through the buffers.</p> <p>Instead of worrying about closing buffers and managing your pseudo-tabs that was mentioned in the previous solution, you just open files that you want to edit using CtrlP and don't worry about closing buffers or how many you have opened.</p> <p>When you are done editing a file, you just save it and then open CtrlP and continue onto the next file.</p> <p>CtrlP offers a few different ways to fuzzy find. You can use the following fuzziness:</p> <ul> <li>Find in your current directory.</li> <li>Find within all your open buffers.</li> <li>Find within all your open buffers sorted by Most Recently Used (MRU).</li> <li>Find with a mix of all the above.</li> </ul>"}, {"location": "vim_tabs/#using-tabs-correctly", "title": "Using Tabs Correctly", "text": "<p>This doesn't mean you should stop using tabs altogether. You should just use them how Vim intended you to use them.</p> <p>Instead you should use them to change the layout among windows. Imagine you are working on a C project. It might be helpful to have one tab dedicated to normal editing, but another tab for using a vertical split for the file.h and file.c files to make editing between them easier.</p> <p>Tabs also work really well to divide up what you are working on. You could be working on one part of the project in one tab and another part of the project in another tab.</p> <p>Just remember though, if you are using a single tab for each file, that isn't how tabs in Vim were designed to be used.</p>"}, {"location": "vim_tabs/#default-option-when-switching", "title": "Default option when switching", "text": "<p>The default behavior when trying to switch the buffer is to not allow you to change buffer if it's not saved, but we can change it if we set one of the next options:</p> <ul> <li><code>set hidden</code>: allow to switch buffers even though it's changes aren't saved.</li> <li><code>set autowrite</code>: Auto save when switching buffers.</li> </ul>"}, {"location": "vim_tabs/#share-buffers-and-all-vim-information-between-vim-instances", "title": "Share buffers and all vim information between vim instances.", "text": "<p>This is not my ideal behavior, nvim should let the user use the window manager to manage the windows... duh, instead of vsplitting buffers or using tabs.</p> <p>But sadly as of Nvim 0.1.7 and Vim 8.0 it's not implemented. You have the <code>--server</code> option but it only sends files to the already opened vim instance.  you can't connect two vim instances to the same buffer pool.</p> <p>It's been discussed in neovim 1, 2.</p> <p>Currently gVim cannot have separate 'toplevel' windows for the same process/session. There is a TODO item to implement an inter-process communication system between multiple Vim instances to make it behave as though the separate processes are unified. (See :help todo and search for \"top-level\".)</p> <p>There is an interesting hax formalized in here which I will want to have time to test.</p> <p>Another solution would be to try to use neovim remote</p>"}, {"location": "virtual_assistant/", "title": "Virtual assistant", "text": "<p>Virtual assistant is a software agent that can perform tasks or services for an individual based on commands or questions.</p> <p>Of the open source solutions kalliope is the one I've liked most. I've also looked at mycroft but it seems less oriented to self hosted solutions, although it's possible. Mycroft has a bigger community behind though.</p> <p>To interact with it I may start with the android app, but then I'll probably install a Raspberry pi zero with Pirate Audio and an akaso external mic in the kitchen to speed up the grocy inventory management.</p>"}, {"location": "virtual_assistant/#stt", "title": "STT", "text": "<p>The only self hosted Speech-To-Text (STT) solution available now is CMUSphinx, which is based on pocketsphinx that has 2.8k stars but last update was on 28<sup>th</sup> of March of 2020.</p> <p>The CMUSphinx documentation suggest you to use Vosk based on vosk-api with 1.2k stars and last updated 2 days ago. There is an open issue to support it in kalliope, with already a french proposal.</p> <p>That led me to the issue to support DeepSpeech, Mozilla's STT solution, that has 16.5k stars and updated 3 days ago, so it would be the way to go in my opinion if the existent one fails. Right now there is no support, but this would be the place to start. For spanish, based on the mozilla discourse thread I arrived to DeepSpeech-Polyglot that has taken many datasets such as Common Voice one and generated the models.</p>"}, {"location": "vite/", "title": "Vite", "text": "<p>Vite is a build tool that aims to provide a faster and leaner development experience for modern web projects. It consists of two major parts:</p> <ul> <li> <p>A dev server that provides rich feature enhancements over native ES modules,     for example extremely fast Hot Module Replacement (HMR).</p> </li> <li> <p>A build command that bundles your code with Rollup, pre-configured to output     highly optimized static assets for production.</p> </li> </ul> <p>Vite is opinionated and comes with sensible defaults out of the box, but is also highly extensible via its Plugin API and JavaScript API with full typing support.</p>"}, {"location": "vite/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "vitest/", "title": "Vitest", "text": "<p>Vitest is a blazing fast unit-test framework powered by Vite.</p>"}, {"location": "vitest/#install", "title": "Install", "text": "<p>Add it to your project with:</p> <pre><code>npm install -D vitest\n</code></pre> <p>If you've used Vite, Vitest will read the configuration from the <code>vite.config.js</code> file so add the <code>test</code> property there.</p> <pre><code>/// &lt;reference types=\"vitest\" /&gt;\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n  test: {\n    // ...\n  },\n})\n</code></pre> <p>To run the tests use <code>npx vitest</code>, to see the coverage use <code>npx vitest --coverage</code>.</p>"}, {"location": "vitest/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "vscodium/", "title": "VSCodium", "text": "<p>VSCodium are binary releases of VS Code without MS branding/telemetry/licensing.</p>"}, {"location": "vscodium/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> </ul>"}, {"location": "vue_snippets/", "title": "Vue snippets", "text": ""}, {"location": "vue_snippets/#apply-a-style-to-a-component-given-a-condition", "title": "Apply a style to a component given a condition", "text": "<p>if you use <code>:class</code> you can write javascript code in the value, for example:</p> <pre><code>&lt;b-form-radio\n  class=\"user-retrieve-language p-2\"\n  :class=\"{'font-weight-bold': selected === language.key}\"\n  v-for=\"language in languages\"\n  v-model=\"selected\"\n  :id=\"language.key\"\n  :checked=\"selected === language.key\"\n  :value=\"language.key\"\n&gt;\n</code></pre>"}, {"location": "vue_snippets/#get-assets-url", "title": "Get assets url", "text": "<p>If you're using Vite, you can save the assets such as images or audios in the <code>src/assets</code> directory, and you can get the url with:</p> <pre><code>getImage() {\n  return new URL(`../assets/pictures/${this.active_id}.jpg`, import.meta.url).href\n},\n</code></pre> <p>This way it will give you the correct url whether you're in the development environment or in production.</p>"}, {"location": "vue_snippets/#play-audio-files", "title": "Play audio files", "text": "<p>You can get the file and save it into a <code>data</code> element with:</p> <pre><code>getAudio() {\n  this.audio = new Audio(new URL(`../assets/audio/${this.active_id}.mp3`, import.meta.url).href)\n},\n</code></pre> <p>You can start playing with <code>this.audio.play()</code>, and stop with <code>this.audio.pause()</code>.</p>"}, {"location": "vue_snippets/#run-function-in-background", "title": "Run function in background", "text": "<p>To achieve that you need to use the javascript method called <code>setInterval()</code>. It\u2019s a simple function that would repeat the same task over and over again. Here\u2019s an example:</p> <pre><code>function myFunction() {\n    setInterval(function(){ alert(\"Hello world\"); }, 3000);\n}\n</code></pre> <p>If you add a call to this method for any button and click on it, it will print Hello world every 3 seconds (3000 milliseconds) until you close the page.</p> <p>In Vue you could do something like:</p> <pre><code>&lt;script&gt;\nexport default {\n  data: () =&gt; ({\n    inbox_retry: undefined\n  }),\n  methods: {\n    retryGetInbox() {\n      this.inbox_retry = setInterval(() =&gt; {\n        if (this.showError) {\n          console.log('Retrying the fetch of the inbox')\n          // Add your code here.\n        } else {\n          clearInterval(this.inbox_retry)\n        }\n      }, 30000)\n    }\n  },\n</code></pre> <p>You can call <code>this.retryGetInbox()</code> whenever you want to start running the function periodically. Once <code>this.showError</code> is <code>false</code>, we stop running the function with <code>clearInterval(this.inbox_retry)</code>.</p>"}, {"location": "vue_snippets/#truncate-text-given-a-height", "title": "Truncate text given a height", "text": "<p>By default css is able to truncate text with the size of the screen but only on one line, if you want to fill up a portion of the screen (specified in number of lines or height css parameter) and then truncate all the text that overflows, you need to use vue-clamp.</p> <p>They have a nice demo in their page where you can see their features.</p>"}, {"location": "vue_snippets/#installation", "title": "Installation", "text": "<p>If you're lucky and this issue has been solved, you can simply:</p> <pre><code>npm i --save vue-clamp\n</code></pre> <p>Else you need to create the Vue component yourself</p> VueClamp.vue <pre><code>&lt;script&gt;\nimport { addListener, removeListener } from \"resize-detector\";\nimport { defineComponent } from \"vue\";\nimport { h } from \"vue\";\n\nexport default defineComponent({\n  name: \"vue-clamp\",\n  props: {\n    tag: {\n      type: String,\n      default: \"div\",\n    },\n    autoresize: {\n      type: Boolean,\n      default: false,\n    },\n    maxLines: Number,\n    maxHeight: [String, Number],\n    ellipsis: {\n      type: String,\n      default: \"\u2026\",\n    },\n    location: {\n      type: String,\n      default: \"end\",\n      validator(value) {\n        return [\"start\", \"middle\", \"end\"].indexOf(value) !== -1;\n      },\n    },\n    expanded: Boolean,\n  },\n  data() {\n    return {\n      offset: null,\n      text: this.getText(),\n      localExpanded: !!this.expanded,\n    };\n  },\n  computed: {\n    clampedText() {\n      if (this.location === \"start\") {\n        return this.ellipsis + (this.text.slice(0, this.offset) || \"\").trim();\n      } else if (this.location === \"middle\") {\n        const split = Math.floor(this.offset / 2);\n        return (\n          (this.text.slice(0, split) || \"\").trim() +\n          this.ellipsis +\n          (this.text.slice(-split) || \"\").trim()\n        );\n      }\n\n      return (this.text.slice(0, this.offset) || \"\").trim() + this.ellipsis;\n    },\n    isClamped() {\n      if (!this.text) {\n        return false;\n      }\n      return this.offset !== this.text.length;\n    },\n    realText() {\n      return this.isClamped ? this.clampedText : this.text;\n    },\n    realMaxHeight() {\n      if (this.localExpanded) {\n        return null;\n      }\n      const { maxHeight } = this;\n      if (!maxHeight) {\n        return null;\n      }\n      return typeof maxHeight === \"number\" ? `${maxHeight}px` : maxHeight;\n    },\n  },\n  watch: {\n    expanded(val) {\n      this.localExpanded = val;\n    },\n    localExpanded(val) {\n      if (val) {\n        this.clampAt(this.text.length);\n      } else {\n        this.update();\n      }\n      if (this.expanded !== val) {\n        this.$emit(\"update:expanded\", val);\n      }\n    },\n    isClamped: {\n      handler(val) {\n        this.$nextTick(() =&gt; this.$emit(\"clampchange\", val));\n      },\n      immediate: true,\n    },\n  },\n  mounted() {\n    this.init();\n\n    this.$watch(\n      (vm) =&gt; [vm.maxLines, vm.maxHeight, vm.ellipsis, vm.isClamped].join(),\n      this.update\n    );\n    this.$watch((vm) =&gt; [vm.tag, vm.text, vm.autoresize].join(), this.init);\n  },\n  updated() {\n    this.text = this.getText();\n    this.applyChange();\n  },\n  beforeUnmount() {\n    this.cleanUp();\n  },\n  methods: {\n    init() {\n      const contents = this.$slots.default();\n\n      if (!contents) {\n        return;\n      }\n\n      this.offset = this.text.length;\n\n      this.cleanUp();\n\n      if (this.autoresize) {\n        addListener(this.$el, this.update);\n        this.unregisterResizeCallback = () =&gt; {\n          removeListener(this.$el, this.update);\n        };\n      }\n      this.update();\n    },\n    update() {\n      if (this.localExpanded) {\n        return;\n      }\n      this.applyChange();\n      if (this.isOverflow() || this.isClamped) {\n        this.search();\n      }\n    },\n    expand() {\n      this.localExpanded = true;\n    },\n    collapse() {\n      this.localExpanded = false;\n    },\n    toggle() {\n      this.localExpanded = !this.localExpanded;\n    },\n    getLines() {\n      return Object.keys(\n        Array.prototype.slice\n          .call(this.$refs.content.getClientRects())\n          .reduce((prev, { top, bottom }) =&gt; {\n            const key = `${top}/${bottom}`;\n            if (!prev[key]) {\n              prev[key] = true;\n            }\n            return prev;\n          }, {})\n      ).length;\n    },\n    isOverflow() {\n      if (!this.maxLines &amp;&amp; !this.maxHeight) {\n        return false;\n      }\n\n      if (this.maxLines) {\n        if (this.getLines() &gt; this.maxLines) {\n          return true;\n        }\n      }\n\n      if (this.maxHeight) {\n        if (this.$el.scrollHeight &gt; this.$el.offsetHeight) {\n          return true;\n        }\n      }\n      return false;\n    },\n    getText() {\n      // Look for the first non-empty text node\n      const [content] = (this.$slots.default() || []).filter(\n        (node) =&gt; !node.tag &amp;&amp; !node.isComment\n      );\n      return content ? content.children : \"\";\n    },\n    moveEdge(steps) {\n      this.clampAt(this.offset + steps);\n    },\n    clampAt(offset) {\n      this.offset = offset;\n      this.applyChange();\n    },\n    applyChange() {\n      this.$refs.text.textContent = this.realText;\n    },\n    stepToFit() {\n      this.fill();\n      this.clamp();\n    },\n    fill() {\n      while (\n        (!this.isOverflow() || this.getLines() &lt; 2) &amp;&amp;\n        this.offset &lt; this.text.length\n      ) {\n        this.moveEdge(1);\n      }\n    },\n    clamp() {\n      while (this.isOverflow() &amp;&amp; this.getLines() &gt; 1 &amp;&amp; this.offset &gt; 0) {\n        this.moveEdge(-1);\n      }\n    },\n    search(...range) {\n      const [from = 0, to = this.offset] = range;\n      if (to - from &lt;= 3) {\n        this.stepToFit();\n        return;\n      }\n      const target = Math.floor((to + from) / 2);\n      this.clampAt(target);\n      if (this.isOverflow()) {\n        this.search(from, target);\n      } else {\n        this.search(target, to);\n      }\n    },\n    cleanUp() {\n      if (this.unregisterResizeCallback) {\n        this.unregisterResizeCallback();\n      }\n    },\n  },\n  render() {\n    const contents = [\n      h(\n        \"span\",\n        {\n          ref: \"text\",\n          attrs: {\n            \"aria-label\": this.text?.trim(),\n          },\n        },\n        this.realText\n      ),\n    ];\n\n    const { expand, collapse, toggle } = this;\n    const scope = {\n      expand,\n      collapse,\n      toggle,\n      clamped: this.isClamped,\n      expanded: this.localExpanded,\n    };\n    const before = this.$slots.before\n      ? this.$slots.before(scope)\n      : this.$slots.before;\n    if (before) {\n      contents.unshift(...(Array.isArray(before) ? before : [before]));\n    }\n    const after = this.$slots.after\n      ? this.$slots.after(scope)\n      : this.$slots.after;\n    if (after) {\n      contents.push(...(Array.isArray(after) ? after : [after]));\n    }\n    const lines = [\n      h(\n        \"span\",\n        {\n          style: {\n            boxShadow: \"transparent 0 0\",\n          },\n          ref: \"content\",\n        },\n        contents\n      ),\n    ];\n    return h(\n      this.tag,\n      {\n        style: {\n          maxHeight: this.realMaxHeight,\n          overflow: \"hidden\",\n        },\n      },\n      lines\n    );\n  },\n});\n&lt;/script&gt;\n</code></pre>"}, {"location": "vue_snippets/#usage", "title": "Usage", "text": "<p>If you were able to install it with <code>npm</code>, use:</p> <pre><code>&lt;template&gt;\n&lt;v-clamp autoresize :max-lines=\"3\"&gt;{{ text }}&lt;/v-clamp&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport VClamp from 'vue-clamp'\n\nexport default {\n  components: {\n    VClamp\n  },\n  data () {\n    return {\n      text: 'Some very very long text content.'\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre> <p>Else use:</p> <pre><code>&lt;template&gt;\n  &lt;VueClamp maxHeight=\"30vh\"&gt;\n  {{ text }}\n  &lt;/VueClamp&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport VueClamp from './VueClamp.vue'\n\nexport default {\n  components: {\n    VueClamp\n  },\n  data () {\n    return {\n      text: 'Some very very long text content.'\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre>"}, {"location": "vue_snippets/#display-time-ago-from-timestamp", "title": "Display time ago from timestamp", "text": "<p>Install with:</p> <pre><code>npm install vue2-timeago@next\n</code></pre>"}, {"location": "vuejs/", "title": "Vue.js", "text": "<p>Vue.js is a progressive JavaScript framework for building user interfaces. It builds on top of standard HTML, CSS and JavaScript, and provides a declarative and component-based programming model that helps you efficiently develop user interfaces, be it simple or complex.</p> <p>Here is a minimal example:</p> <pre><code>import { createApp } from 'vue'\n\ncreateApp({\n  data() {\n    return {\n      count: 0\n    }\n  }\n}).mount('#app')\n</code></pre> <pre><code>&lt;div id=\"app\"&gt;\n  &lt;button @click=\"count++\"&gt;\n    Count is: {{ count }}\n  &lt;/button&gt;\n&lt;/div&gt;\n</code></pre> <p>The above example demonstrates the two core features of Vue:</p> <ul> <li> <p>Declarative Rendering: Vue extends standard HTML with a template syntax that     allows us to declaratively describe HTML output based on JavaScript state.</p> </li> <li> <p>Reactivity: Vue automatically tracks JavaScript state changes and     efficiently updates the DOM when changes happen.</p> </li> </ul>"}, {"location": "vuejs/#features", "title": "Features", "text": ""}, {"location": "vuejs/#single-file-components", "title": "Single file components", "text": "<p>Single-File Component (also known as <code>*.vue</code> files, abbreviated as SFC) encapsulates the component's logic (JavaScript), template (HTML), and styles (CSS) in a single file. Here's the previous example, written in SFC format:</p> <pre><code>&lt;script&gt;\nexport default {\n  data() {\n    return {\n      count: 0\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"count++\"&gt;Count is: {{ count }}&lt;/button&gt;\n&lt;/template&gt;\n\n&lt;style scoped&gt;\nbutton {\n  font-weight: bold;\n}\n&lt;/style&gt;\n</code></pre>"}, {"location": "vuejs/#api-styles", "title": "API Styles", "text": "<p>Vue components can be authored in two different API styles: Options API and Composition API.</p>"}, {"location": "vuejs/#options-api", "title": "Options API", "text": "<p>With Options API, we define a component's logic using an object of options such as <code>data</code>, <code>methods</code>, and <code>mounted</code>. Properties defined by options are exposed on this inside functions, which points to the component instance:</p> <pre><code>&lt;script&gt;\nexport default {\n  // Properties returned from data() becomes reactive state\n  // and will be exposed on `this`.\n  data() {\n    return {\n      count: 0\n    }\n  },\n\n  // Methods are functions that mutate state and trigger updates.\n  // They can be bound as event listeners in templates.\n  methods: {\n    increment() {\n      this.count++\n    }\n  },\n\n  // Lifecycle hooks are called at different stages\n  // of a component's lifecycle.\n  // This function will be called when the component is mounted.\n  mounted() {\n    console.log(`The initial count is ${this.count}.`)\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"increment\"&gt;Count is: {{ count }}&lt;/button&gt;\n&lt;/template&gt;\n</code></pre> <p>The Options API is centered around the concept of a \"component instance\" (<code>this</code> as seen in the example), which typically aligns better with a class-based mental model for users coming from OOP language backgrounds. It is also more beginner-friendly by abstracting away the reactivity details and enforcing code organisation via option groups.</p>"}, {"location": "vuejs/#composition-api", "title": "Composition API", "text": "<p>With Composition API, we define a component's logic using imported API functions. In SFCs, Composition API is typically used with <code>&lt;script setup&gt;</code>. The setup attribute is a hint that makes Vue perform compile-time transforms that allow us to use Composition API with less boilerplate. For example, imports and top-level variables / functions declared in <code>&lt;script setup&gt;</code> are directly usable in the template.</p> <p>Here is the same component, with the exact same template, but using Composition API and <code>&lt;script setup&gt;</code> instead:</p> <pre><code>&lt;script setup&gt;\nimport { ref, onMounted } from 'vue'\n\n// reactive state\nconst count = ref(0)\n\n// functions that mutate state and trigger updates\nfunction increment() {\n  count.value++\n}\n\n// lifecycle hooks\nonMounted(() =&gt; {\n  console.log(`The initial count is ${count.value}.`)\n})\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"increment\"&gt;Count is: {{ count }}&lt;/button&gt;\n&lt;/template&gt;\n</code></pre> <p>The Composition API is centered around declaring reactive state variables directly in a function scope, and composing state from multiple functions together to handle complexity. It is more free-form, and requires understanding of how reactivity works in Vue to be used effectively. In return, its flexibility enables more powerful patterns for organizing and reusing logic.</p>"}, {"location": "vuejs/#initialize-a-project", "title": "Initialize a project", "text": "<p>To create a build-tool-enabled Vue project on your machine, run the following command in your command line. If you don't have <code>npm</code> follow these instructions.</p> <pre><code>npm init vue@latest\n</code></pre> <p>This command will install and execute create-vue, the official Vue project scaffolding tool. You will be presented with prompts for a number of optional features such as TypeScript and testing support. If you are unsure about an option, simply choose <code>No</code>. Follow their instructions.</p> <p>Once the project is created, follow the instructions to install dependencies and start the dev server:</p> <pre><code>cd &lt;your-project-name&gt;\nnpm install\nnpm run dev\n</code></pre> <p>When you are ready to ship your app to production, run the following:</p> <pre><code>npm run build\n</code></pre>"}, {"location": "vuejs/#the-basics", "title": "The basics", "text": ""}, {"location": "vuejs/#creating-a-vue-application", "title": "Creating a Vue Application", "text": "<p>Every Vue application starts by creating a new application instance with the <code>createApp</code> function:</p> <pre><code>import { createApp } from 'vue'\n\nconst app = createApp({\n  /* root component options */\n})\n</code></pre> <p>The object we are passing into <code>createApp</code> is in fact a component. Every app requires a \"root component\" that can contain other components as its children.</p> <p>If you are using Single-File Components, we typically import the root component from another file:</p> <pre><code>import { createApp } from 'vue'\n// import the root component App from a single-file component.\nimport App from './App.vue'\n\nconst app = createApp(App)\n</code></pre> <p>An application instance won't render anything until its <code>.mount()</code> method is called. It expects a \"container\" argument, which can either be an actual DOM element or a selector string:</p> <pre><code>&lt;div id=\"app\"&gt;&lt;/div&gt;\n</code></pre> <pre><code>app.mount('#app')\n</code></pre> <p>The content of the app's root component will be rendered inside the container element. The container element itself is not considered part of the app.</p> <p>The <code>.mount()</code> method should always be called after all app configurations and asset registrations are done. Also note that its return value, unlike the asset registration methods, is the root component instance instead of the application instance.</p> <p>You are not limited to a single application instance on the same page. The <code>createApp</code> API allows multiple Vue applications to co-exist on the same page, each with its own scope for configuration and global assets:</p> <pre><code>const app1 = createApp({\n  /* ... */\n})\napp1.mount('#container-1')\n\nconst app2 = createApp({\n  /* ... */\n})\napp2.mount('#container-2')\n</code></pre>"}, {"location": "vuejs/#app-configurations", "title": "App configurations", "text": "<p>The application instance exposes a <code>.config</code> object that allows us to configure a few app-level options, for example defining an app-level error handler that captures errors from all descendent components:</p> <pre><code>app.config.errorHandler = (err) =&gt; {\n  /* handle error */\n}\n</code></pre> <p>The application instance also provides a few methods for registering app-scoped assets. For example, registering a component:</p> <pre><code>app.component('TodoDeleteButton', TodoDeleteButton)\n</code></pre> <p>This makes the <code>TodoDeleteButton</code> available for use anywhere in our app.</p> <p>You can use also environment variables</p>"}, {"location": "vuejs/#declarative-rendering", "title": "Declarative rendering", "text": "<p>The core feature of Vue is declarative rendering: using a template syntax that extends HTML, we can describe how the HTML should look like based on JavaScript state. When the state changes, the HTML updates automatically.</p> <p>State that can trigger updates when changed are considered reactive. In Vue, reactive state is held in components.</p> <p>We can declare reactive state using the data component option, which should be a function that returns an object:</p> <pre><code>export default {\n  data() {\n    return {\n      message: 'Hello World!'\n    }\n  }\n}\n</code></pre> <p>The message property will be made available in the template. This is how we can render dynamic text based on the value of message, using mustaches syntax:</p> <pre><code>&lt;h1&gt;{{ message }}&lt;/h1&gt;\n</code></pre> <p>The double mustaches interprets the data as plain text, not HTML. In order to output real HTML, you will need to use the <code>v-html</code> directive, although you should try to avoid it for security reasons.</p> <p>Directives are prefixed with <code>v-</code> to indicate that they are special attributes provided by Vue, they apply special reactive behavior to the rendered DOM.</p>"}, {"location": "vuejs/#attribute-bindings", "title": "Attribute bindings", "text": "<p>To bind an attribute to a dynamic value, we use the <code>v-bind</code> directive:</p> <pre><code>&lt;div v-bind:id=\"dynamicId\"&gt;&lt;/div&gt;\n</code></pre> <p>A directive is a special attribute that starts with the <code>v-</code> prefix. They are part of Vue's template syntax. Similar to text interpolations, directive values are JavaScript expressions that have access to the component's state.</p> <p>The part after the colon (<code>:id</code>) is the \"argument\" of the directive. Here, the element's <code>id</code> attribute will be synced with the <code>dynamicId</code> property from the component's state.</p> <p>Because <code>v-bind</code> is used so frequently, it has a dedicated shorthand syntax:</p> <pre><code>&lt;div :id=\"dynamicId\"&gt;&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#class-binding", "title": "Class binding", "text": "<p>For example to turn the <code>h1</code> in red:</p> <pre><code>&lt;script&gt;\nexport default {\n  data() {\n    return {\n      titleClass: 'title'\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;h1 :class='titleClass'&gt;Make me red&lt;/h1&gt; &lt;!-- add dynamic class binding here --&gt;\n&lt;/template&gt;\n\n&lt;style&gt;\n.title {\n  color: red;\n}\n&lt;/style&gt;\n</code></pre> <p>You can have multiple classes toggled by having more fields in the object. In addition, the <code>:class</code> directive can also co-exist with the plain class attribute. So given the following state:</p> <pre><code>data() {\n  return {\n    isActive: true,\n    hasError: false\n  }\n}\n</code></pre> <p>And the following template:</p> <pre><code>&lt;div\n  class=\"static\"\n  :class=\"{ active: isActive, 'text-danger': hasError }\"\n&gt;&lt;/div&gt;\n</code></pre> <p>It will render:</p> <pre><code>&lt;div class=\"static active\"&gt;&lt;/div&gt;\n</code></pre> <p>When <code>isActive</code> or <code>hasError</code> changes, the class list will be updated accordingly. For example, if <code>hasError</code> becomes true, the class list will become <code>static active text-danger</code>.</p> <p>The bound object doesn't have to be inline:</p> <pre><code>data() {\n  return {\n    classObject: {\n      active: true,\n      'text-danger': false\n    }\n  }\n}\n</code></pre> <pre><code>&lt;div :class=\"classObject\"&gt;&lt;/div&gt;\n</code></pre> <p>This will render the same result. We can also bind to a computed property that returns an object. This is a common and powerful pattern:</p> <pre><code>data() {\n  return {\n    isActive: true,\n    error: null\n  }\n},\ncomputed: {\n  classObject() {\n    return {\n      active: this.isActive &amp;&amp; !this.error,\n      'text-danger': this.error &amp;&amp; this.error.type === 'fatal'\n    }\n  }\n}\n</code></pre> <pre><code>&lt;div :class=\"classObject\"&gt;&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#style-binding", "title": "Style binding", "text": "<p><code>:style</code> supports binding to JavaScript object values.</p> <pre><code>data() {\n  return {\n    activeColor: 'red',\n    fontSize: 30\n  }\n}\n</code></pre> <pre><code>&lt;div :style=\"{ color: activeColor, fontSize: fontSize + 'px' }\"&gt;&lt;/div&gt;\n</code></pre> <p>It is often a good idea to bind to a style object directly so that the template is cleaner:</p> <pre><code>data() {\n  return {\n    styleObject: {\n      color: 'red',\n      fontSize: '13px'\n    }\n  }\n}\n</code></pre> <pre><code>&lt;div :style=\"styleObject\"&gt;&lt;/div&gt;\n</code></pre> <p>Again, object style binding is often used in conjunction with computed properties that return objects.</p>"}, {"location": "vuejs/#event-listeners", "title": "Event listeners", "text": "<p>We can listen to DOM events using the <code>v-on</code> directive:</p> <pre><code>&lt;button v-on:click=\"increment\"&gt;{{ count }}&lt;/button&gt;\n</code></pre> <p>Due to its frequent use, <code>v-on</code> also has a shorthand syntax:</p> <pre><code>&lt;button @click=\"increment\"&gt;{{ count }}&lt;/button&gt;\n</code></pre> <p>Here, <code>increment</code> references a function declared using the methods option:</p> <pre><code>export default {\n  data() {\n    return {\n      count: 0\n    }\n  },\n  methods: {\n    increment() {\n      // update component state\n      this.count++\n    }\n  }\n}\n</code></pre> <p>Inside a method, we can access the component instance using <code>this</code>. The component instance exposes the data properties declared by data. We can update the component state by mutating these properties.</p> <p>You should avoid using arrow functions when defining methods, as that prevents Vue from binding the appropriate this value.</p>"}, {"location": "vuejs/#event-modifiers", "title": "Event modifiers", "text": "<p>It is a very common need to call <code>event.preventDefault()</code> or <code>event.stopPropagation()</code> inside event handlers. Although we can do this easily inside methods, it would be better if the methods can be purely about data logic rather than having to deal with DOM event details.</p> <p>To address this problem, Vue provides event modifiers for <code>v-on</code>. Recall that modifiers are directive postfixes denoted by a dot.</p> <ul> <li><code>.stop</code></li> <li><code>.prevent</code></li> <li><code>.self</code></li> <li><code>.capture</code></li> <li><code>.once</code></li> <li><code>.passive</code></li> </ul> <pre><code>&lt;!-- the click event's propagation will be stopped --&gt;\n&lt;a @click.stop=\"doThis\"&gt;&lt;/a&gt;\n\n&lt;!-- the submit event will no longer reload the page --&gt;\n&lt;form @submit.prevent=\"onSubmit\"&gt;&lt;/form&gt;\n\n&lt;!-- modifiers can be chained --&gt;\n&lt;a @click.stop.prevent=\"doThat\"&gt;&lt;/a&gt;\n\n&lt;!-- just the modifier --&gt;\n&lt;form @submit.prevent&gt;&lt;/form&gt;\n\n&lt;!-- only trigger handler if event.target is the element itself --&gt;\n&lt;!-- i.e. not from a child element --&gt;\n&lt;div @click.self=\"doThat\"&gt;...&lt;/div&gt;\n\n&lt;!-- use capture mode when adding the event listener --&gt;\n&lt;!-- i.e. an event targeting an inner element is handled here before being handled by that element --&gt;\n&lt;div @click.capture=\"doThis\"&gt;...&lt;/div&gt;\n\n&lt;!-- the click event will be triggered at most once --&gt;\n&lt;a @click.once=\"doThis\"&gt;&lt;/a&gt;\n\n&lt;!-- the scroll event's default behavior (scrolling) will happen --&gt;\n&lt;!-- immediately, instead of waiting for `onScroll` to complete  --&gt;\n&lt;!-- in case it contains `event.preventDefault()`                --&gt;\n&lt;div @scroll.passive=\"onScroll\"&gt;...&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#key-modifiers", "title": "Key Modifiers", "text": "<p>When listening for keyboard events, we often need to check for specific keys. Vue allows adding key modifiers for <code>v-on</code> or <code>@</code> when listening for key events:</p> <pre><code>&lt;!-- only call `vm.submit()` when the `key` is `Enter` --&gt;\n&lt;input @keyup.enter=\"submit\" /&gt;\n</code></pre> <p>You can directly use any valid key names exposed via <code>KeyboardEvent.key</code> as modifiers by converting them to kebab-case.</p> <pre><code>&lt;input @keyup.page-down=\"onPageDown\" /&gt;\n</code></pre> <p>Vue provides aliases for the most commonly used keys:</p> <ul> <li><code>.enter</code></li> <li><code>.tab</code></li> <li><code>.delete</code> (captures both \"Delete\" and \"Backspace\" keys)</li> <li><code>.esc</code></li> <li><code>.space</code></li> <li><code>.up</code></li> <li><code>.down</code></li> <li><code>.left</code></li> <li><code>.right</code></li> </ul> <p>You can use the following modifiers to trigger mouse or keyboard event listeners only when the corresponding modifier key is pressed:</p> <ul> <li><code>.ctrl</code></li> <li><code>.alt</code></li> <li><code>.shift</code></li> <li><code>.meta</code></li> </ul> <p>For example:</p> <pre><code>&lt;!-- Alt + Enter --&gt;\n&lt;input @keyup.alt.enter=\"clear\" /&gt;\n\n&lt;!-- Ctrl + Click --&gt;\n&lt;div @click.ctrl=\"doSomething\"&gt;Do something&lt;/div&gt;\n</code></pre> <p>The <code>.exact</code> modifier allows control of the exact combination of system modifiers needed to trigger an event.</p> <pre><code>&lt;!-- this will fire even if Alt or Shift is also pressed --&gt;\n&lt;button @click.ctrl=\"onClick\"&gt;A&lt;/button&gt;\n\n&lt;!-- this will only fire when Ctrl and no other keys are pressed --&gt;\n&lt;button @click.ctrl.exact=\"onCtrlClick\"&gt;A&lt;/button&gt;\n\n&lt;!-- this will only fire when no system modifiers are pressed --&gt;\n&lt;button @click.exact=\"onClick\"&gt;A&lt;/button&gt;\n</code></pre>"}, {"location": "vuejs/#mouse-button-modifiers", "title": "Mouse Button Modifiers", "text": "<ul> <li><code>.left</code></li> <li><code>.right</code></li> <li><code>.middle</code></li> </ul> <p>These modifiers restrict the handler to events triggered by a specific mouse button.</p>"}, {"location": "vuejs/#form-bindings", "title": "Form bindings", "text": ""}, {"location": "vuejs/#basic-usage", "title": "Basic usage", "text": ""}, {"location": "vuejs/#text", "title": "Text", "text": "<p>Using <code>v-bind</code> and <code>v-on</code> together, we can create two-way bindings on form input elements:</p> <pre><code>&lt;input :value=\"text\" @input=\"onInput\"&gt;\n&lt;p&gt;{{ text }}&lt;/p&gt;\n</code></pre> <pre><code>methods: {\n  onInput(e) {\n    // a v-on handler receives the native DOM event\n    // as the argument.\n    this.text = e.target.value\n  }\n}\n</code></pre> <p>To simplify two-way bindings, Vue provides a directive, <code>v-model</code>, which is essentially a syntax sugar for the above:</p> <pre><code>&lt;input v-model=\"text\"&gt;\n</code></pre> <p><code>v-model</code> automatically syncs the <code>&lt;input&gt;</code>'s value with the bound state, so we no longer need to use a event handler for that.</p> <p><code>v-model</code> works not only on text inputs, but also other input types such as checkboxes, radio buttons, and select dropdowns.</p>"}, {"location": "vuejs/#multiline-text", "title": "Multiline text", "text": "<pre><code>&lt;span&gt;Multiline message is:&lt;/span&gt;\n&lt;p style=\"white-space: pre-line;\"&gt;{{ message }}&lt;/p&gt;\n&lt;textarea v-model=\"message\" placeholder=\"add multiple lines\"&gt;&lt;/textarea&gt;\n</code></pre>"}, {"location": "vuejs/#checkbox", "title": "Checkbox", "text": "<pre><code>&lt;input type=\"checkbox\" id=\"checkbox\" v-model=\"checked\" /&gt;\n&lt;label for=\"checkbox\"&gt;{{ checked }}&lt;/label&gt;\n</code></pre> <p>We can also bind multiple checkboxes to the same array or Set value:</p> <pre><code>export default {\n  data() {\n    return {\n      checkedNames: []\n    }\n  }\n}\n</code></pre> <pre><code>&lt;div&gt;Checked names: {{ checkedNames }}&lt;/div&gt;\n\n&lt;input type=\"checkbox\" id=\"jack\" value=\"Jack\" v-model=\"checkedNames\"&gt;\n&lt;label for=\"jack\"&gt;Jack&lt;/label&gt;\n\n&lt;input type=\"checkbox\" id=\"john\" value=\"John\" v-model=\"checkedNames\"&gt;\n&lt;label for=\"john\"&gt;John&lt;/label&gt;\n\n&lt;input type=\"checkbox\" id=\"mike\" value=\"Mike\" v-model=\"checkedNames\"&gt;\n&lt;label for=\"mike\"&gt;Mike&lt;/label&gt;\n</code></pre>"}, {"location": "vuejs/#radio-checkboxes", "title": "Radio checkboxes", "text": "<pre><code>&lt;div&gt;Picked: {{ picked }}&lt;/div&gt;\n\n&lt;input type=\"radio\" id=\"one\" value=\"One\" v-model=\"picked\" /&gt;\n&lt;label for=\"one\"&gt;One&lt;/label&gt;\n\n&lt;input type=\"radio\" id=\"two\" value=\"Two\" v-model=\"picked\" /&gt;\n&lt;label for=\"two\"&gt;Two&lt;/label&gt;\n</code></pre>"}, {"location": "vuejs/#select", "title": "Select", "text": "<p>Single select:</p> <pre><code>&lt;div&gt;Selected: {{ selected }}&lt;/div&gt;\n\n&lt;select v-model=\"selected\"&gt;\n  &lt;option disabled value=\"\"&gt;Please select one&lt;/option&gt;\n  &lt;option&gt;A&lt;/option&gt;\n  &lt;option&gt;B&lt;/option&gt;\n  &lt;option&gt;C&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p>Multiple select (bound to array):</p> <pre><code>&lt;div&gt;Selected: {{ selected }}&lt;/div&gt;\n\n&lt;select v-model=\"selected\" multiple&gt;\n  &lt;option&gt;A&lt;/option&gt;\n  &lt;option&gt;B&lt;/option&gt;\n  &lt;option&gt;C&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p>Select options can be dynamically rendered with <code>v-for</code>:</p> <pre><code>export default {\n  data() {\n    return {\n      selected: 'A',\n      options: [\n        { text: 'One', value: 'A' },\n        { text: 'Two', value: 'B' },\n        { text: 'Three', value: 'C' }\n      ]\n    }\n  }\n}\n</code></pre> <pre><code>&lt;select v-model=\"selected\"&gt;\n  &lt;option v-for=\"option in options\" :value=\"option.value\"&gt;\n    {{ option.text }}\n  &lt;/option&gt;\n&lt;/select&gt;\n\n&lt;div&gt;Selected: {{ selected }}&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#value-bindings", "title": "Value bindings", "text": "<p>For radio, checkbox and select options, the <code>v-model</code> binding values are usually static strings (or booleans for checkbox):.</p> <pre><code>&lt;!-- `picked` is a string \"a\" when checked --&gt;\n&lt;input type=\"radio\" v-model=\"picked\" value=\"a\" /&gt;\n\n&lt;!-- `toggle` is either true or false --&gt;\n&lt;input type=\"checkbox\" v-model=\"toggle\" /&gt;\n\n&lt;!-- `selected` is a string \"abc\" when the first option is selected --&gt;\n&lt;select v-model=\"selected\"&gt;\n  &lt;option value=\"abc\"&gt;ABC&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p>But sometimes we may want to bind the value to a dynamic property on the current active instance. We can use <code>v-bind</code> to achieve that. In addition, using <code>v-bind</code> allows us to bind the input value to non-string values.</p>"}, {"location": "vuejs/#checkbox_1", "title": "Checkbox", "text": "<pre><code>&lt;input\n  type=\"checkbox\"\n  v-model=\"toggle\"\n  true-value=\"yes\"\n  false-value=\"no\" /&gt;\n</code></pre> <p><code>true-value</code> and <code>false-value</code> are Vue-specific attributes that only work with <code>v-model</code>. Here the toggle property's value will be set to 'yes' when the box is checked, and set to 'no' when unchecked. You can also bind them to dynamic values using <code>v-bind</code>:</p> <pre><code>&lt;input\n  type=\"checkbox\"\n  v-model=\"toggle\"\n  :true-value=\"dynamicTrueValue\"\n  :false-value=\"dynamicFalseValue\" /&gt;\n</code></pre>"}, {"location": "vuejs/#radio", "title": "Radio", "text": "<pre><code>&lt;input type=\"radio\" v-model=\"pick\" :value=\"first\" /&gt;\n&lt;input type=\"radio\" v-model=\"pick\" :value=\"second\" /&gt;\n</code></pre> <p><code>pick</code> will be set to the value of first when the first radio input is checked, and set to the value of second when the second one is checked.</p>"}, {"location": "vuejs/#select-options", "title": "Select Options", "text": "<pre><code>&lt;select v-model=\"selected\"&gt;\n  &lt;!-- inline object literal --&gt;\n  &lt;option :value=\"{ number: 123 }\"&gt;123&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p><code>v-model</code> supports value bindings of non-string values as well! In the above example, when the option is selected, selected will be set to the object literal value of <code>{ number: 123 }</code>.</p>"}, {"location": "vuejs/#form-modifiers", "title": "Form modifiers", "text": ""}, {"location": "vuejs/#lazy", "title": "<code>.lazy</code>", "text": "<p>By default, <code>v-model</code> syncs the input with the data after each input event. You can add the lazy modifier to instead sync after change events:</p> <pre><code>&lt;!-- synced after \"change\" instead of \"input\" --&gt;\n&lt;input v-model.lazy=\"msg\" /&gt;\n</code></pre>"}, {"location": "vuejs/#number", "title": "<code>.number</code>", "text": "<p>If you want user input to be automatically typecast as a number, you can add the number modifier to your <code>v-model</code> managed inputs:</p> <pre><code>&lt;input v-model.number=\"age\" /&gt;\n</code></pre> <p>If the value cannot be parsed with <code>parseFloat()</code>, then the original value is used instead.</p> <p>The number modifier is applied automatically if the input has <code>type=\"number\"</code>.</p>"}, {"location": "vuejs/#trim", "title": "<code>.trim</code>", "text": "<p>If you want whitespace from user input to be trimmed automatically, you can add the trim modifier to your <code>v-model</code> managed inputs:</p> <pre><code>&lt;input v-model.trim=\"msg\" /&gt;\n</code></pre>"}, {"location": "vuejs/#conditional-rendering", "title": "Conditional rendering", "text": "<p>We can use the <code>v-if</code> directive to conditionally render an element:</p> <pre><code>&lt;h1 v-if=\"awesome\"&gt;Vue is awesome!&lt;/h1&gt;\n</code></pre> <p>This <code>&lt;h1&gt;</code> will be rendered only if the value of <code>awesome</code> is truthy. If awesome changes to a falsy value, it will be removed from the DOM.</p> <p>We can also use <code>v-else</code> and <code>v-else-if</code> to denote other branches of the condition:</p> <pre><code>&lt;h1 v-if=\"awesome\"&gt;Vue is awesome!&lt;/h1&gt;\n&lt;h1 v-else&gt;Oh no \ud83d\ude22&lt;/h1&gt;\n</code></pre> <p>Because <code>v-if</code> is a directive, it has to be attached to a single element. But what if we want to toggle more than one element? In this case we can use <code>v-if</code> on a <code>&lt;template&gt;</code> element, which serves as an invisible wrapper. The final rendered result will not include the <code>&lt;template&gt;</code> element.</p> <pre><code>&lt;template v-if=\"ok\"&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p&gt;Paragraph 1&lt;/p&gt;\n  &lt;p&gt;Paragraph 2&lt;/p&gt;\n&lt;/template&gt;\n</code></pre> <p>Another option for conditionally displaying an element is the <code>v-show</code> directive. The usage is largely the same:</p> <pre><code>&lt;h1 v-show=\"ok\"&gt;Hello!&lt;/h1&gt;\n</code></pre> <p>The difference is that an element with <code>v-show</code> will always be rendered and remain in the DOM; <code>v-show</code> only toggles the display CSS property of the element.</p> <p><code>v-show</code> doesn't support the <code>&lt;template&gt;</code> element, nor does it work with <code>v-else</code>.</p> <p>Generally speaking, <code>v-if</code> has higher toggle costs while <code>v-show</code> has higher initial render costs. So prefer <code>v-show</code> if you need to toggle something very often, and prefer <code>v-if</code> if the condition is unlikely to change at runtime.</p>"}, {"location": "vuejs/#list-rendering", "title": "List rendering", "text": "<p>We can use the <code>v-for</code> directive to render a list of elements based on a source array:</p> <pre><code>&lt;ul&gt;\n  &lt;li v-for=\"todo in todos\" :key=\"todo.id\"&gt;\n    {{ todo.text }}\n  &lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> <p>Here <code>todo</code> is a local variable representing the array element currently being iterated on. It's only accessible on or inside the <code>v-for</code> element.</p> <p>Notice how we are also giving each todo object a unique <code>id</code>, and binding it as the special key attribute for each <code>&lt;li&gt;</code>. The key allows Vue to accurately move each <code>&lt;li&gt;</code> to match the position of its corresponding object in the array.</p> <p>There are two ways to update the list:</p> <ul> <li> <p>Call mutating methods on the source array:</p> <pre><code>this.todos.push(newTodo)\n</code></pre> </li> <li> <p>Replace the array with a new one:</p> <pre><code>this.todos = this.todos.filter(/* ... */)\n</code></pre> </li> </ul> <p>Example:</p> <pre><code>&lt;script&gt;\n// give each todo a unique id\nlet id = 0\n\nexport default {\n  data() {\n    return {\n      newTodo: '',\n      todos: [\n        { id: id++, text: 'Learn HTML' },\n        { id: id++, text: 'Learn JavaScript' },\n        { id: id++, text: 'Learn Vue' }\n      ]\n    }\n  },\n  methods: {\n    addTodo() {\n      this.todos.push({ id: id++, text: this.newTodo})\n      this.newTodo = ''\n    },\n    removeTodo(todo) {\n      this.todos = this.todos.filter((element) =&gt; element.id != todo.id)\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;form @submit.prevent=\"addTodo\"&gt;\n    &lt;input v-model=\"newTodo\"&gt;\n    &lt;button&gt;Add Todo&lt;/button&gt;\n  &lt;/form&gt;\n  &lt;ul&gt;\n    &lt;li v-for=\"todo in todos\" :key=\"todo.id\"&gt;\n      {{ todo.text }}\n      &lt;button @click=\"removeTodo(todo)\"&gt;X&lt;/button&gt;\n    &lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/template&gt;\n</code></pre> <p><code>v-for</code> also supports an optional second alias for the index of the current item:</p> <pre><code>data() {\n  return {\n    parentMessage: 'Parent',\n    items: [{ message: 'Foo' }, { message: 'Bar' }]\n  }\n}\n</code></pre> <pre><code>&lt;li v-for=\"(item, index) in items\"&gt;\n  {{ parentMessage }} - {{ index }} - {{ item.message }}\n&lt;/li&gt;\n</code></pre> <p>Similar to template <code>v-if</code>, you can also use a <code>&lt;template&gt;</code> tag with <code>v-for</code> to render a block of multiple elements. For example:</p> <pre><code>&lt;ul&gt;\n  &lt;template v-for=\"item in items\"&gt;\n    &lt;li&gt;{{ item.msg }}&lt;/li&gt;\n    &lt;li class=\"divider\" role=\"presentation\"&gt;&lt;/li&gt;\n  &lt;/template&gt;\n&lt;/ul&gt;\n</code></pre> <p>It's not recommended to use <code>v-if</code> and <code>v-for</code> on the same element due to implicit precedence. Instead of:</p> <pre><code>&lt;li v-for=\"todo in todos\" v-if=\"!todo.isComplete\"&gt;\n  {{ todo.name }}\n&lt;/li&gt;\n</code></pre> <p>Use:</p> <pre><code>&lt;template v-for=\"todo in todos\"&gt;\n  &lt;li v-if=\"!todo.isComplete\"&gt;\n    {{ todo.name }}\n  &lt;/li&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuejs/#v-for-with-an-object", "title": "<code>v-for</code> with an object", "text": "<p>You can also use <code>v-for</code> to iterate through the properties of an object.</p> <pre><code>data() {\n  return {\n    myObject: {\n      title: 'How to do lists in Vue',\n      author: 'Jane Doe',\n      publishedAt: '2016-04-10'\n    }\n  }\n}\n</code></pre> <pre><code>&lt;ul&gt;\n  &lt;li v-for=\"value in myObject\"&gt;\n    {{ value }}\n  &lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> <p>You can also provide a second alias for the property's name:</p> <pre><code>&lt;li v-for=\"(value, key) in myObject\"&gt;\n  {{ key }}: {{ value }}\n&lt;/li&gt;\n</code></pre> <p>And another for the index:</p> <pre><code>&lt;li v-for=\"(value, key, index) in myObject\"&gt;\n  {{ index }}. {{ key }}: {{ value }}\n&lt;/li&gt;\n</code></pre>"}, {"location": "vuejs/#v-for-with-a-range", "title": "v-for with a Range", "text": "<p><code>v-for</code> can also take an integer. In this case it will repeat the template that many times, based on a range of <code>1...n</code>.</p> <pre><code>&lt;span v-for=\"n in 10\"&gt;{{ n }}&lt;/span&gt;\n</code></pre> <p>Note here <code>n</code> starts with an initial value of 1 instead of 0.</p>"}, {"location": "vuejs/#v-for-with-a-component", "title": "<code>v-for</code> with a Component", "text": "<p>You can directly use <code>v-for</code> on a component, like any normal element (don't forget to provide a key):</p> <pre><code>&lt;my-component v-for=\"item in items\" :key=\"item.id\"&gt;&lt;/my-component&gt;\n</code></pre> <p>However, this won't automatically pass any data to the component, because components have isolated scopes of their own. In order to pass the iterated data into the component, we should also use props:</p> <pre><code>&lt;my-component\n  v-for=\"(item, index) in items\"\n  :item=\"item\"\n  :index=\"index\"\n  :key=\"item.id\"\n&gt;&lt;/my-component&gt;\n</code></pre> <p>The reason for not automatically injecting item into the component is because that makes the component tightly coupled to how <code>v-for</code> works. Being explicit about where its data comes from makes the component reusable in other situations.</p>"}, {"location": "vuejs/#computed-property", "title": "Computed Property", "text": "<p>We can declare a property that is reactively computed from other properties using the <code>computed</code> option:</p> <pre><code>export default {\n  // ...\n  computed: {\n    filteredTodos() {\n        if (this.hideCompleted) {\n            return this.todos.filter((t) =&gt; t.done === false)\n          } else {\n            return this.todos\n          }\n        }\n    }\n  }\n}\n</code></pre> <p>A computed property tracks other reactive state used in its computation as dependencies. It caches the result and automatically updates it when its dependencies change. So it's better than defining the function as a <code>method</code></p>"}, {"location": "vuejs/#lifecycle-hooks", "title": "Lifecycle hooks", "text": "<p>Each Vue component instance goes through a series of initialization steps when it's created - for example, it needs to set up data observation, compile the template, mount the instance to the DOM, and update the DOM when data changes. Along the way, it also runs functions called lifecycle hooks, giving users the opportunity to add their own code at specific stages.</p> <p>For example, the <code>mounted</code> hook can be used to run code after the component has finished the initial rendering and created the DOM nodes:</p> <pre><code>export default {\n  mounted() {\n    console.log(`the component is now mounted.`)\n  }\n}\n</code></pre> <p>There are also other hooks which will be called at different stages of the instance's lifecycle, with the most commonly used being <code>mounted</code>, <code>updated</code>, and <code>unmounted</code>.</p> <p>All lifecycle hooks are called with their <code>this</code> context pointing to the current active instance invoking it. Note this means you should avoid using arrow functions when declaring lifecycle hooks, as you won't be able to access the component instance via this if you do so.</p>"}, {"location": "vuejs/#template-refs", "title": "Template Refs", "text": "<p>While Vue's declarative rendering model abstracts away most of the direct DOM operations for you, there may still be cases where we need direct access to the underlying DOM elements. To achieve this, we can use the special <code>ref</code> attribute:</p> <pre><code>&lt;input ref=\"input\"&gt;\n</code></pre> <p><code>ref</code> allows us to obtain a direct reference to a specific DOM element or child component instance after it's mounted. This may be useful when you want to, for example, programmatically focus an input on component mount, or initialize a 3<sup>rd</sup> party library on an element.</p> <p>The resulting ref is exposed on <code>this.$refs</code>:</p> <pre><code>&lt;script&gt;\nexport default {\n  mounted() {\n    this.$refs.input.focus()\n  }\n}\n&lt;/script&gt;\n</code></pre> <pre><code>&lt;template&gt;\n  &lt;input ref=\"input\" /&gt;\n&lt;/template&gt;\n</code></pre> <p>Note that you can only access the <code>ref</code> after the component is mounted. If you try to access <code>$refs.input</code> in a template expression, it will be <code>null</code> on the first render. This is because the element doesn't exist until after the first render!</p>"}, {"location": "vuejs/#watchers", "title": "Watchers", "text": "<p>Computed properties allow us to declaratively compute derived values. However, there are cases where we need to perform \"side effects\" in reaction to state changes, for example, mutating the DOM, or changing another piece of state based on the result of an async operation.</p> <p>With Options API, we can use the <code>watch</code> option to trigger a function whenever a reactive property changes:</p> <pre><code>export default {\n  data() {\n    return {\n      question: '',\n      answer: 'Questions usually contain a question mark. ;-)'\n    }\n  },\n  watch: {\n    // whenever question changes, this function will run\n    question(newQuestion, oldQuestion) {\n      if (newQuestion.indexOf('?') &gt; -1) {\n        this.getAnswer()\n      }\n    }\n  },\n  methods: {\n    async getAnswer() {\n      this.answer = 'Thinking...'\n      try {\n        const res = await fetch('https://yesno.wtf/api')\n        this.answer = (await res.json()).answer\n      } catch (error) {\n        this.answer = 'Error! Could not reach the API. ' + error\n      }\n    }\n  }\n}\n</code></pre> <pre><code>&lt;p&gt;\n  Ask a yes/no question:\n  &lt;input v-model=\"question\" /&gt;\n&lt;/p&gt;\n&lt;p&gt;{{ answer }}&lt;/p&gt;\n</code></pre>"}, {"location": "vuejs/#deep-watchers", "title": "Deep watchers", "text": "<p><code>watch</code> is shallow by default: the callback will only trigger when the watched property has been assigned a new value - it won't trigger on nested property changes. If you want the callback to fire on all nested mutations, you need to use a deep watcher:</p> <pre><code>export default {\n  watch: {\n    someObject: {\n      handler(newValue, oldValue) {\n        // Note: `newValue` will be equal to `oldValue` here\n        // on nested mutations as long as the object itself\n        // hasn't been replaced.\n      },\n      deep: true\n    }\n  }\n}\n</code></pre> <p>Note</p> <pre><code>\"Deep watch requires traversing all nested properties in the watched object, and can be expensive when used on large data structures. Use it only when necessary and beware of the performance implications.\"\n</code></pre>"}, {"location": "vuejs/#eager-watchers", "title": "Eager watchers", "text": "<p><code>watch</code> is lazy by default: the callback won't be called until the watched source has changed. But in some cases we may want the same callback logic to be run eagerly, for example, we may want to fetch some initial data, and then re-fetch the data whenever relevant state changes.</p> <p>We can force a watcher's callback to be executed immediately by declaring it using an object with a <code>handler</code> function and the <code>immediate: true</code> option:</p> <pre><code>export default {\n  // ...\n  watch: {\n    question: {\n      handler(newQuestion) {\n        // this will be run immediately on component creation.\n      },\n      // force eager callback execution\n      immediate: true\n    }\n  }\n  // ...\n}\n</code></pre>"}, {"location": "vuejs/#environment-variables", "title": "Environment variables", "text": "<p>If you're using Vue 3 and Vite you can use the environment variables by defining them in <code>.env</code> files.</p> <p>You can specify environment variables by placing the following files in your project root:</p> <ul> <li><code>.env</code>: Loaded in all cases.</li> <li><code>.env.local</code>: Loaded in all cases, ignored by git.</li> <li><code>.env.[mode]</code>: Only loaded in specified mode.</li> <li><code>.env.[mode].local</code>: Only loaded in specified mode, ignored by git.</li> </ul> <p>An env file simply contains <code>key=value</code> pairs of environment variables, by default only variables that start with <code>VITE_</code> will be exposed.:</p> <pre><code>DB_PASSWORD=foobar\nVITE_SOME_KEY=123\n</code></pre> <p>Only <code>VITE_SOME_KEY</code> will be exposed as <code>import.meta.env.VITE_SOME_KEY</code> to your client source code, but <code>DB_PASSWORD</code> will not. So for example in a component you can use:</p> <pre><code>export default {\n  props: {},\n  mounted() {\n    console.log(import.meta.env.VITE_SOME_KEY)\n  },\n  data: () =&gt; ({\n  }),\n}\n</code></pre>"}, {"location": "vuejs/#make-http-requests", "title": "Make HTTP requests", "text": "<p>There are many ways to do requests to external services:</p> <ul> <li>Fetch API</li> <li>Axios</li> </ul>"}, {"location": "vuejs/#fetch-api", "title": "Fetch API", "text": "<p>The Fetch API is a standard API for making HTTP requests on the browser.</p> <p>It a great alternative to the old <code>XMLHttpRequestconstructor</code> for making requests.</p> <p>It supports all kinds of requests, including GET, POST, PUT, PATCH, DELETE, and OPTIONS, which is what most people need.</p> <p>To make a request with the Fetch API, we don\u2019t have to do anything. All we have to do is to make the request directly with the <code>fetch</code> object. For instance, you can write:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    {{data}}\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;export default {\n  name: \"App\",\n  data() {\n    return {\n      data: {}\n    }\n  },\n  beforeMount(){\n    this.getName();\n  },\n  methods: {\n    async getName(){\n      const res = await fetch('https://api.agify.io/?name=michael');\n      const data = await res.json();\n      this.data = data;\n    }\n  }\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we made a simple GET request from an API and then convert the data from JSON to a JavaScript object with the <code>json()</code> method.</p>"}, {"location": "vuejs/#adding-headers", "title": "Adding headers", "text": "<p>Like most HTTP clients, we can send request headers and bodies with the Fetch API.</p> <p>To send a request with HTTP headers, we can write:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img :src=\"data.src.tiny\"&gt;\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\nexport default {\n  name: \"App\",\n  data() {\n    return {\n      data: {\n        src: {}\n      }\n    };\n  },\n  beforeMount() {\n    this.getPhoto();\n  },\n  methods: {\n    async getPhoto() {\n      const headers = new Headers();\n      headers.append(\n        \"Authorization\",\n        \"api_key\"\n      );\n      const request = new Request(\n        \"https://api.pexels.com/v1/curated?per_page=11&amp;page=1\",\n        {\n          method: \"GET\",\n          headers,\n          mode: \"cors\",\n          cache: \"default\"\n        }\n      );      const res = await fetch(request);\n      const { photos } = await res.json();\n      this.data = photos[0];\n    }\n  }\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we used the <code>Headers</code> constructor, which is used to add requests headers to Fetch API requests.</p> <p>The append method appends our 'Authorization' header to the request.</p> <p>We\u2019ve to set the mode to 'cors' for a cross-domain request and headers is set to the headers object returned by the <code>Headers</code> constructor.</p>"}, {"location": "vuejs/#adding-body-to-a-request", "title": "Adding body to a request", "text": "<p>To make a request body, we can write the following:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;form @submit.prevent=\"createPost\"&gt;\n      &lt;input placeholder=\"name\" v-model=\"post.name\"&gt;\n      &lt;input placeholder=\"title\" v-model=\"post.title\"&gt;\n      &lt;br&gt;\n      &lt;button type=\"submit\"&gt;Create&lt;/button&gt;\n    &lt;/form&gt;\n    {{data}}\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\nexport default {\n  name: \"App\",\n  data() {\n    return {\n      post: {},\n      data: {}\n    };\n  },\n  methods: {\n    async createPost() {\n      const request = new Request(\n        \"https://jsonplaceholder.typicode.com/posts\",\n        {\n          method: \"POST\",\n          mode: \"cors\",\n          cache: \"default\",\n          body: JSON.stringify(this.post)\n        }\n      );      const res = await fetch(request);\n      const data = await res.json();\n      this.data = data;\n    }\n  }\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we made the request by stringifying the this.post object and then sending it with a POST request.</p>"}, {"location": "vuejs/#axios", "title": "Axios", "text": "<p>Axios is a popular HTTP client that works on both browser and Node.js apps.</p> <p>We can install it by running:</p> <pre><code>npm i axios\n</code></pre> <p>Then we can use it to make requests a simple GET request as follows:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;{{data}}&lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\n  name: \"App\",\n  data() {\n    return {\n      data: {}\n    };\n  },\n  beforeMount(){\n    this.getName();\n  },\n  methods: {\n    async getName(){\n      const { data } = await axios.get(\"https://api.agify.io/?name=michael\");\n      this.data = data;\n    }\n  }\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we call the <code>axios.get</code> method with the URL to make the request.</p> <p>Then we assign the response data to an object.</p>"}, {"location": "vuejs/#adding-headers_1", "title": "Adding headers", "text": "<p>If we want to make a request with headers, we can write:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img :src=\"data.src.tiny\"&gt;\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\n  name: \"App\",\n  data() {\n    return {\n      data: {}\n    };\n  },\n  beforeMount() {\n    this.getPhoto();\n  },\n  methods: {\n    async getPhoto() {\n      const {\n        data: { photos }\n      } = await axios({\n        url: \"https://api.pexels.com/v1/curated?per_page=11&amp;page=1\",\n        headers: {\n          Authorization: \"api_key\"\n        }\n      });\n      this.data = photos[0];\n    }\n  }\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we made a GET request with our Pexels API key with the axios method, which can be used for making any kind of request.</p> <p>If no request verb is specified, then it\u2019ll be a GET request.</p> <p>As we can see, the code is a bit shorter since we don\u2019t have to create an object with the <code>Headers</code> constructor.</p> <p>If we want to set the same header in multiple requests, we can use a request interceptor to set the header or other config for all requests.</p> <p>For instance, we can rewrite the above example as follows:</p> <pre><code>// main.js:\n\nimport Vue from \"vue\";\nimport App from \"./App.vue\";\nimport axios from 'axios'\n\naxios.interceptors.request.use(\n  config =&gt; {\n    return {\n      ...config,\n      headers: {\n        Authorization: \"api_key\"\n      }\n    };\n  },\n  error =&gt; Promise.reject(error)\n);\n\nVue.config.productionTip = false;\n\nnew Vue({\n  render: h =&gt; h(App)\n}).$mount(\"#app\");\n</code></pre> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img :src=\"data.src.tiny\"&gt;\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\n  name: \"App\",\n  data() {\n    return {\n      data: {}\n    };\n  },\n  beforeMount() {\n    this.getPhoto();\n  },\n  methods: {\n    async getPhoto() {\n      const {\n        data: { photos }\n      } = await axios({\n        url: \"https://api.pexels.com/v1/curated?per_page=11&amp;page=1\"\n      });\n      this.data = photos[0];\n    }\n  }\n};\n&lt;/script&gt;\n\nWe moved the header to `main.js` inside the code for our interceptor.\n\nThe first argument that\u2019s passed into `axios.interceptors.request.use` is\na function for modifying the request config for all requests.\n\nAnd the 2nd argument is an error handler for handling error of all requests.\n\nLikewise, we can configure interceptors for responses as well.\n\n#### Adding body to a request\n\nTo make a POST request with a request body, we can use the `axios.post` method.\n\n```html\n&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;form @submit.prevent=\"createPost\"&gt;\n      &lt;input placeholder=\"name\" v-model=\"post.name\"&gt;\n      &lt;input placeholder=\"title\" v-model=\"post.title\"&gt;\n      &lt;br&gt;\n      &lt;button type=\"submit\"&gt;Create&lt;/button&gt;\n    &lt;/form&gt;\n    {{data}}\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\n  name: \"App\",\n  data() {\n    return {\n      post: {},\n      data: {}\n    };\n  },\n  methods: {\n    async createPost() {\n      const { data } = await axios.post(\n        \"https://jsonplaceholder.typicode.com/posts\",\n        this.post\n      );\n      this.data = data;\n    }\n  }\n};\n&lt;/script&gt;\n</code></pre> <p>We make the POST request with the <code>axios.post</code> method with the request body in the second argument. Axios also sets the Content-Type header to application/json. This enables web frameworks to automatically parse the data.</p> <p>Then we get back the response data by getting the data property from the resulting response.</p>"}, {"location": "vuejs/#shorthand-methods-for-axios-http-requests", "title": "Shorthand methods for Axios HTTP requests", "text": "<p>Axios also provides a set of shorthand methods for performing different types of requests. The methods are as follows:</p> <ul> <li><code>axios.request(config)</code></li> <li><code>axios.get(url[, config])</code></li> <li><code>axios.delete(url[, config])</code></li> <li><code>axios.head(url[, config])</code></li> <li><code>axios.options(url[, config])</code></li> <li><code>axios.post(url[, data[, config]])</code></li> <li><code>axios.put(url[, data[, config]])</code></li> <li><code>axios.patch(url[, data[, config]])</code></li> </ul> <p>For instance, the following code shows how the previous example could be written using the <code>axios.post()</code> method:</p> <pre><code>axios.post('/login', {\n  firstName: 'Finn',\n  lastName: 'Williams'\n})\n.then((response) =&gt; {\n  console.log(response);\n}, (error) =&gt; {\n  console.log(error);\n});\n</code></pre> <p>Once an HTTP POST request is made, Axios returns a promise that is either fulfilled or rejected, depending on the response from the backend service.</p> <p>To handle the result, you can use the <code>then()</code>. method. If the promise is fulfilled, the first argument of <code>then()</code> will be called; if the promise is rejected, the second argument will be called. According to the documentation, the fulfillment value is an object containing the following information:</p> <pre><code>{\n  // `data` is the response that was provided by the server\n  data: {},\n\n  // `status` is the HTTP status code from the server response\n  status: 200,\n\n  // `statusText` is the HTTP status message from the server response\n  statusText: 'OK',\n\n  // `headers` the headers that the server responded with\n  // All header names are lower cased\n  headers: {},\n\n  // `config` is the config that was provided to `axios` for the request\n  config: {},\n\n  // `request` is the request that generated this response\n  // It is the last ClientRequest instance in node.js (in redirects)\n  // and an XMLHttpRequest instance the browser\n  request: {}\n}\n</code></pre>"}, {"location": "vuejs/#using-interceptors", "title": "Using interceptors", "text": "<p>One of the key features of Axios is its ability to intercept HTTP requests. HTTP interceptors come in handy when you need to examine or change HTTP requests from your application to the server or vice versa (e.g., logging, authentication, or retrying a failed HTTP request).</p> <p>With interceptors, you won\u2019t have to write separate code for each HTTP request. HTTP interceptors are helpful when you want to set a global strategy for how you handle request and response.</p> <pre><code>axios.interceptors.request.use(config =&gt; {\n  // log a message before any HTTP request is sent\n  console.log('Request was sent');\n\n  return config;\n});\n\n// sent a GET request\naxios.get('https://api.github.com/users/sideshowbarker')\n  .then(response =&gt; {\n    console.log(response.data);\n  });\n</code></pre> <p>In this code, the <code>axios.interceptors.request.use()</code> method is used to define code to be run before an HTTP request is sent. Also, <code>axios.interceptors.response.use()</code> can be used to intercept the response from the server. Let\u2019s say there is a network error; using the response interceptors, you can retry that same request using interceptors.</p>"}, {"location": "vuejs/#handling-errors", "title": "Handling errors", "text": "<p>To catch errors when doing requests you could use:</p> <pre><code>try {\n    let res = await axios.get('/my-api-route');\n\n    // Work with the response...\n} catch (error) {\n    if (error.response) {\n        // The client was given an error response (5xx, 4xx)\n        console.log(err.response.data);\n        console.log(err.response.status);\n        console.log(err.response.headers);\n    } else if (error.request) {\n        // The client never received a response, and the request was never left\n        console.log(err.request);\n    } else {\n        // Anything else\n        console.log('Error', err.message);\n    }\n}\n</code></pre> <p>The differences in the <code>error</code> object, indicate where the request encountered the issue.</p> <ul> <li> <p><code>error.response</code>: If your <code>error</code> object has a <code>response</code> property, it means     that your server returned a 4xx/5xx error. This will assist you choose what     sort of message to return to users.</p> </li> <li> <p><code>error.request</code>: This error is caused by a network error, a hanging backend     that does not respond instantly to each request, unauthorized or     cross-domain requests, and lastly if the backend API returns an error.</p> <p>This occurs when the browser was able to initiate a request but did not receive a valid answer for any reason.</p> </li> <li> <p>Other errors: It's possible that the <code>error</code> object does not have either     a <code>response</code> or <code>request</code> object attached to it. In this case it is implied that     there was an issue in setting up the request, which eventually triggered an     error.</p> <p>For example, this could be the case if you omit the URL parameter from the <code>.get()</code> call, and thus no request was ever made.</p> </li> </ul>"}, {"location": "vuejs/#sending-multiple-requests", "title": "Sending multiple requests", "text": "<p>One of Axios\u2019 more interesting features is its ability to make multiple requests in parallel by passing an array of arguments to the <code>axios.all()</code> method. This method returns a single promise object that resolves only when all arguments passed as an array have resolved.</p> <p>Here\u2019s a simple example of how to use <code>axios.all</code> to make simultaneous HTTP requests:</p> <pre><code>// execute simultaneous requests\naxios.all([\n  axios.get('https://api.github.com/users/mapbox'),\n  axios.get('https://api.github.com/users/phantomjs')\n])\n.then(responseArr =&gt; {\n  //this will be executed only when all requests are complete\n  console.log('Date created: ', responseArr[0].data.created_at);\n  console.log('Date created: ', responseArr[1].data.created_at);\n});\n\n// logs:\n// =&gt; Date created:  2011-02-04T19:02:13Z\n// =&gt; Date created:  2017-04-03T17:25:46Z\n</code></pre> <p>This code makes two requests to the GitHub API and then logs the value of the <code>created_at</code> property of each response to the console. Keep in mind that if any of the arguments rejects then the promise will immediately reject with the reason of the first promise that rejects.</p> <p>For convenience, Axios also provides a method called <code>axios.spread()</code> to assign the properties of the response array to separate variables. Here\u2019s how you could use this method:</p> <pre><code>axios.all([\n  axios.get('https://api.github.com/users/mapbox'),\n  axios.get('https://api.github.com/users/phantomjs')\n])\n.then(axios.spread((user1, user2) =&gt; {\n  console.log('Date created: ', user1.data.created_at);\n  console.log('Date created: ', user2.data.created_at);\n}));\n\n// logs:\n// =&gt; Date created:  2011-02-04T19:02:13Z\n// =&gt; Date created:  2017-04-03T17:25:46Z\n</code></pre> <p>The output of this code is the same as the previous example. The only difference is that the <code>axios.spread()</code> method is used to unpack values from the response array.</p>"}, {"location": "vuejs/#veredict", "title": "Veredict", "text": "<p>If you\u2019re working on multiple requests, you\u2019ll find that Fetch requires you to write more code than Axios, even when taking into consideration the setup needed for it. Therefore, for simple requests, Fetch API and Axios are quite the same. However, for more complex requests, Axios is better as it allows you to configure multiple requests in one place.</p> <p>If you're making a simple request use the Fetch API, for the other cases use axios because:</p> <ul> <li>It allows you to configure multiple requests in one place</li> <li>Code is shorter.</li> <li>It allows you to place all the API calls under services so that these can be     reused across components wherever they are     needed.</li> <li>It's easy to set a timeout of the request.</li> <li>It supports HTTP interceptors by befault</li> <li>It does automatic JSON data transformation.</li> <li>It's supported by old browsers, although you can bypass the problem with fetch     too.</li> <li>It has a progress indicator for large files.</li> <li>Supports simultaneous requests by default.</li> </ul> <p>Axios provides an easy-to-use API in a compact package for most of your HTTP communication needs. However, if you prefer to stick with native APIs, nothing stops you from implementing Axios features.</p> <p>For more information read:</p> <ul> <li>How To Make API calls in Vue.JS Applications by Bhargav Bachina</li> <li>Axios vs. fetch(): Which is best for making HTTP requests? by Faraz     Kelhini</li> </ul>"}, {"location": "vuejs/#vue-router", "title": "Vue Router", "text": "<p>Creating a Single-page Application with Vue + Vue Router feels natural, all we need to do is map our components to the routes and let Vue Router know where to render them. Here's a basic example:</p> <pre><code>&lt;script src=\"https://unpkg.com/vue@3\"&gt;&lt;/script&gt;\n&lt;script src=\"https://unpkg.com/vue-router@4\"&gt;&lt;/script&gt;\n\n&lt;div id=\"app\"&gt;\n  &lt;h1&gt;Hello App!&lt;/h1&gt;\n  &lt;p&gt;\n    &lt;!-- use the router-link component for navigation. --&gt;\n    &lt;!-- specify the link by passing the `to` prop. --&gt;\n    &lt;!-- `&lt;router-link&gt;` will render an `&lt;a&gt;` tag with the correct `href` attribute --&gt;\n    &lt;router-link to=\"/\"&gt;Go to Home&lt;/router-link&gt;\n    &lt;router-link to=\"/about\"&gt;Go to About&lt;/router-link&gt;\n  &lt;/p&gt;\n  &lt;!-- route outlet --&gt;\n  &lt;!-- component matched by the route will render here --&gt;\n  &lt;router-view&gt;&lt;/router-view&gt;\n&lt;/div&gt;\n</code></pre> <p>Note how instead of using regular <code>a</code> tags, we use a custom component <code>router-link</code> to create links. This allows Vue Router to change the URL without reloading the page, handle URL generation as well as its encoding.</p> <p><code>router-view</code> will display the component that corresponds to the url. You can put it anywhere to adapt it to your layout.</p> <pre><code>// 1. Define route components.\n// These can be imported from other files\nconst Home = { template: '&lt;div&gt;Home&lt;/div&gt;' }\nconst About = { template: '&lt;div&gt;About&lt;/div&gt;' }\n\n// 2. Define some routes\n// Each route should map to a component.\n// We'll talk about nested routes later.\nconst routes = [\n  { path: '/', component: Home },\n  { path: '/about', component: About },\n]\n\n// 3. Create the router instance and pass the `routes` option\n// You can pass in additional options here, but let's\n// keep it simple for now.\nconst router = VueRouter.createRouter({\n  // 4. Provide the history implementation to use. We are using the hash history for simplicity here.\n  history: VueRouter.createWebHashHistory(),\n  routes, // short for `routes: routes`\n})\n\n// 5. Create and mount the root instance.\nconst app = Vue.createApp({})\n// Make sure to _use_ the router instance to make the\n// whole app router-aware.\napp.use(router)\n\napp.mount('#app')\n\n// Now the app has started!\n</code></pre> <p>By calling <code>app.use(router)</code>, we get access to it as <code>this.$router</code> as well as the current route as <code>this.$route</code> inside of any component:</p> <pre><code>// Home.vue\nexport default {\n  computed: {\n    username() {\n      // We will see what `params` is shortly\n      return this.$route.params.username\n    },\n  },\n  methods: {\n    goToDashboard() {\n      if (isAuthenticated) {\n        this.$router.push('/dashboard')\n      } else {\n        this.$router.push('/login')\n      }\n    },\n  },\n}\n</code></pre> <p>To access the router or the route inside the <code>setup</code> function, call the <code>useRouter</code> or <code>useRoute</code> functions.</p>"}, {"location": "vuejs/#dynamic-route-matching-with-params", "title": "Dynamic route matching with params", "text": "<p>Very often we will need to map routes with the given pattern to the same component. For example we may have a User component which should be rendered for all users but with different user IDs. In Vue Router we can use a dynamic segment in the path to achieve that, we call that a <code>param</code>:</p> <pre><code>const User = {\n  template: '&lt;div&gt;User&lt;/div&gt;',\n}\n\n// these are passed to `createRouter`\nconst routes = [\n  // dynamic segments start with a colon\n  { path: '/users/:id', component: User },\n]\n</code></pre> <p>Now URLs like <code>/users/johnny</code> and <code>/users/jolyne</code> will both map to the same route.</p> <p>A <code>param</code> is denoted by a colon <code>:.</code> When a route is matched, the value of its params will be exposed as <code>this.$route.params</code> in every component. Therefore, we can render the current user ID by updating User's template to this:</p> <pre><code>const User = {\n  template: '&lt;div&gt;User {{ $route.params.id }}&lt;/div&gt;',\n}\n</code></pre> <p>You can have multiple <code>params</code> in the same route, and they will map to corresponding fields on <code>$route.params</code>. Examples:</p> pattern matched path $route.params /users/:username /users/eduardo { username: 'eduardo' } /users/:username/posts/:postId /users/eduardo/posts/123 { username: 'eduardo', postId: '123' } <p>In addition to <code>$route.params</code>, the <code>$route</code> object also exposes other useful information such as <code>$route.query</code> (if there is a query in the URL), <code>$route.hash</code>, etc.</p>"}, {"location": "vuejs/#reacting-to-params-changes", "title": "Reacting to params changes", "text": "<p>One thing to note when using routes with params is that when the user navigates from <code>/users/johnny</code> to <code>/users/jolyne</code>, the same component instance will be reused. Since both routes render the same component, this is more efficient than destroying the old instance and then creating a new one. However, this also means that the lifecycle hooks of the component will not be called.</p> <p>To react to <code>params</code> changes in the same component, you can simply <code>watch</code> anything on the <code>$route</code> object, in this scenario, the <code>$route.params</code>:</p> <pre><code>const User = {\n  template: '...',\n  created() {\n    this.$watch(\n      () =&gt; this.$route.params,\n      (toParams, previousParams) =&gt; {\n        // react to route changes...\n      }\n    )\n  },\n}\n</code></pre> <p>Or, use the <code>beforeRouteUpdate</code> navigation guard, which also allows to cancel the navigation:</p> <pre><code>const User = {\n  template: '...',\n  async beforeRouteUpdate(to, from) {\n    // react to route changes...\n    this.userData = await fetchUser(to.params.id)\n  },\n}\n</code></pre>"}, {"location": "vuejs/#components", "title": "Components", "text": "<p>Components allow us to split the UI into independent and reusable pieces, and think about each piece in isolation. It's common for an app to be organized into a tree of nested components</p>"}, {"location": "vuejs/#defining-a-component", "title": "Defining a component", "text": "<p>When using a build step, we typically define each Vue component in a dedicated file using the <code>.vue</code> extension.</p> <pre><code>&lt;script&gt;\nexport default {\n  data() {\n    return {\n      count: 0\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"count++\"&gt;You clicked me {{ count }} times.&lt;/button&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuejs/#using-a-component", "title": "Using a component", "text": "<p>To use a child component, we need to import it in the parent component. Assuming we placed our counter component inside a file called <code>ButtonCounter.vue</code>, the component will be exposed as the file's default export:</p> <pre><code>&lt;script&gt;\nimport ButtonCounter from './ButtonCounter.vue'\n\nexport default {\n  components: {\n    ButtonCounter\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;h1&gt;Here is a child component!&lt;/h1&gt;\n  &lt;ButtonCounter /&gt;\n&lt;/template&gt;\n</code></pre> <p>To expose the imported component to our template, we need to register it with the <code>components</code> option. The component will then be available as a tag using the key it is registered under.</p> <p>Components can be reused as many times as you want:</p> <pre><code>&lt;h1&gt;Here are many child components!&lt;/h1&gt;\n&lt;ButtonCounter /&gt;\n&lt;ButtonCounter /&gt;\n&lt;ButtonCounter /&gt;\n</code></pre> <p>When clicking on the buttons, each one maintains its own, separate count. That's because each time you use a component, a new instance of it is created.</p>"}, {"location": "vuejs/#passing-props", "title": "Passing props", "text": "<p>Props are custom attributes you can register on a component. Vue components require explicit <code>props</code> declaration so that Vue knows what external props passed to the component should be treated as fallthrough attributes.</p> <pre><code>&lt;!-- BlogPost.vue --&gt;\n&lt;script&gt;\nexport default {\n  props: ['title']\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;h4&gt;{{ title }}&lt;/h4&gt;\n&lt;/template&gt;\n</code></pre> <p>When a value is passed to a prop attribute, it becomes a property on that component instance. The value of that property is accessible within the template and on the component's <code>this</code> context, just like any other component property.</p> <p>A component can have as many props as you like and, by default, any value can be passed to any prop.</p> <p>Once a prop is registered, you can pass data to it as a custom attribute, like this:</p> <pre><code>&lt;BlogPost title=\"My journey with Vue\" /&gt;\n&lt;BlogPost title=\"Blogging with Vue\" /&gt;\n&lt;BlogPost title=\"Why Vue is so fun\" /&gt;\n</code></pre> <p>In a typical app, however, you'll likely have an array of posts in your parent component:</p> <pre><code>export default {\n  // ...\n  data() {\n    return {\n      posts: [\n        { id: 1, title: 'My journey with Vue' },\n        { id: 2, title: 'Blogging with Vue' },\n        { id: 3, title: 'Why Vue is so fun' }\n      ]\n    }\n  }\n}\n</code></pre> <p>Then want to render a component for each one, using <code>v-for</code>:</p> <pre><code>&lt;BlogPost\n  v-for=\"post in posts\"\n  :key=\"post.id\"\n  :title=\"post.title\"\n /&gt;\n</code></pre> <p>We declare long prop names using camelCase because this avoids having to use quotes when using them as property keys.</p> <pre><code>export default {\n  props: {\n    greetingMessage: String\n  }\n}\n</code></pre> <pre><code>&lt;span&gt;{{ greetingMessage }}&lt;/span&gt;\n</code></pre> <p>However, the convention is using kebab-case when passing props to a child component.</p> <pre><code>&lt;MyComponent greeting-message=\"hello\" /&gt;\n</code></pre>"}, {"location": "vuejs/#passing-different-value-types-on-props", "title": "Passing different value types on props", "text": "<ul> <li> <p>Numbers:</p> <pre><code>&lt;!-- Even though `42` is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.       --&gt;\n&lt;BlogPost :likes=\"42\" /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :likes=\"post.likes\" /&gt;\n</code></pre> </li> <li> <p>Boolean:     <pre><code>&lt;!-- Including the prop with no value will imply `true`. --&gt;\n&lt;BlogPost is-published /&gt;\n\n&lt;!-- Even though `false` is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.          --&gt;\n&lt;BlogPost :is-published=\"false\" /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :is-published=\"post.isPublished\" /&gt;\n</code></pre></p> </li> <li> <p>Array</p> <pre><code>&lt;!-- Even though the array is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.            --&gt;\n&lt;BlogPost :comment-ids=\"[234, 266, 273]\" /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :comment-ids=\"post.commentIds\" /&gt;\n</code></pre> </li> <li> <p>Object</p> <pre><code>&lt;!-- Even though the object is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.             --&gt;\n&lt;BlogPost\n  :author=\"{\n    name: 'Veronica',\n    company: 'Veridian Dynamics'\n  }\"\n /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :author=\"post.author\" /&gt;\n</code></pre> <p>If you want to pass all the properties of an object as props, you can use v-bind without an argument.</p> <pre><code>export default {\n  data() {\n    return {\n      post: {\n        id: 1,\n        title: 'My Journey with Vue'\n      }\n    }\n  }\n}\n</code></pre> <p>The following template:</p> <pre><code>&lt;BlogPost v-bind=\"post\" /&gt;\n</code></pre> <p>Will be equivalent to:</p> <pre><code>&lt;BlogPost :id=\"post.id\" :title=\"post.title\" /&gt;\n</code></pre> </li> </ul> <p>All props form a one-way-down binding between the child property and the parent one: when the parent property updates, it will flow down to the child, but not the other way around.</p> <p>Every time the parent component is updated, all props in the child component will be refreshed with the latest value. This means you should not attempt to mutate a prop inside a child component.</p>"}, {"location": "vuejs/#one-way-data-flow-in-props", "title": "One-way data flow in props", "text": ""}, {"location": "vuejs/#prop-validation", "title": "Prop validation", "text": "<p>Components can specify requirements for their props, if a requirement is not met, Vue will warn you in the browser's JavaScript console.</p> <pre><code>export default {\n  props: {\n    // Basic type check\n    //  (`null` and `undefined` values will allow any type)\n    propA: Number,\n    // Multiple possible types\n    propB: [String, Number],\n    // Required string\n    propC: {\n      type: String,\n      required: true\n    },\n    // Number with a default value\n    propD: {\n      type: Number,\n      default: 100\n    },\n    // Object with a default value\n    propE: {\n      type: Object,\n      // Object or array defaults must be returned from\n      // a factory function. The function receives the raw\n      // props received by the component as the argument.\n      default(rawProps) {\n        // default function receives the raw props object as argument\n        return { message: 'hello' }\n      }\n    },\n    // Custom validator function\n    propF: {\n      validator(value) {\n        // The value must match one of these strings\n        return ['success', 'warning', 'danger'].includes(value)\n      }\n    },\n    // Function with a default value\n    propG: {\n      type: Function,\n      // Unlike object or array default, this is not a factory function - this is a function to serve as a default value\n      default() {\n        return 'Default function'\n      }\n    }\n  }\n}\n</code></pre> <p>Additional details:</p> <ul> <li>All props are optional by default, unless <code>required: true</code> is specified.</li> <li>An absent optional prop will have <code>undefined</code> value.</li> <li>If a <code>default</code> value is specified, it will be used if the resolved prop value     is <code>undefined</code>, this includes both when the prop is absent, or an explicit     <code>undefined</code> value is passed.</li> </ul>"}, {"location": "vuejs/#listening-to-events", "title": "Listening to Events", "text": "<p>As we develop our <code>&lt;BlogPost&gt;</code> component, some features may require communicating back up to the parent. For example, we may decide to include an accessibility feature to enlarge the text of blog posts, while leaving the rest of the page at its default size.</p> <p>In the parent, we can support this feature by adding a <code>postFontSize</code> data property:</p> <pre><code>data() {\n  return {\n    posts: [\n      /* ... */\n    ],\n    postFontSize: 1\n  }\n}\n</code></pre> <p>Which can be used in the template to control the font size of all blog posts:</p> <pre><code>&lt;div :style=\"{ fontSize: postFontSize + 'em' }\"&gt;\n  &lt;BlogPost\n    v-for=\"post in posts\"\n    :key=\"post.id\"\n    :title=\"post.title\"\n   /&gt;\n&lt;/div&gt;\n</code></pre> <p>Now let's add a button to the <code>&lt;BlogPost&gt;</code> component's template:</p> <pre><code>&lt;!-- BlogPost.vue, omitting &lt;script&gt; --&gt;\n&lt;template&gt;\n  &lt;div class=\"blog-post\"&gt;\n    &lt;h4&gt;{{ title }}&lt;/h4&gt;\n    &lt;button&gt;Enlarge text&lt;/button&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n</code></pre> <p>The button currently doesn't do anything yet - we want clicking the button to communicate to the parent that it should enlarge the text of all posts. To solve this problem, component instances provide a custom events system. The parent can choose to listen to any event on the child component instance with <code>v-on</code> or <code>@,</code> just as we would with a native DOM event:</p> <pre><code>&lt;BlogPost\n  ...\n  @enlarge-text=\"postFontSize += 0.1\"\n /&gt;\n</code></pre> <p>Then the child component can emit an event on itself by calling the built-in <code>$emit</code> method, passing the name of the event:</p> <pre><code>&lt;!-- BlogPost.vue, omitting &lt;script&gt; --&gt;\n&lt;template&gt;\n  &lt;div class=\"blog-post\"&gt;\n    &lt;h4&gt;{{ title }}&lt;/h4&gt;\n    &lt;button @click=\"$emit('enlarge-text')\"&gt;Enlarge text&lt;/button&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n</code></pre> <p>The first argument to <code>this.$emit()</code> is the event name. Any additional arguments are passed on to the event listener.</p> <p>Thanks to the <code>@enlarge-text=\"postFontSize += 0.1\"</code> listener, the parent will receive the event and update the value of <code>postFontSize</code>.</p> <p>We can optionally declare emitted events using the emits option:</p> <pre><code>&lt;!-- BlogPost.vue --&gt;\n&lt;script&gt;\nexport default {\n  props: ['title'],\n  emits: ['enlarge-text']\n}\n&lt;/script&gt;\n</code></pre> <p>This documents all the events that a component emits and optionally validates them. It also allows Vue to avoid implicitly applying them as native listeners to the child component's root element.</p>"}, {"location": "vuejs/#event-arguments", "title": "Event arguments", "text": "<p>It's sometimes useful to emit a specific value with an event. For example, we may want the <code>&lt;BlogPost&gt;</code> component to be in charge of how much to enlarge the text by. In those cases, we can pass extra arguments to $emit to provide this value:</p> <pre><code>&lt;button @click=\"$emit('increaseBy', 1)\"&gt;\n  Increase by 1\n&lt;/button&gt;\n</code></pre> <p>Then, when we listen to the event in the parent, we can use an inline arrow function as the listener, which allows us to access the event argument:</p> <pre><code>&lt;MyButton @increase-by=\"(n) =&gt; count += n\" /&gt;\n</code></pre> <p>Or, if the event handler is a method:</p> <p></p> <p>Then the value will be passed as the first parameter of that method:</p> <pre><code>methods: {\n  increaseCount(n) {\n    this.count += n\n  }\n}\n</code></pre>"}, {"location": "vuejs/#declaring-emitted-events", "title": "Declaring emitted events", "text": "<p>Emitted events can be explicitly declared on the component via the <code>emits</code> option.</p> <pre><code>export default {\n  emits: ['inFocus', 'submit']\n}\n</code></pre> <p>The <code>emits</code> option also supports an object syntax, which allows us to perform runtime validation of the payload of the emitted events:</p> <pre><code>export default {\n  emits: {\n    submit(payload) {\n      // return `true` or `false` to indicate\n      // validation pass / fail\n    }\n  }\n}\n</code></pre> <p>Although optional, it is recommended to define all emitted events in order to better document how a component should work.</p>"}, {"location": "vuejs/#content-distribution-with-slots", "title": "Content distribution with Slots", "text": "<p>In addition to passing data via props, the parent component can also pass down template fragments to the child via slots:</p> <pre><code>&lt;ChildComp&gt;\n  This is some slot content!\n&lt;/ChildComp&gt;\n</code></pre> <p>In the child component, it can render the slot content from the parent using the <code>&lt;slot&gt;</code> element as outlet:</p> <pre><code>&lt;!-- in child template --&gt;\n&lt;slot/&gt;\n</code></pre> <p>Content inside the <code>&lt;slot&gt;</code> outlet will be treated as \"fallback\" content: it will be displayed if the parent did not pass down any slot content:</p> <pre><code>&lt;slot&gt;Fallback content&lt;/slot&gt;\n</code></pre> <p>Slot content is not just limited to text. It can be any valid template content. For example, we can pass in multiple elements, or even other components:</p> <pre><code>&lt;FancyButton&gt;\n  &lt;span style=\"color:red\"&gt;Click me!&lt;/span&gt;\n  &lt;AwesomeIcon name=\"plus\" /&gt;\n&lt;/FancyButton&gt;\n</code></pre> <p>Slot content has access to the data scope of the parent component, because it is defined in the parent. However, slot content does not have access to the child component's data. As a rule, remember that everything in the parent template is compiled in parent scope; everything in the child template is compiled in the child scope. You can however use child content using scoped slots.</p>"}, {"location": "vuejs/#named-slots", "title": "Named Slots", "text": "<p>There are times when it's useful to have multiple slot outlets in a single component.</p> <p>For these cases, the <code>&lt;slot&gt;</code> element has a special attribute, <code>name</code>, which can be used to assign a unique ID to different slots so you can determine where content should be rendered:</p> <pre><code>&lt;div class=\"container\"&gt;\n  &lt;header&gt;\n    &lt;slot name=\"header\"&gt;&lt;/slot&gt;\n  &lt;/header&gt;\n  &lt;main&gt;\n    &lt;slot&gt;&lt;/slot&gt;\n  &lt;/main&gt;\n  &lt;footer&gt;\n    &lt;slot name=\"footer\"&gt;&lt;/slot&gt;\n  &lt;/footer&gt;\n&lt;/div&gt;\n</code></pre> <p>To pass a named slot, we need to use a <code>&lt;template&gt;</code> element with the <code>v-slot</code> directive, and then pass the name of the slot as an argument to <code>v-slot</code>:</p> <p><pre><code>&lt;BaseLayout&gt;\n  &lt;template #header&gt;\n    &lt;h1&gt;Here might be a page title&lt;/h1&gt;\n  &lt;/template&gt;\n\n  &lt;template #default&gt;\n    &lt;p&gt;A paragraph for the main content.&lt;/p&gt;\n    &lt;p&gt;And another one.&lt;/p&gt;\n  &lt;/template&gt;\n\n  &lt;template #footer&gt;\n    &lt;p&gt;Here's some contact info&lt;/p&gt;\n  &lt;/template&gt;\n&lt;/BaseLayout&gt;\n</code></pre> Where <code>#</code> is the shorthand of <code>v-slot</code>.</p>"}, {"location": "vuejs/#dynamic-components", "title": "Dynamic components", "text": "<p>Sometimes, it's useful to dynamically switch between components, like in a tabbed interface, for example in this page.</p> <p>The above is made possible by Vue's <code>&lt;component&gt;</code> element with the special <code>is</code> attribute:</p> <pre><code>&lt;!-- Component changes when currentTab changes --&gt;\n&lt;component :is=\"currentTab\"&gt;&lt;/component&gt;\n</code></pre> <p>In the example above, the value passed to <code>:is</code> can contain either:</p> <ul> <li>The name string of a registered component, OR.</li> <li>The actual imported component object.</li> </ul> <p>You can also use the is attribute to create regular HTML elements.</p> <p>When switching between multiple components with <code>&lt;component :is=\"...\"&gt;</code>, a component will be unmounted when it is switched away from. We can force the inactive components to stay \"alive\" with the built-in <code>&lt;KeepAlive&gt;</code> component.</p>"}, {"location": "vuejs/#async-components", "title": "Async components", "text": "<p>In large applications, we may need to divide the app into smaller chunks and only load a component from the server when it's needed. To make that possible, Vue has a <code>defineAsyncComponent</code> function:</p> <pre><code>import { defineAsyncComponent } from 'vue'\n\nconst AsyncComp = defineAsyncComponent(() =&gt;\n  import('./components/MyComponent.vue')\n)\n</code></pre> <p>Asynchronous operations inevitably involve loading and error states, <code>defineAsyncComponent()</code> supports handling these states via advanced options:</p> <pre><code>const AsyncComp = defineAsyncComponent({\n  // the loader function\n  loader: () =&gt; import('./Foo.vue'),\n\n  // A component to use while the async component is loading\n  loadingComponent: LoadingComponent,\n  // Delay before showing the loading component. Default: 200ms.\n  delay: 200,\n\n  // A component to use if the load fails\n  errorComponent: ErrorComponent,\n  // The error component will be displayed if a timeout is\n  // provided and exceeded. Default: Infinity.\n  timeout: 3000\n})\n</code></pre>"}, {"location": "vuejs/#testing", "title": "Testing", "text": "<p>When designing your Vue application's testing strategy, you should leverage the following testing types:</p> <ul> <li>Unit: Checks that inputs to a given function, class, or composable are     producing the expected output or side effects.</li> <li>Component: Checks that your component mounts, renders, can be interacted     with, and behaves as expected. These tests import more code than unit tests,     are more complex, and require more time to execute.</li> <li>End-to-end: Checks features that span multiple pages and make real network     requests against your production-built Vue application. These tests often     involve standing up a database or other backend.</li> </ul>"}, {"location": "vuejs/#unit-testing", "title": "Unit testing", "text": "<p>Unit tests will catch issues with a function's business logic and logical correctness.</p> <p>Take for example this increment function:</p> <pre><code>// helpers.js\nexport function increment (current, max = 10) {\n  if (current &lt; max) {\n    return current + 1\n  }\n  return current\n}\n</code></pre> <p>Because it's very self-contained, it'll be easy to invoke the <code>increment</code> function and assert that it returns what it's supposed to, so we'll write a Unit Test.</p> <p>If any of these assertions fail, it's clear that the issue is contained within the <code>increment</code> function.</p> <pre><code>// helpers.spec.js\nimport { increment } from './helpers'\n\ndescribe('increment', () =&gt; {\n  test('increments the current number by 1', () =&gt; {\n    expect(increment(0, 10)).toBe(1)\n  })\n\n  test('does not increment the current number over the max', () =&gt; {\n    expect(increment(10, 10)).toBe(10)\n  })\n\n  test('has a default max of 10', () =&gt; {\n    expect(increment(10)).toBe(10)\n  })\n})\n</code></pre> <p>Unit testing is typically applied to self-contained business logic, components, classes, modules, or functions that do not involve UI rendering, network requests, or other environmental concerns.</p> <p>These are typically plain JavaScript / TypeScript modules unrelated to Vue. In general, writing unit tests for business logic in Vue applications does not differ significantly from applications using other frameworks.</p> <p>There are two instances where you DO unit test Vue-specific features:</p> <ul> <li>Composables</li> <li>Components</li> </ul>"}, {"location": "vuejs/#component-testing", "title": "Component testing", "text": "<p>In Vue applications, components are the main building blocks of the UI. Components are therefore the natural unit of isolation when it comes to validating your application's behavior. From a granularity perspective, component testing sits somewhere above unit testing and can be considered a form of integration testing. Much of your Vue Application should be covered by a component test and we recommend that each Vue component has its own spec file.</p> <p>Component tests should catch issues relating to your component's props, events, slots that it provides, styles, classes, lifecycle hooks, and more.</p> <p>Component tests should not mock child components, but instead test the interactions between your component and its children by interacting with the components as a user would. For example, a component test should click on an element like a user would instead of programmatically interacting with the component.</p> <p>Component tests should focus on the component's public interfaces rather than internal implementation details. For most components, the public interface is limited to: events emitted, props, and slots. When testing, remember to test what a component does, not how it does it. For example:</p> <ul> <li>For Visual logic assert correct render output based on inputted props and     slots.</li> <li>For Behavioral logic: assert correct render updates or emitted events in     response to user input events.</li> </ul> <p>The recommendation is to use Vitest for components or composables that render headlessly, and Cypress Component Testing for components whose expected behavior depends on properly rendering styles or triggering native DOM event.</p> <p>The main differences between Vitest and browser-based runners are speed and execution context. In short, browser-based runners, like Cypress, can catch issues that node-based runners, like Vitest, cannot (e.g. style issues, real native DOM events, cookies, local storage, and network failures), but browser-based runners are orders of magnitude slower than Vitest because they do open a browser, compile your stylesheets, and more.</p> <p>Component testing often involves mounting the component being tested in isolation, triggering simulated user input events, and asserting on the rendered DOM output. There are dedicated utility libraries that make these tasks simpler.</p> <ul> <li> <p><code>@testing-library/vue</code> is a Vue testing library focused on testing components     without relying on implementation details. Built with accessibility in mind,     its approach also makes refactoring a breeze. Its guiding principle is that     the more tests resemble the way software is used, the more confidence they     can provide.</p> </li> <li> <p><code>@vue/test-utils</code> is the official low-level component testing library that was     written to provide users access to Vue specific APIs. It's also the     lower-level library <code>@testing-library/vue</code> is built on top of.</p> </li> </ul> <p>I recommend using cypress so that you can use the same language either you are doing E2E tests or unit tests.</p> <p>If you're using Vuetify don't try to do component testing, I've tried for days and was unable to make it work.</p>"}, {"location": "vuejs/#e2e-testing", "title": "E2E Testing", "text": "<p>While unit tests provide developers with some degree of confidence, unit and component tests are limited in their abilities to provide holistic coverage of an application when deployed to production. As a result, end-to-end (E2E) tests provide coverage on what is arguably the most important aspect of an application: what happens when users actually use your applications.</p> <p>End-to-end tests focus on multi-page application behavior that makes network requests against your production-built Vue application. They often involve standing up a database or other backend and may even be run against a live staging environment.</p> <p>End-to-end tests will often catch issues with your router, state management library, top-level components (e.g. an App or Layout), public assets, or any request handling. As stated above, they catch critical issues that may be impossible to catch with unit tests or component tests.</p> <p>End-to-end tests do not import any of your Vue application's code, but instead rely completely on testing your application by navigating through entire pages in a real browser.</p> <p>End-to-end tests validate many of the layers in your application. They can either target your locally built application, or even a live Staging environment. Testing against your Staging environment not only includes your frontend code and static server, but all associated backend services and infrastructure.</p>"}, {"location": "vuejs/#e2e-tests-decisions", "title": "E2E tests decisions", "text": "<p>When doing E2E tests keep in mind:</p> <ul> <li> <p>Cross-browser testing: One of the primary benefits that end-to-end (E2E)     testing is known for is its ability to test your application across multiple     browsers. While it may seem desirable to have 100% cross-browser coverage,     it is important to note that cross browser testing has diminishing returns     on a team's resources due the additional time and machine power required to     run them consistently. As a result, it is important to be mindful of this     trade-off when choosing the amount of cross-browser testing your application     needs.</p> </li> <li> <p>Faster feedback loops: One of the primary problems with end-to-end (E2E) tests     and development is that running the entire suite takes a long time.     Typically, this is only done in continuous integration and deployment     (CI/CD) pipelines. Modern E2E testing frameworks have helped to solve this     by adding features like parallelization, which allows for CI/CD pipelines to     often run magnitudes faster than before. In addition, when developing     locally, the ability to selectively run a single test for the page you are     working on while also providing hot reloading of tests can help to boost     a developer's workflow and productivity.</p> </li> <li> <p>Visibility in headless mode: When end-to-end (E2E) tests are run in continuous     integration / deployment pipelines, they are often run in headless browsers     (i.e., no visible browser is opened for the user to watch). A critical     feature of modern E2E testing frameworks is the ability to see snapshots     and/or videos of the application during testing, providing some insight into     why errors are happening. Historically, it was tedious to maintain these     integrations.</p> </li> </ul> <p>Vue developers suggestion is to use Cypress as it provides the most complete E2E solution with features like an informative graphical interface, excellent debuggability, built-in assertions and stubs, flake-resistance, parallelization, and snapshots. It also provides support for Component Testing. However, it only supports Chromium-based browsers and Firefox.</p>"}, {"location": "vuejs/#installation", "title": "Installation", "text": "<p>In a Vite-based Vue project, run:</p> <pre><code>npm install -D vitest happy-dom @testing-library/vue@next\n</code></pre> <p>Next, update the Vite configuration to add the test option block:</p> <pre><code>// vite.config.js\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n  // ...\n  test: {\n    // enable jest-like global test APIs\n    globals: true,\n    // simulate DOM with happy-dom\n    // (requires installing happy-dom as a peer dependency)\n    environment: 'happy-dom'\n  }\n})\n</code></pre> <p>Then create a file ending in <code>*.test.js</code> in your project. You can place all test files in a test directory in project root, or in test directories next to your source files. Vitest will automatically search for them using the naming convention.</p> <pre><code>// MyComponent.test.js\nimport { render } from '@testing-library/vue'\nimport MyComponent from './MyComponent.vue'\n\ntest('it should work', () =&gt; {\n  const { getByText } = render(MyComponent, {\n    props: {\n      /* ... */\n    }\n  })\n\n  // assert output\n  getByText('...')\n})\n</code></pre> <p>Finally, update <code>package.json</code> to add the test script and run it:</p> <pre><code>{\n  // ...\n  \"scripts\": {\n    \"test\": \"vitest\"\n  }\n}\n</code></pre> <pre><code>npm test\n</code></pre>"}, {"location": "vuejs/#deploying", "title": "Deploying", "text": "<p>It is common these days to run front-end and back-end services inside Docker containers. The front-end service usually talks using a API with the back-end service.</p> <pre><code>FROM node as ui-builder\nRUN mkdir /usr/src/app\nWORKDIR /usr/src/app\nENV PATH /usr/src/app/node_modules/.bin:$PATH\nCOPY package.json /usr/src/app/package.json\nRUN npm install\nRUN npm install -g @vue/cli\nCOPY . /usr/src/app\nRUN npm run build\n\nFROM nginx\nCOPY  --from=ui-builder /usr/src/app/dist /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre> <p>The above makes use of the multi-stage build feature of Docker. The first half of the Dockerfile build the artifacts and second half use those artifacts and create a new image from them.</p> <p>To build the production image, run:</p> <pre><code>docker build -t myapp .\n</code></pre> <p>You can run the container by executing the following command:</p> <pre><code>docker run -it -p 80:80 --rm myapp-prod\n</code></pre> <p>The application will now be accessible at <code>http://localhost</code>.</p>"}, {"location": "vuejs/#configuration-through-environmental-variables", "title": "Configuration through environmental variables", "text": "<p>In production you want to be able to scale up or down the frontend and the backend independently, to be able to do that you usually have one or many docker for each role. Usually there is an SSL Proxy that acts as gate keeper and is the only component exposed to the public.</p> <p>If the user requests for <code>/api</code> it will forward the requests to the backend, if it asks for any other url it will forward it to the frontend.</p> <p>Note</p> <pre><code>\"You probably don't need to configure the backend api url as an environment\nvariable see\n[here](frontend_development.md#your-frontend-probably-doesn't-talk-to-your-backend)\nwhy.\"\n</code></pre> <p>For the frontend, we need to configure the application. This is usually done through environmental variables, such as <code>EXTERNAL_BACKEND_URL</code>. The problem is that these environment variables are set at build time, and can't be changed at runtime by default, so you can't offer a generic fronted Docker and particularize for the different cases. I've literally cried for hours trying to find a solution for this until Jos\u00e9 Silva came to my rescue. The tweak is to use a docker entrypoint to inject the values we want. To do so you need to:</p> <ul> <li> <p>Edit the site main <code>index.html</code> (if you use Vite is in <code>/index.html</code> otherwise     it might be at <code>public/index.html</code> to add a placeholder that will be     replaced by the dynamic configurations.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;script&gt;\n      // CONFIGURATIONS_PLACEHOLDER\n    &lt;/script&gt;\n    ...\n</code></pre> </li> <li> <p>Create an executable file named <code>entrypoint.sh</code> in the root of the project.</p> <pre><code>#!/bin/sh\n\nJSON_STRING='window.configs = { \\\n  \"VITE_APP_VARIABLE_1\":\"'\"${VITE_APP_VARIABLE_1}\"'\", \\\n  \"VITE_APP_VARIABLE_2\":\"'\"${VITE_APP_VARIABLE_2}\"'\" \\\n}'\n\nsed -i \"s@// CONFIGURATIONS_PLACEHOLDER@${JSON_STRING}@\" /usr/share/nginx/html/index.html\n\nexec \"$@\"\n</code></pre> <p>Its function is to replace the placeholder in the index.html by the configurations, injecting them in the browser window.</p> </li> <li> <p>Create a file named <code>src/utils/env.js</code> with the following utility function:</p> <pre><code>export default function getEnv(name) {\n  return window?.configs?.[name] || process.env[name]\n}\n</code></pre> <p>Which allows us to easily get the value of the configuration. If it exists in <code>window.configs</code> (used in remote environments like staging or production) it will have priority over the <code>process.env</code> (used for development).</p> </li> <li> <p>Replace the content of the <code>App.vue</code> file with the following:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img alt=\"Vue logo\" src=\"./assets/logo.png\"&gt;\n    &lt;div&gt;{{ variable1 }}&lt;/div&gt;\n    &lt;div&gt;{{ variable2 }}&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport getEnv from '@/utils/env'export default {\n  name: 'App',\n  data() {\n    return {\n      variable1: getEnv('VITE_APP_VARIABLE_1'),\n      variable2: getEnv('VITE_APP_VARIABLE_2')\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre> <p>At this point, if you create the <code>.env.local</code> file, in the root of the project, with the values for the printed variables:</p> <pre><code>VITE_APP_VARIABLE_1='I am the develoment variable 1'\nVITE_APP_VARIABLE_2='I am the develoment variable 2'\n</code></pre> <p>And run the development server <code>npm run dev</code> you should see those values printed in the application (http://localhost:8080).</p> </li> <li> <p>Update the <code>Dockerfile</code> to load the <code>entrypoint.sh</code>.</p> <pre><code>FROM node as ui-builder\nRUN mkdir /usr/src/app\nWORKDIR /usr/src/app\nENV PATH /usr/src/app/node_modules/.bin:$PATH\nCOPY package.json /usr/src/app/package.json\nRUN npm install\nRUN npm install -g @vue/cli\nCOPY . /usr/src/app\nARG VUE_APP_API_URL\nENV VUE_APP_API_URL $VUE_APP_API_URL\nRUN npm run build\n\nFROM nginx\nCOPY  --from=ui-builder /usr/src/app/dist /usr/share/nginx/html\nCOPY entrypoint.sh /usr/share/nginx/\nENTRYPOINT [\"/usr/share/nginx/entrypoint.sh\"]\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre> </li> <li> <p>Build the docker</p> <pre><code>docker build -t my-app .\n</code></pre> </li> </ul> <p>Now if you have a <code>.env.production.local</code> file with the next contents:</p> <pre><code>VITE_APP_VARIABLE_1='I am the production variable 1'\nVITE_APP_VARIABLE_2='I am the production variable 2'\n</code></pre> <p>And run <code>docker run -it -p 80:80 --env-file=.env.production.local --rm my-app</code>, you'll see the values of the production variables. You can also pass the variables directly with <code>-e VITE_APP_VARIABLE_1=\"Overriden variable\"</code>.</p>"}, {"location": "vuejs/#deploy-static-site-on-github-pages", "title": "Deploy static site on github pages", "text": "<p>Sites in Github pages have the url structure of <code>https://github_user.github.io/repo_name/</code> we need to tell vite that the base url is <code>/repo_name/</code>, otherwise the application will try to load the assets in <code>https://github_user.github.io/assets/</code> instead of <code>https://github_user.github.io/rpeo_name/assets/</code>.</p> <p>To change it, add in the <code>vite.config.js</code> file:</p> <pre><code>export default defineConfig({\n  base: '/repo_name/'\n})\n</code></pre> <p>Now you need to configure the deployment workflow, to do so, create a new file: <code>.github/workflows/deploy.yml</code> and paste the following code:</p> <pre><code>---\nname: Deploy\n\non:\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\njobs:\n  build:\n    name: Build\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v2\n\n      - name: Setup Node\n        uses: actions/setup-node@v1\n        with:\n          node-version: 16\n\n      - name: Install dependencies\n        uses: bahmutov/npm-install@v1\n\n      - name: Build project\n        run: npm run build\n\n      - name: Upload production-ready build files\n        uses: actions/upload-artifact@v2\n        with:\n          name: production-files\n          path: ./dist\n\n  deploy:\n    name: Deploy\n    needs: build\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    steps:\n      - name: Download artifact\n        uses: actions/download-artifact@v2\n        with:\n          name: production-files\n          path: ./dist\n\n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./dist\n</code></pre> <p>You'd probably need to change your repository settings under Actions/General and set the Workflow permissions to Read and write permissions.</p> <p>Once the workflow has been successful, in the repository settings under Pages you need to enable Github Pages to use the <code>gh-pages</code> branch as source.</p>"}, {"location": "vuejs/#tip-handling-vue-router-with-a-custom-404-page", "title": "Tip Handling Vue Router with a Custom 404 Page", "text": "<p>One thing to keep in mind when setting up the Github Pages site, is that working with Vue Router gets a little tricky.</p> <p>If you\u2019re using history mode in Vue router, you\u2019ll notice that if you try to go directly to a page other than <code>/</code> you\u2019ll get a 404 error. This is because Github Pages does not automatically redirect all requests to serve <code>index.html</code>.</p> <p>Luckily, there is an easy little workaround. All you have to do is duplicate your <code>index.html</code> file and name the copy <code>404.html</code>.</p> <p>What this does is make your 404 page serve the same content as your <code>index.html</code>, which means your Vue router will be able to display the right page.</p>"}, {"location": "vuejs/#testing_1", "title": "Testing", "text": ""}, {"location": "vuejs/#debug-jest-tests", "title": "Debug Jest tests", "text": "<p>If you're not developing in Visual code, running a debugger is not easy in the middle of the tests, so to debug one you can use <code>console.log()</code> statements and when you run them with <code>yarn test:unit</code> you'll see the traces.</p>"}, {"location": "vuejs/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "vuejs/#failed-to-resolve-component-x", "title": "Failed to resolve component: X", "text": "<p>If you've already imported the component with <code>import X from './X.vue</code> you may have forgotten to add the component to the <code>components</code> property of the module:</p> <pre><code>export default {\n  name: 'Inbox',\n  components: {\n    X\n  }\n}\n</code></pre>"}, {"location": "vuejs/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Homepage</li> <li>Tutorial</li> <li> <p>Examples</p> </li> <li> <p>Awesome Vue     Components</p> </li> </ul>"}, {"location": "vuejs/#axios_1", "title": "Axios", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Homepage</li> </ul>"}, {"location": "vuetify/", "title": "Vuetify", "text": "<p>Vuetify is a Vue UI Library with beautifully handcrafted Material Components.</p>"}, {"location": "vuetify/#install", "title": "Install", "text": "<p>First you need <code>vue-cli</code>, install it with:</p> <pre><code>sudo npm install -g @vue/cli\n</code></pre> <p>Then run:</p> <pre><code>vue add vuetify\n</code></pre> <p>If you're using Vite select <code>Vite Preview (Vuetify 3 + Vite)</code>.</p>"}, {"location": "vuetify/#usage", "title": "Usage", "text": ""}, {"location": "vuetify/#flex", "title": "Flex", "text": "<p>Control the layout of flex containers with alignment, justification and more with responsive flexbox utilities.</p> <p>Note</p> <pre><code>\"I suggest you use this page only as a reference, if it's the first time\nyou see this content, it's better to see it at the\n[source](https://vuetifyjs.com/en/styles/flex) as you can see Flex in\naction at the same time you read, which makes it much more easy to\nunderstand.\"\n</code></pre> <p>Using <code>display</code> utilities you can turn any element into a flexbox container transforming direct children elements into flex items. Using additional flex property utilities, you can customize their interaction even further.</p> <p>You can also customize flex utilities to apply based upon various breakpoints.</p> <ul> <li><code>.d-flex</code></li> <li><code>.d-inline-flex</code></li> <li><code>.d-sm-flex</code></li> <li><code>.d-sm-inline-flex</code></li> <li><code>.d-md-flex</code></li> <li><code>.d-md-inline-flex</code></li> <li><code>.d-lg-flex</code></li> <li><code>.d-lg-inline-flex</code></li> <li><code>.d-xl-flex</code></li> <li><code>.d-xl-inline-flex</code></li> </ul> <p>You define the attributes inside the <code>class</code> of the Vuetify object. For example:</p> <pre><code>&lt;v-card class=\"d-flex flex-row mb-6\" /&gt;\n</code></pre>"}, {"location": "vuetify/#display-breakpoints", "title": "Display breakpoints", "text": "<p>With Vuetify you can control various aspects of your application based upon the window size.</p> Device Code Type Range Extra small xs Small to large phone <code>&lt; 600px</code> Small sm Small to medium tablet <code>600px &gt; &lt; 960px</code> Medium md Large tablet to laptop <code>960px &gt; &lt; 1264px*</code> Large lg Desktop <code>1264px &gt; &lt; 1904px*</code> Extra large xl 4k and ultra-wide <code>&gt; 1904px*</code> <p>The breakpoint service is a programmatic way of accessing viewport information within components. It exposes a number of properties on the <code>$vuetify</code> object that can be used to control aspects of your application based upon the viewport size. The <code>name</code> property correlates to the currently active breakpoint; e.g. xs, sm, md, lg, xl.</p> <p>In the following snippet, we use a switch statement and the current breakpoint name to modify the <code>height</code> property of the <code>v-card</code> component:</p> <pre><code>&lt;template&gt;\n  &lt;v-card :height=\"height\"&gt;\n    ...\n  &lt;/v-card&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\n  export default {\n    computed: {\n      height () {\n        switch (this.$vuetify.breakpoint.name) {\n          case 'xs': return 220\n          case 'sm': return 400\n          case 'md': return 500\n          case 'lg': return 600\n          case 'xl': return 800\n        }\n      },\n    },\n  }\n&lt;/script&gt;\n</code></pre> <p>The following is the public signature for the breakpoint service:</p> <pre><code>{\n  // Breakpoints\n  xs: boolean\n  sm: boolean\n  md: boolean\n  lg: boolean\n  xl: boolean\n\n  // Conditionals\n  xsOnly: boolean\n  smOnly: boolean\n  smAndDown: boolean\n  smAndUp: boolean\n  mdOnly: boolean\n  mdAndDown: boolean\n  mdAndUp: boolean\n  lgOnly: boolean\n  lgAndDown: boolean\n  lgAndUp: boolean\n  xlOnly: boolean\n\n  // true if screen width &lt; mobileBreakpoint\n  mobile: boolean\n  mobileBreakpoint: number\n\n  // Current breakpoint name (e.g. 'md')\n  name: string\n\n  // Dimensions\n  height: number\n  width: number\n\n  // Thresholds\n  // Configurable through options\n  {\n    xs: number\n    sm: number\n    md: number\n    lg: number\n  }\n\n  // Scrollbar\n  scrollBarWidth: number\n}\n</code></pre> <p>Access these properties within Vue files by referencing <code>$vuetify.breakpoint.&lt;property&gt;</code> For example to log the current viewport width to the console once the component fires the mounted lifecycle hook you can use:</p> <pre><code>&lt;!-- Vue Component --&gt;\n\n&lt;script&gt;\n  export default {\n    mounted () {\n      console.log(this.$vuetify.breakpoint.width)\n    }\n  }\n&lt;/script&gt;\n</code></pre>"}, {"location": "vuetify/#flex-direction", "title": "Flex direction", "text": "<p>By default, <code>d-flex</code> applies <code>flex-direction: row</code> and can generally be omitted.</p> <p>The <code>flex-column</code> and <code>flex-column-reverse</code> utility classes can be used to change the orientation of the flexbox container.</p> <p>There are also responsive variations for flex-direction.</p> <ul> <li><code>.flex-row</code></li> <li><code>.flex-row-reverse</code></li> <li><code>.flex-column</code></li> <li><code>.flex-column-reverse</code></li> <li><code>.flex-sm-row</code></li> <li><code>.flex-sm-row-reverse</code></li> <li><code>.flex-sm-column</code></li> <li><code>.flex-sm-column-reverse</code></li> <li><code>.flex-md-row</code></li> <li><code>.flex-md-row-reverse</code></li> <li><code>.flex-md-column</code></li> <li><code>.flex-md-column-reverse</code></li> <li><code>.flex-lg-row</code></li> <li><code>.flex-lg-row-reverse</code></li> <li><code>.flex-lg-column</code></li> <li><code>.flex-lg-column-reverse</code></li> <li><code>.flex-xl-row</code></li> <li><code>.flex-xl-row-reverse</code></li> <li><code>.flex-xl-column</code></li> <li><code>.flex-xl-column-reverse</code></li> </ul>"}, {"location": "vuetify/#flex-justify", "title": "Flex justify", "text": "<p>The <code>justify-content</code> flex setting can be changed using the flex justify classes. This by default will modify the flexbox items on the x-axis but is reversed when using <code>flex-direction: column</code>, modifying the y-axis. Choose from:</p> <ul> <li><code>start</code> (browser default): Everything together on the left.</li> <li><code>end</code>: Everything together on the right.</li> <li><code>center</code>: Everything together on the center.</li> <li><code>space-between</code>: First item on the top left, second on the center, third at     the end, with space between the items.</li> <li><code>space-around</code>: Like <code>space-between</code> but with space on the top left and right     too.</li> </ul> <p>For example:</p> <pre><code>&lt;v-card class=\"d-flex justify-center mb-6\" /&gt;\n</code></pre> <p>There are also responsive variations for <code>justify-content</code>.</p> <ul> <li><code>.justify-start</code></li> <li><code>.justify-end</code></li> <li><code>.justify-center</code></li> <li><code>.justify-space-between</code></li> <li><code>.justify-space-around</code></li> <li><code>.justify-sm-start</code></li> <li><code>.justify-sm-end</code></li> <li><code>.justify-sm-center</code></li> <li><code>.justify-sm-space-between</code></li> <li><code>.justify-sm-space-around</code></li> <li><code>.justify-md-start</code></li> <li><code>.justify-md-end</code></li> <li><code>.justify-md-center</code></li> <li><code>.justify-md-space-between</code></li> <li><code>.justify-md-space-around</code></li> <li><code>.justify-lg-start</code></li> <li><code>.justify-lg-end</code></li> <li><code>.justify-lg-center</code></li> <li><code>.justify-lg-space-between</code></li> <li><code>.justify-lg-space-around</code></li> <li><code>.justify-xl-start</code></li> <li><code>.justify-xl-end</code></li> <li><code>.justify-xl-center</code></li> <li><code>.justify-xl-space-between</code></li> <li><code>.justify-xl-space-around</code></li> </ul>"}, {"location": "vuetify/#flex-align", "title": "Flex align", "text": "<p>The <code>align-items</code> flex setting can be changed using the flex align classes. This by default will modify the flexbox items on the y-axis but is reversed when using <code>flex-direction: column</code>, modifying the x-axis. Choose from:</p> <ul> <li><code>start</code>: Everything together on the top.</li> <li><code>end</code>: Everything together on the bottom.</li> <li><code>center</code>: Everything together on the center.</li> <li><code>baseline</code>: (I don't understand this one).</li> <li><code>align-stretch</code>: Align content to the top but extend the container to the     bottom. For example:</li> </ul> <pre><code>&lt;v-card class=\"d-flex align-center mb-6\" /&gt;\n</code></pre> <p>There are also responsive variations for <code>align-items</code>.</p> <ul> <li><code>.align-start</code></li> <li><code>.align-end</code></li> <li><code>.align-center</code></li> <li><code>.align-baseline</code></li> <li><code>.align-stretch</code></li> <li><code>.align-sm-start</code></li> <li><code>.align-sm-end</code></li> <li><code>.align-sm-center</code></li> <li><code>.align-sm-baseline</code></li> <li><code>.align-sm-stretch</code></li> <li><code>.align-md-start</code></li> <li><code>.align-md-end</code></li> <li><code>.align-md-center</code></li> <li><code>.align-md-baseline</code></li> <li><code>.align-md-stretch</code></li> <li><code>.align-lg-start</code></li> <li><code>.align-lg-end</code></li> <li><code>.align-lg-center</code></li> <li><code>.align-lg-baseline</code></li> <li><code>.align-lg-stretch</code></li> <li><code>.align-xl-start</code></li> <li><code>.align-xl-end</code></li> <li><code>.align-xl-center</code></li> <li><code>.align-xl-baseline</code></li> <li><code>.align-xl-stretch</code></li> </ul> <p>The <code>align-self</code> attribute works like <code>align</code> but for a single element instead of all the children.</p>"}, {"location": "vuetify/#margins", "title": "Margins", "text": "<p>You can define the margins you want with:</p> <ul> <li><code>ma-2</code>: 2 points in all directions.</li> <li><code>mb-2</code>: 2 points of margin on bottom.</li> <li><code>mt-2</code>: 2 points of margin on top.</li> <li><code>mr-2</code>: 2 points of margin on right.</li> <li><code>ml-2</code>: 2 points of margin on left.</li> </ul> <p>If instead of a number you use <code>auto</code> it will fill it till the end of the container.</p> <p>To center things around, you can use <code>mx-auto</code> to center in the X axis and <code>my-auto</code> for the Y axis.</p> <p>If you are using a <code>flex-column</code> and you want to put an element to the bottom, you'll use <code>mt-auto</code> so that the space filled on top of the element is filled automatically.</p>"}, {"location": "vuetify/#flex-grow-and-shrink", "title": "Flex grow and shrink", "text": "<p>Vuetify has helper classes for applying grow and shrink manually. These can be applied by adding the helper class in the format <code>flex-{condition}-{value}</code>, where condition can be either <code>grow</code> or <code>shrink</code> and value can be either <code>0</code> or <code>1</code>. The condition <code>grow</code> will permit an element to grow to fill available space, whereas <code>shrink</code> will permit an element to shrink down to only the space needs for its contents. However, this will only happen if the element must shrink to fit their container such as a container resize or being effected by a <code>flex-grow-1</code>. The value <code>0</code> will prevent the condition from occurring whereas <code>1</code> will permit the condition. The following classes are available:</p> <ul> <li><code>flex-grow-0</code></li> <li><code>flex-grow-1</code></li> <li><code>flex-shrink-0</code></li> <li><code>flex-shrink-1</code></li> </ul> <p>For example:</p> <pre><code>&lt;template&gt;\n  &lt;v-container&gt;\n    &lt;v-row\n      no-gutters\n      style=\"flex-wrap: nowrap;\"\n    &gt;\n      &lt;v-col\n        cols=\"2\"\n        class=\"flex-grow-0 flex-shrink-0\"\n      &gt;\n        &lt;v-card&gt;\n          I'm 2 column wide\n        &lt;/v-card&gt;\n      &lt;/v-col&gt;\n      &lt;v-col\n        cols=\"1\"\n        style=\"min-width: 100px; max-width: 100%;\"\n        class=\"flex-grow-1 flex-shrink-0\"\n      &gt;\n        &lt;v-card&gt;\n          I'm 1 column wide and I grow to take all the space\n        &lt;/v-card&gt;\n      &lt;/v-col&gt;\n      &lt;v-col\n        cols=\"5\"\n        style=\"min-width: 100px;\"\n        class=\"flex-grow-0 flex-shrink-1\"\n      &gt;\n        &lt;v-card&gt;\n          I'm 5 column wide and I shrink if there's not enough space\n        &lt;/v-card&gt;\n      &lt;/v-col&gt;\n    &lt;/v-row&gt;\n  &lt;/v-container&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuetify/#position-elements-with-flex", "title": "Position elements with Flex", "text": "<p>If the properties above don't give you the control you need you can use rows and columns directly. Vuetify comes with a 12 point grid system built using Flexbox. The grid is used to create specific layouts within an application\u2019s content.</p> <p>Using <code>v-row</code> (as a flex-container) and <code>v-col</code> (as a flex-item).</p> <pre><code> &lt;v-container&gt;\n   &lt;v-row&gt;\n    &lt;v-col&gt;\n      &lt;v-card\n        class=\"pa-2\"\n        outlined\n        tile\n      &gt;\n        One of three columns\n      &lt;/v-card&gt;\n    &lt;/v-col&gt;\n    &lt;v-col&gt;\n      &lt;v-card\n        class=\"pa-2\"\n        outlined\n        tile\n      &gt;\n        One of three columns\n      &lt;/v-card&gt;\n    &lt;/v-col&gt;\n    &lt;v-col&gt;\n      &lt;v-card\n        class=\"pa-2\"\n        outlined\n        tile\n      &gt;\n        One of three columns\n      &lt;/v-card&gt;\n    &lt;/v-col&gt;\n  &lt;/v-row&gt;\n&lt;/v-container&gt;\n</code></pre> <p><code>v-row</code> has the next properties:</p> <ul> <li><code>align</code>: set the vertical alignment of flex items (one of     <code>start</code>, <code>center</code> and <code>end</code>). It also has one property for each device size     (<code>align-md</code>, <code>align-xl</code>, ...). The <code>align-content</code> variation is also     available.</li> <li><code>justify</code>: set the horizontal alignment of the flex items (one of <code>start</code>,     <code>center</code>, <code>end</code>, <code>space-around</code>, <code>space-between</code>). It also has one property for each device size     (<code>justify-md</code>, <code>justify-xl</code>, ...).</li> <li><code>no-gutters</code>: Removes the spaces between items.</li> <li><code>dense</code>: Reduces the spaces between items.</li> </ul> <p><code>v-col</code> has the next properties:</p> <ul> <li> <p><code>cols</code>: Sets the default number of columns the component extends. Available     options are <code>1 -&gt; 12</code> and <code>auto</code>. you can use <code>lg</code>, <code>md</code>, ... to define the     number of columns for the other sizes.</p> </li> <li> <p><code>offset</code>: Sets the default offset for the column. You can also use <code>offset-lg</code>     and the other sizes.</p> </li> </ul>"}, {"location": "vuetify/#keep-the-structure-even-if-some-components-are-hidden", "title": "Keep the structure even if some components are hidden", "text": "<p>If you want the components to remain in their position even if the items around disappear, you need to use <code>&lt;v-row&gt;</code> and <code>&lt;v-col&gt;</code>. For example:</p> <pre><code>&lt;v-row align=end justify=center class=\"mt-auto\"&gt;\n  &lt;v-col align=center&gt;\n    &lt;v-btn\n      v-show=isNotFirstElement\n      ...\n    &gt;Button&lt;/v-btn&gt;\n  &lt;/v-col&gt;\n  &lt;v-col align=center&gt;\n    &lt;v-rating\n      v-show=\"isNotLastElement\"\n      ...\n    &gt;&lt;/v-rating&gt;\n  &lt;/v-col&gt;\n  &lt;v-col align=center&gt;\n    &lt;v-btn\n      v-show=\"isNotLastVisitedElement &amp;&amp; isNotLastElement\"\n      ...\n    &gt;Button&lt;/v-btn&gt;\n  &lt;/v-col&gt;\n&lt;/v-row&gt;\n</code></pre> <p>If instead you had use the next snippet, whenever one of the elements got hidden, the rest would move around to fill up the remaining space.</p> <pre><code>&lt;v-row align=end justify=center class=\"mt-auto\"&gt;\n  &lt;v-btn\n    v-show=isNotFirstElement\n    ...\n  &gt;Button&lt;/v-btn&gt;\n  &lt;v-rating\n    v-show=\"isNotLastElement\"\n    ...\n  &gt;&lt;/v-rating&gt;\n  &lt;v-btn\n    v-show=\"isNotLastVisitedElement &amp;&amp; isNotLastElement\"\n    ...\n  &gt;Button&lt;/v-btn&gt;\n&lt;/v-row&gt;\n</code></pre>"}, {"location": "vuetify/#themes", "title": "Themes", "text": "<p>Vuetify comes with two themes pre-installed, light and dark. To set the default theme of your application, use the <code>defaultTheme</code> option.</p> <p>File: <code>src/plugins/vuetify.js</code></p> <pre><code>import { createApp } from 'vue'\nimport { createVuetify } from 'vuetify'\n\nexport default createVuetify({\n  theme: {\n    defaultTheme: 'dark'\n  }\n})\n</code></pre> <p>Adding new themes is as easy as defining a new property in the <code>theme.themes</code> object. A theme is a collection of colors and options that change the overall look and feel of your application. One of these options designates the theme as being either a light or dark variation. This makes it possible for Vuetify to implement Material Design concepts such as elevated surfaces having a lighter overlay color the higher up they are.</p> <p>File: <code>src/plugins/vuetify.js</code></p> <pre><code>import { createApp } from 'vue'\nimport { createVuetify, ThemeDefinition } from 'vuetify'\n\nexport default createVuetify({\n  theme: {\n    defaultTheme: 'myCustomLightTheme',\n    themes: {\n      myCustomLightTheme: {\n        dark: false,\n        colors: {\n          background: '#FFFFFF',\n          surface: '#FFFFFF',\n          primary: '#510560',\n          'primary-darken-1': '#3700B3',\n          secondary: '#03DAC6',\n          'secondary-darken-1': '#018786',\n          error: '#B00020',\n          info: '#2196F3',\n          success: '#4CAF50',\n          warning: '#FB8C00',\n        }\n      }\n    }\n  }\n})\n</code></pre> <p>To dynamically change theme during runtime.</p> <pre><code>&lt;template&gt;\n  &lt;v-app&gt;\n    &lt;v-btn @click=\"toggleTheme\"&gt;toggle theme&lt;/v-btn&gt;\n    ...\n  &lt;/v-app&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport { useTheme } from 'vuetify'\n\nexport default {\n  setup () {\n    const theme = useTheme()\n\n    return {\n      theme,\n      toggleTheme: () =&gt; theme.global.name.value = theme.global.current.value.dark ? 'light' : 'dark'\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre> <p>Most components support the <code>theme</code> prop. When used, a new context is created for that specific component and all of its children. In the following example, the <code>v-btn</code> uses the dark theme applied by its parent <code>v-card</code>.</p> <pre><code>&lt;template&gt;\n  &lt;v-app&gt;\n    &lt;v-card theme=\"dark\"&gt;\n      &lt;!-- button uses dark theme --&gt;\n      &lt;v-btn&gt;foo&lt;/v-btn&gt;\n    &lt;/v-card&gt;\n  &lt;/v-app&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuetify/#elements", "title": "Elements", "text": ""}, {"location": "vuetify/#cards", "title": "Cards", "text": "<p>The <code>v-card</code> can be used to place any kind of text on your site, in this case use the <code>variant=text</code>.</p>"}, {"location": "vuetify/#buttons", "title": "Buttons", "text": "<p>The <code>sizes</code> can be: <code>x-small</code>, <code>small</code>, <code>default</code>, <code>large</code>, <code>x-large</code>.</p>"}, {"location": "vuetify/#illustrations", "title": "Illustrations", "text": "<p>You can get nice illustrations for your web on Drawkit, for example I like to use the Classic kit.</p>"}, {"location": "vuetify/#icons", "title": "Icons", "text": "<p>The <code>v-icon</code> component provides a large set of glyphs to provide context to various aspects of your application.</p> <pre><code>&lt;v-icon&gt;fas fa-user&lt;/v-icon&gt;\n</code></pre> <p>If you have the FontAwesome icons installed, browse them here</p>"}, {"location": "vuetify/#install-font-awesome-icons", "title": "Install font awesome icons", "text": "<pre><code>npm install @fortawesome/fontawesome-free -D\n</code></pre> <pre><code>// src/plugins/vuetify.js\nimport '@fortawesome/fontawesome-free/css/all.css' // Ensure your project is capable of handling css files\nimport { createVuetify } from 'vuetify'\nimport { aliases, fa } from 'vuetify/lib/iconsets/fa'\n\nexport default createVuetify({\n  icons: {\n    defaultSet: 'fa',\n    aliases,\n    sets: {\n      fa,\n    },\n  },\n})\n</code></pre> <pre><code>&lt;template&gt;\n  &lt;v-icon icon=\"fas fa-home\" /&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuetify/#fonts", "title": "Fonts", "text": "<p>By default it uses the webfontload plugin which slows down a lot the page load, instead you can install the fonts directly. For example for the Roboto font:</p> <ul> <li> <p>Install the font</p> <pre><code>npm install --save typeface-roboto\n</code></pre> </li> <li> <p>Uninstall the webfontload plugin</p> <pre><code>npm remove webfontloader\n</code></pre> </li> <li> <p>Remove the loading of the webfontload in <code>/main.js</code> the lines:</p> <p><pre><code>import { loadFonts } from './plugins/webfontloader'\n\nloadFonts()\n</code></pre> * Add the font in the <code>App.vue</code> file:</p> <pre><code>&lt;style lang=\"sass\"&gt;\n  @import '../node_modules/typeface-roboto/index.css'\n&lt;/style&gt;\n</code></pre> </li> </ul>"}, {"location": "vuetify/#carousels", "title": "Carousels", "text": "<p>Vuetify has their own carousel component, here's it's API. In the Awesome Vue.js compilation there are other suggestions. As some users say, it looks like Vuetify's doesn't have the best responsive behaviour.</p> <p>The best looking alternatives I've seen are:</p> <ul> <li>vue-agile:     Demo.</li> <li>vue-picture-swipe</li> <li>vue-slick-carousel:     Demo. It     doesn't yet support Vue3</li> <li>swiper: Demo</li> <li>vue-splide: Demo</li> </ul>"}, {"location": "vuetify/#vuetify-component", "title": "Vuetify component", "text": "<p>I tried binding the model with <code>v-model</code> but when I click on the arrows, the image doesn't change and the binded property doesn't change. If I change the property with other component, the image does change</p>"}, {"location": "vuetify/#vue-agile", "title": "vue-agile", "text": "<p>If you encounter the <code>modules have no default</code> error, add this to your <code>vite.config.js</code>:</p> <pre><code>export default defineConfig({\n  ...\n  optimizeDeps: { include: [ 'lodash.throttle', 'lodash.orderby' ] },\n  ...\n})\n</code></pre>"}, {"location": "vuetify/#small-vertical-carousel", "title": "Small vertical carousel", "text": "<p>If you want to do a vertical carousel for example the one shown in the video playlists, you can't yet use <code>v-slide-group</code>. vue-agile doesn't either yet have vertical option.</p>"}, {"location": "vuetify/#audio", "title": "Audio", "text": "<ul> <li>vuejs-sound-player</li> <li>vue-audio-visual: Demo</li> <li>vue3-audio-player:     Demo</li> <li>vuetify-audio:     Demo</li> </ul>"}, {"location": "vuetify/#testing", "title": "Testing", "text": "<p>I tried doing component tests with Jest, Vitest and Cypress and found no way of making component tests, they all fail one way or the other.</p> <p>E2E tests worked with Cypress however, that's going to be my way of action till this is solved.</p>"}, {"location": "vuetify/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Home</li> <li>Git</li> <li>Discord</li> </ul>"}, {"location": "wake_on_lan/", "title": "Wake on LAN", "text": "<p>Wake on LAN (WoL) is a feature to switch on a computer via the network.</p>"}, {"location": "wake_on_lan/#usage", "title": "Usage", "text": ""}, {"location": "wake_on_lan/#host-configuration", "title": "Host configuration", "text": "<p>On the host you want to activate the wake on lan execute:</p> <pre><code>$: ethtool *interface* | grep Wake-on\n\nSupports Wake-on: pumbag\nWake-on: d\n</code></pre> <p>The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver:</p> <pre><code>$: ethtool -s interface wol g\n</code></pre> <p>If it was not enabled check in the Arch wiki how to make the change persistent.</p> <p>To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine:</p> <pre><code>$: ip link\n\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default\n   link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: enp1s0: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000\n    link/ether 48:05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>Here the MAC address is <code>48:05:ca:09:0e:6a</code>.</p> <p>In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link).</p> <p>If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so.</p>"}, {"location": "wake_on_lan/#client-trigger", "title": "Client trigger", "text": "<p>If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects.</p> <p>If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9.</p> <p>In the simplest case the default broadcast address 255.255.255.255 is used:</p> <pre><code>$ wakeonlan *target_MAC_address*\n</code></pre> <p>To broadcast the magic packet only to a specific subnet or host, use the <code>-i</code> switch:</p> <pre><code>$ wakeonlan -i *target_IP* *target_MAC_address*\n</code></pre>"}, {"location": "wake_on_lan/#references", "title": "References", "text": "<ul> <li>Arch wiki post</li> </ul>"}, {"location": "wallabag/", "title": "Wallabag", "text": "<p>Wallabag is a self-hosted read-it-later application: it saves a web page by keeping content only. Elements like navigation or ads are deleted.</p>"}, {"location": "wallabag/#installation", "title": "Installation", "text": "<p>They provide a working docker-compose</p> <pre><code>version: '3'\nservices:\n  wallabag:\n    image: wallabag/wallabag\n    environment:\n      - MYSQL_ROOT_PASSWORD=wallaroot\n      - SYMFONY__ENV__DATABASE_DRIVER=pdo_mysql\n      - SYMFONY__ENV__DATABASE_HOST=db\n      - SYMFONY__ENV__DATABASE_PORT=3306\n      - SYMFONY__ENV__DATABASE_NAME=wallabag\n      - SYMFONY__ENV__DATABASE_USER=wallabag\n      - SYMFONY__ENV__DATABASE_PASSWORD=wallapass\n      - SYMFONY__ENV__DATABASE_CHARSET=utf8mb4\n      - SYMFONY__ENV__SECRET=supersecretenv\n      - SYMFONY__ENV__MAILER_HOST=127.0.0.1\n      - SYMFONY__ENV__MAILER_USER=~\n      - SYMFONY__ENV__MAILER_PASSWORD=~\n      - SYMFONY__ENV__FROM_EMAIL=wallabag@example.com\n      - SYMFONY__ENV__DOMAIN_NAME=https://your-wallabag-url-instance.com\n      - SYMFONY__ENV__SERVER_NAME=\"Your wallabag instance\"\n    ports:\n      - \"80\"\n    volumes:\n      - /opt/wallabag/images:/var/www/wallabag/web/assets/images\n    healthcheck:\n      test: [\"CMD\", \"wget\" ,\"--no-verbose\", \"--tries=1\", \"--spider\", \"http://localhost\"]\n      interval: 1m\n      timeout: 3s\n    depends_on:\n      - db\n      - redis\n  db:\n    image: mariadb\n    environment:\n      - MYSQL_ROOT_PASSWORD=wallaroot\n    volumes:\n      - /opt/wallabag/data:/var/lib/mysql\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\"]\n      interval: 20s\n      timeout: 3s\n  redis:\n    image: redis:alpine\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 20s\n      timeout: 3s\n</code></pre> <p>If you don't want to enable the public registration use the <code>SYMFONY__ENV__FOSUSER_REGISTRATION=false</code> flag. The emailer configuration is only used when a user creates an account, so if you're only going to use it for yourself, it's safe to disable.</p> <p>Remember to change all passwords to a random value.</p> <p>If you create RSS feeds for a user, all articles are shared by default, if you only want to share the starred articles, add to your nginx config:</p> <pre><code>    location ~* /feed/.*/.*/(?!starred){\n        deny all;\n        return 404;\n    }\n</code></pre>"}, {"location": "wallabag/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "week_management/", "title": "Week Management", "text": "<p>I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend.</p> <p>Until I've got pydo ready to natively incorporate all this processes, I heavily use taskwarrior to manage my tasks and logs. To make the process faster and reproducible, I've written small python scripts using tasklib.</p>"}, {"location": "week_management/#week-review", "title": "Week review", "text": "<p>Life logging is the only purpose of my weekly review.</p> <p>I've made <code>diw</code> a small python script that for each overdue task allows me to:</p> <ul> <li>Review: Opens vim to write a diary entry related with the task. The text is   saved as an annotation of the task and another form is filled to record whom   I've shared it with. The last information is used to help me take care of   people around me.</li> <li>Skip: Don't interact with this task.</li> <li>Done: Complete the task.</li> <li>Delete: Remove the task.</li> <li>Reschedule: Opens a form to specify the new due date.</li> </ul>"}, {"location": "week_management/#week-planning", "title": "Week planning", "text": "<p>The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode.</p> <p>First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP.</p> <p>Taking different actions to each INBOX element type:</p> <ul> <li>Tasks or human arrangements: Do it if it can be completed in less than   3 minutes. Otherwise, create a taskwarrior task.</li> <li>Behavior: Add it to taskwarrior.</li> <li>Movie/Serie recommendation: Introduce it into my media monitorization system.</li> <li>Book recommendation: Introduce into my library management system.</li> <li>Miscellaneous thoughts: Refactor into the blue-book, project documentation or   Anki.</li> </ul> <p>Then I split my workspace in two terminals, in the first I run <code>task due.before:7d diary</code> where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I:</p> <ul> <li>Execute <code>gcal .</code> to show the calendar of the previous, current and next month.</li> <li>Check the weather for the whole week to decide which plans are suitable.</li> <li>Analyze the tasks that need to be done answering the following questions:</li> <li>Do I need to do this task this week? If not, reschedule or delete it.</li> <li>Does it need a due date? If not, remove the <code>due</code> attribute.     Having the minimum number of tasks with a fixed date reduces wasted     rescheduling time and allows better prioritizing.</li> <li>Can I do the task on the selected date? As most humans, I tend to     underestimate both the required time to complete a task and to switch     contexts. To avoid it, If the day is full it's better to reschedule.</li> <li>Check that every day has at least one task. Particularly tasks that will   help with life logging.</li> <li>If there aren't enough things to fill up all days, check the things that   I want to do list and try to do one.</li> </ul>"}, {"location": "wesnoth/", "title": "The Battle for Wesnoth", "text": "<p>The Battle for Wesnoth is an open source, turn-based strategy game with a high fantasy theme. It features both singleplayer and online/hotseat multiplayer combat.</p> <p>Explore the world of Wesnoth and take part in its different adventures! Embark on a desperate quest to reclaim your rightful throne\u2026 Flee the Lich Lords to a new home across the sea\u2026 Delve into the darkest depths of the earth to craft a jewel of fire itself\u2026 Defend your kingdom against the ravaging hordes of a foul necromancer\u2026 Or lead a straggly band of survivors across the blazing sands to confront an unseen evil.</p>"}, {"location": "wesnoth/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Wiki</li> <li>Game manual</li> <li>Mainline campaigns</li> </ul>"}, {"location": "wesnoth_loyalist/", "title": "Loyalist", "text": "<p>The Loyalist are a faction of Humans who are loyal to the throne of Wesnoth.</p> <p>The race of men is an extremely diverse one. Although they originally came from the Old Continent, men have spread all over the world and split into many different cultures and races. Although they are not imbued with magic like other creatures, humans can learn to wield it and able to learn more types than most others. They have no extra special abilities or aptitudes except their versatility and drive. While often at odds with all races, they can occasionally form alliances with the less aggressive races such as elves and dwarves. The less scrupulous among them do not shrink back from hiring orcish mercenaries, either. They have no natural enemies, although the majority of men, like most people of all races, have an instinctive dislike of the undead. Men are shorter than the elves, but taller still than dwarves. Their skin color can vary, from almost white to dark brown. Humans are a versatile race who specialize in many different areas.</p> <p>Similarly, the Loyalist faction is a very versatile melee oriented faction with important ranged support from bowmen and mages.</p>"}, {"location": "wesnoth_loyalist/#how-to-play-loyalists", "title": "How to play loyalists", "text": "<p>Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction.</p> <p>Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman, but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among:</p> <ul> <li> <p>Heavy Infantrymen: Few strikes, high damage per strike, slow, higher     hit-points, good resistances, low defenses, deals impact damage which is good     against undead.</p> </li> <li> <p>Spearmen: Average strikes, average damage per strike, average movement,     medium hit-points, normal resistances, average defenses, deals pierce damage     which is good against drakes, has a weak ranged attack.</p> </li> <li> <p>Fencer: High number of strikes, low damage per strike, quick, low hit-points,     bad resistances, good defenses, deals blade damage which is good against     elusive foots or wose, deals less total damage than the other two, is     a skirmisher.</p> </li> </ul> <p>The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold).</p> <p>Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen. Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack.</p> <p>The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman, which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman, who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman.</p> <p>When attacking, Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported.</p> <p>The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves.</p> <p>All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent.</p> <p>The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman, cavalryman, merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front.</p> <p>When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman, bowman, fencer, mage, merman fighter, cavalryman. As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game.</p>"}, {"location": "wesnoth_loyalist/#loyalists-vs-undead", "title": "Loyalists vs. Undead", "text": "<p>When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages. If you see lots of dark adepts, you'll want some cavalry, horsemen and maybe some spearmen.</p> <ul> <li> <p>Bowman: (D-) Almost entirely useless. You might use them to poke at     walking corpses, ghouls, vampire bats and ghosts, but mages are     much better at all of these things. I would only buy a bowman to counter     a large zombie horde.</p> </li> <li> <p>Cavalryman: (B+) You'll want some cavalry for scouting and dealing     initial damage to dark adepts so that a horseman can finish them. They     are also decent at fighting Skeletons in the daytime, because cavalry are     blade-resistant. However, the cheap skeleton archers will really ruin     cavalry quickly at night, so if there are any skeleton archers on the other     side you won't be able to use cavalry to hold territory. If your opponent     over-recruits dark adepts, however, you can use cold-resistant cavalry to hold     territory against them. They are also decent for holding territory against     ghouls because they can run away and heal (unlike your heavy infantrymen).</p> </li> <li> <p>Fencer: (C-) Fencers are a bad recruit in this match up because they are     vulnerable to the blade and pierce damage of skeletons and cannot damage     them much in return. They also are incapable of holding territory against     dark adepts, who cut right through the high defense of fencers. However,     you may want to have a fencer or two around for trapping dark adepts or     getting in that last hit. With luck, they may also be able to frustrate     non-magical attacks from skeletons and the like.</p> </li> <li> <p>Heavy Infantryman: (B+) You need heavy infantry to hold territory     against skeletons and skeleton archers, and they will be your unit of     choice for dealing melee damage, especially to the cheap skeleton archers.     Any heavy infantrymen in the daytime can kill a full health skeleton     archer in two hits, while a strong heavy infantrymen in the daytime can     kill a full health skeleton in two hits, dealing 18 damage per strike (even     a mage in daytime cannot kill a skeleton in one round, unless supported by     a lieutenant). A fearless heavy infantryman may be dangerous even at     night. If you don't have enough heavy infantryman to go around, you can     get your initial hits in on a skeleton archer with them and finish him     with a mage. Just keep heavy infantrymen away from dark adepts, and     only let ghouls hit heavy infantrymen if they are on a village (or has     a white mage or other healer next to him), since they can't easily run     away to heal. Also beware walking corpses, which deal a surprising amount     of damage at all times of day since heavy infantrymen can't dodge and     impact damage goes around their resistances, and the ranged cold attack of     ghosts. The biggest problem with heavy infantrymen is they are slow,     which means they're hard to retreat at night and hard to advance in day.     Without shielding units they'll get trapped and killed, and if you have to     shield a unit maybe it should be a mage instead.</p> </li> <li> <p>Horseman: (B) - Because they deal pierce damage, horsemen may not be     very useful when faced with skeletons. However, if your opponent     over-recruits dark adepts, horsemen can be extremely useful, as dark     adepts deal no return damage to the normally risky charge attack.     horsemen can even be used to finish skeleton archers, their nemesis, in     the daytime. However, if your opponent recruits enough skeleton archers     you will have a hard time shielding your horsemen from their devastating     pierce attacks, and skeleton archers are dirt cheap. horsemen can also     one-shot bats and zombies, which can be useful if you need to clear out     a lot of level 0 units quickly. I would want to have at least one horseman     around to keep my opponent from getting too bold with dark adepts, if not     more. Your opponent will be forced to recruit dark adepts if you have     heavy infantrymen in the field.</p> </li> <li> <p>Mage: (A+) mages are an absolute necessity against Undead. If you do     not have mages it will be almost impossible for you to kill ghosts, but     with mages it's a piece of cake. mages are the best unit for killing     almost everything Undead can throw at you, and can even be used to finish     dark adepts in the daytime. Your main problem is that dark adepts are     cheaper and deal almost as much damage, so your opponent can spam dark     adepts while you cannot afford to spam mages. You will also have the     difficult task of shielding fragile, expensive mages against lots of cheap     Undead units. Your opponent will use skeletons and ghouls to attack your     mages when he can, but bats, zombies or just about any other unit will     do for killing your mages in a pinch. Shield your mages well, surround     them with damage soakers and if you can deliver them safely to their targets     you'll be able to clear out the Undead quickly.</p> </li> <li> <p>Merman Fighter: (C-) Mermen make a decent navy against Undead, since     bats and ghosts will have a hard time killing them with their high     defense. Even dark adepts will find Mermen annoying because of their 20%     cold resistance. However, Mermen will have a hard time hurting anything the     Undead have with their lame pierce weapon. Generally Mermen are only good     for holding water hexes and scouting, but don't underestimate how useful     that can be. Some well-placed Mermen on a water map can prevent bats from     sneaking behind your lines and capturing villages or killing injured units.     Even on mostly land maps, a Merman in a river can help hold a defensive     line, or a quick Merman can use a river to slip behind the enemy to trap     dark adepts or other units that are trying to escape at daytime.</p> </li> <li> <p>Spearman: (C-) Spearmen are mostly useful as cheap units for holding     villages and occupying space when faced with dark adepts or skeleton     archers. (You'll want to avoid letting dark adepts hit your heavy     infantrymen because of their vulnerability to cold.) However, you don't     really want spearmen to take hits from dark adepts, it would be better     to let the cold-resistant cavalry absorb the damage. The only units     spearmen are good for attacking are dark adepts and walking corpses.     spearmen are completely useless against skeletons unless you level one     into a swordsman, and even then they're pretty mediocre. However, if there     are lots of skeleton archers you won't be able to use much cavalry or     horsemen, so a spearman or two may be necessary as defenders and damage     soakers even if they are lousy at dealing damage to Undead.</p> </li> </ul>"}, {"location": "wesnoth_loyalist/#loyalists-vs-rebels", "title": "Loyalists vs. Rebels", "text": "<p>Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day.</p> <p>The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses. Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers, so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry. If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages, and will reduce the effectiveness of archer spam. spearmen are useless against woses, recruit them sparingly and keep them away from unscouted forest.</p> <p>An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours.</p> <ul> <li> <p>Bowman: (C-) Rebels in general are effective in both melee and ranged     attacks, so recruiting a ranged unit is less beneficial than most factions     because most of the Rebels' units can retaliate against you. They can be     useful for taking out an Scout or two, but otherwise, this is not really     a smart buy.</p> </li> <li> <p>Cavalryman: (B+) A good scout and an effective counter to woses. Watch     out for archers, though, they can really tear horses to shreds. And as     always, they are effective against ranged oriented units like the mage.</p> <p>(A) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds.</p> </li> <li> <p>Fencer: (B+) The fencer shares the 70% defense in the forest like most     of the elves, but it has negative resistances. Theoretically, they should be     effective against woses, but more often than not woses will crush them     easily. In spite of this, Fencers still have skirmsher, which means that     they can sneak behind an injured unit and finish them off. Do not     over-recruit them, as enemy mages will tear though their high defense.</p> </li> <li> <p>Heavy Infantryman: (C-) Usefulness similar to an archer. It's too slow     to deal with most of the Rebel units, and it is owned by Mages, woses, and     even archers. Even though it has a high damage potential and good     resistances, because of its inherently low defense shamans can easily get     hits on it and turn it into a piece of metal for a turn.</p> </li> <li> <p>Horseman: (B) One horseman may be nice, but no more than that. Enemy     archers will tear them apart and shamans totally mess them up. They're     expensive too. Their greatest use is probably killing mages or an archer     that has gone slightly off-track.</p> <p>(A-) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged.</p> </li> <li> <p>Mage: (A-) You'll need these guys to tear though those high elvish     60-70% evasion in the forest; but be careful, archers and shamans will     retaliate and some pretty nasty things may come from that. One purely     positive thing though, they just absolutely destroy woses. 13-3 at day.     Ouch. mages are expensive and fragile though, so keep them protected.</p> </li> <li> <p>Merman Fighter: (B+) If the map has a lot of water, maybe recruit a few     to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise     they don't really contribute much else.</p> </li> <li> <p>Spearman - A - A necessity. To defend at night, to kill pretty much     anything except for woses, and to be cheap and cost only 14 gold. These     guys pretty much tear though most of the Rebel units, if it were not only     for the high-defense archer and shaman. Get a bunch, and move them like     a wall against the enemy units.</p> </li> </ul> <p>At the start of the game, recruit 1-2 cavalrymen, depending on the map size, 1 mage, (1 merman fighter if you need some water coverage), maybe 1 fencer, and the rest spearman. Later on you maybe can recruit some horseman if you opponent recruits mass mages, or more cavalrymen and mages if he masses woses. Otherwise, spearmen and mages should help you get through most of the match.</p> <p>If you're going for speed, recruit 2-3 cavalrymen, depending on the map, 1 horseman, maybe 1 fencer, and the rest spearmen. Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can.</p>"}, {"location": "wesnoth_loyalist/#loyalists-vs-northerners", "title": "Loyalists vs Northerners", "text": "<p>The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins, it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen/bowmen. Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts, trolls and wolfriders (even at night), use your magic to finish off wounded assassins, and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry. If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you.</p> <ul> <li> <p>Spearmen: (A-) Pretty much your best unit against northerners. Their     first strike ability is great in defense and their attack power to cost     ratio is quite high. They also have good health and can retaliate in ranged.     As in many matchups, the bulk of your army.</p> </li> <li> <p>Mage: (B+) The mage is fragile and expensive and has a weak melee     attack, which is the exact opposite of northerners. Not a great defender     unless your opponent goes mad with assassins. However, they have lots of     attack power at day and are very useful for nocking trolls of their     perches and finishing off those damn assassins. You'll want a few of     these, but make sure you keep them protected.</p> </li> <li> <p>Bowmen: (B) Good unit to have. They can attack grunts and trolls     (although you'd want mages to attack trolls) without retaliation, and     can defend against assassins in the night. They can also take out any     wolves that stray too far from the main orchish army. They're not as good as     mages, but they're cheaper and tougher.</p> </li> <li> <p>Cavalryman: (B) cavalryman are superior to their wolves and can hold     ground against grunts and trolls reasonably well. It's also a good idea     if your opponent likes to spam lots of assassins, to let them be the     target of their poison; cavalryman and can run away and heal. Your heavy     infantrymen? Not so much. Beware though, of the cheap orchish archer, as     cavalrymen are weak to pierce.</p> </li> <li> <p>Heavy Infantryman: (B-) Heavy infantryman are heavily resistant to     physical attacks like blade, pierce and to a lesser degree, impact. You'd     think this would make them great units against northerners, if the orchish     archer didn't have a ranged fire attack. heavy infantrymen are also much     too slow too effectively deal with poison, and that's a 19 gold unit that     can't fight. However, they are useful in defense if you opponent hasn't     spammed many archers, and they can even be useful in attack to crush up     injured grunts.</p> </li> <li> <p>Horseman: (B) The horseman is a little controversial in this matchup.     northerners are cheap and melee orientated, which is exactly what horseman     do badly against. They're also quite tough, which means one hit kills are     rare. However, they have one very important advantage over the northerners-     high damage per hex. A horseman is very useful for softening up units or     outright killing them (particularly when paired with a mage) which will be     important in breaking ZoC lines, which can be a real pain with all the units     northerners can field. Get one or two, but no more, else it quickly produces     diminishing returns.</p> </li> <li> <p>Fencer: (C+) The fencer is a melee based unit that is fragile against     blade attacks of grunts, which means they don't have an awful lot of     fighting effectiveness with them. On the other hand, the fencer's skirmisher     ability is really valuable with the many northener units, and can finish off     injured archers or go on sneaky village grabbing incursions. One or two     might be useful, but no more, this unit is not a serious fighter! Make sure     you keep them on 70% terrain if you can as well. Two hitter grunts can     have trouble taking them out.</p> </li> <li> <p>Merman Fighter: (C+) Water based melee unit that doesn't do well against     the orc nagas, recruit only if there's lots of water and keep them in     defendible terrain, else they'll quickly be killed.</p> </li> </ul> <p>As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day.</p> <p>Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages, they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.</p>"}, {"location": "wesnoth_northerners/", "title": "Northerners", "text": "<p>Northerners are a faction of Orcs and their allies who live in the north of the Great Continent, thus their name. Northerners consist of the warrior orcs race, the enslaved goblins, trolls who are tricked into combat by the orcs, and the serpentine naga. The Northerners play best by taking advantage of having many low-cost and high HP soldiers.</p>"}, {"location": "wesnoth_rebels/", "title": "Rebels", "text": "<p>Rebels are a faction of Elves and their various forest-dwelling allies. They get their human name, Rebels, from the time of Heir to the Throne, when they started the rebellion against the evil Queen Asheviere. Elves are a magical race that are masters of the bow and are capable of living many years longer than humans. In harmony with nature, the elves find allies with the human mages, certain merfolk, and tree creatures called woses. Rebels are best played taking advantage of their high forest defense, mastery of ranged attacks, and the elves' neutral alignment.</p>"}, {"location": "wireshark/", "title": "Wireshark", "text": "<p>Wireshark is the world\u2019s foremost and widely-used network protocol analyzer. It lets you see what\u2019s happening on your network at a microscopic level and is the de facto (and often de jure) standard across many commercial and non-profit enterprises, government agencies, and educational institutions.</p>"}, {"location": "wireshark/#installation", "title": "Installation", "text": "<pre><code>apt-get install wireshark\n</code></pre> <p>If the version delivered by your distribution is not high enough, use Jezz's Docker</p> <pre><code>docker run -d \\\n    -v /etc/localtime:/etc/localtime:ro \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -e DISPLAY=unix$DISPLAY \\\n    -v /tmp/wireshark:/data \\\n    jess/wireshark\n</code></pre>"}, {"location": "wireshark/#usage", "title": "Usage", "text": ""}, {"location": "wireshark/#filter", "title": "Filter", "text": "<p>You can filter by traffic type with <code>tcp and tcp.port == 80</code>, <code>http or ftp</code> or <code>not ftp</code>.</p> <p>It's also possible to nest many operators with <code>(http or ftp) and ip.addr == 192.168.1.14</code></p> <p>The most common filters are:</p> Item Description ip.addr IP address (check both source and destination) tcp.port TCP Layer 4 port (check both source and destination) udp.port UDP Layer 4 port (check both source and destination) ip.src IP source address ip.dst IP destination address tcp.srcport TCP source port tcp.dstport TCP destination port udp.srcport UDP source port udp.dstport UDP destination port icmp.type ICMP numeric type ip.tos.precedence IP precedence eth.addr MAC address ip.ttl IP Time to Live (TTL)"}, {"location": "wireshark/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "work_interruption_analysis/", "title": "Work Interruption Analysis", "text": "<p>This is the interruption analysis report applied to my everyday work.</p> <p>I've identified the next interruption sources:</p> <ul> <li>Physical interruptions.</li> <li>Emails.</li> <li>Calls.</li> <li>Instant message applications.</li> <li>Calendar events.</li> </ul>"}, {"location": "work_interruption_analysis/#physical-interruptions", "title": "Physical interruptions", "text": "<p>Physical interactions are when someone comes to your desk and expect you to attend them immediately. These interruptions can be categorized as:</p> <ul> <li>Asking for help.</li> <li>Social interactions.</li> </ul> <p>The obvious solution is to remote work as much as possible. It's less easy for people to interrupt through digital channels than physically.</p> <p>It goes the other way around too. Be respectful to your colleagues and try to use asynchronous communications as much as possible, so they can manage when they attend you.</p>"}, {"location": "work_interruption_analysis/#asking-for-help", "title": "Asking for help", "text": "<p>These interruptions are the most difficult to delay, as it's hard to tell a person to wait when it's already in front of you. If you don't take care of them you may end up in the situation where you can receive 5 o 6 interruptions per minute which can drive you crazy. By definition all these events require an immediate action. The priority and delay may depend on many factors, such as the person or moment.</p> <p>The first thing I'd do is make a mental prioritization of the people that interrupt you, to decide which ones do you accept and which ones you need to regulate. Once you have it, work on how to assertively tell them that they need to reduce their interruptions. You can agree with them a non interruption time where they can aggregate and prepare all the questions so you can work through them efficiently. Often they are able to answer most of them themselves. The length of the period needs to be picked wisely as you want to be interrupted the minimum number of times while you don't make them loose their time trying to solve something you could work out quickly.</p> <p>Other times it's easier to forward them to the team's interruption manager.</p>"}, {"location": "work_interruption_analysis/#social-interactions", "title": "Social interactions", "text": "<p>Depending how popular you are, you'll have more or less of these interactions. The way I've found to be able to be in control of them is by scheduling social events in my calendar and introducing them in my task management workflow. For example, we agree to go to have lunch all together at the same hour every day, or I arrange a coffee break with someone every Monday at a defined hour.</p>"}, {"location": "work_interruption_analysis/#emails", "title": "Emails", "text": "<p>Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as:</p> <ul> <li>General information: They don't usually require any direct action, so they can     wait more than 24 hours.</li> <li>Support to internal agents: At work, we have decided that email is not to be     used as the internal main communication channel, so I don't receive many and     their priority is low.</li> <li>Support to external agents: I'm lucky to not have many of these and they have     less priority than internal people so they can wait 4 or more hours.</li> <li>Infrastructure notifications: For example LetsEncrypt renewals or cloud provider     notification or support cases. The related actions can wait 4 hours or more.</li> <li>Calendar events: Someone creates a new meeting, changes an existing one or     confirms/declines its assistance. We have defined a policy that we don't     create or change events with less than 24 hours notice, and in the special     cases that we need to, they will be addressed in the chat rooms. So these     mails can be read once per day.</li> <li>Monitorization notifications: We've configured Prometheus's     alertmanager to send the notifications to the email as     a fallback channel, but it's to be checked only if the main channel is down.</li> <li>Source code manager notifications: The web where we host our source code sends     us emails when there are new pull requests or when there are comments on     existent ones. I automatically mark them as read and move them to a mail     directory as I manage these interruptions with other workflow.</li> <li>The CI sends notifications when some job fails. Unless it's a new     pipeline or I'm actively working on it, a failed job can wait four     hours broken before I interact with it.</li> <li>The issue tracker notifications: It sends them on new or changed issues.     At work, I filter them out as I delegate it's management to the Scrum     Master.</li> </ul> <p>In conclusion, I can check the work email only when I start working, on the lunch break and when I'm about to leave. So its safe to disable the notifications.</p> <p>I'm eager to start the email automation project so I can spend even less time and willpower managing the email.</p>"}, {"location": "work_interruption_analysis/#calls", "title": "Calls", "text": "<p>We've agreed that the calls are the communication channel used only for critical situations, similar to the physical interruptions, they are synchronous so they're more difficult to manage.</p> <p>As calls are very rare and of high priority, I have my phone configured to ring on incoming calls.</p> <p>Have a work phone independent of your personal</p> <p>Nowadays you can have phone contracts of 0$/month used only to receive calls.</p> <p>Remember to give it to the fewer people as possible.</p>"}, {"location": "work_interruption_analysis/#instant-messages", "title": "Instant messages", "text": "<p>It's the main internal communication channel, so it has a great volume of events with a wide range of priorities. They can be categorized as:</p> <ul> <li>Asking for help through direct messages: We don't have many as we've agreed to     use groups as much as     possible.     So they have high priority and I have the notifications enabled.</li> <li>Social interaction through direct messages: I don't have many as I try to     arrange one on one meetings     instead,     so they have a low priority. As notifications are defined for all direct     messages, I inherit the notifications from the last category.</li> <li>Team group or support rooms: We've defined the interruption role so I check them     whenever an chosen interruption event comes. If I'm assuming the role     I enable the notifications on the channel, if not I'll check them whenever     I check the application.</li> <li>Information rooms: They have no priority and can be checked each 4 hours.</li> </ul> <p>In conclusion, I can check the work chat applications each pomodoro cycle or when I receive a direct notification until the improve the notification management in Linux project is ready.</p>"}, {"location": "work_interruption_analysis/#calendar-events", "title": "Calendar events", "text": "<p>Often with a wide range of priorities.</p> <ul> <li>decide if you have to go</li> <li>Define an agenda with times</li> </ul>"}, {"location": "write_neovim_plugins/", "title": "Write neovim plugins", "text": "<ul> <li>plugin example</li> <li>plugin repo</li> </ul> <p>The plugin repo has some examples in the tests directory</p>"}, {"location": "write_neovim_plugins/#control-an-existing-nvim-instance", "title": "Control an existing nvim instance", "text": "<p>A number of different transports are supported, but the simplest way to get started is with the python REPL. First, start Nvim with a known address (or use the <code>$NVIM_LISTEN_ADDRESS</code> of a running instance):</p> <pre><code>$ NVIM_LISTEN_ADDRESS=/tmp/nvim nvim\n</code></pre> <p>In another terminal, connect a python REPL to Nvim (note that the API is similar to the one exposed by the python-vim bridge:</p> <pre><code>&gt;&gt;&gt; from neovim import attach\n# Create a python API session attached to unix domain socket created above:\n&gt;&gt;&gt; nvim = attach('socket', path='/tmp/nvim')\n# Now do some work.\n&gt;&gt;&gt; buffer = nvim.current.buffer # Get the current buffer\n&gt;&gt;&gt; buffer[0] = 'replace first line'\n&gt;&gt;&gt; buffer[:] = ['replace whole buffer']\n&gt;&gt;&gt; nvim.command('vsplit')\n&gt;&gt;&gt; nvim.windows[1].width = 10\n&gt;&gt;&gt; nvim.vars['global_var'] = [1, 2, 3]\n&gt;&gt;&gt; nvim.eval('g:global_var')\n[1, 2, 3]\n</code></pre>"}, {"location": "write_neovim_plugins/#load-buffer", "title": "Load buffer", "text": "<pre><code>buffer = nvim.current.buffer # Get the current buffer\nbuffer[0] = 'replace first line'\nbuffer[:] = ['replace whole buffer']\n</code></pre>"}, {"location": "write_neovim_plugins/#get-cursor-position", "title": "Get cursor position", "text": "<pre><code>nvim.current.window.cursor\n</code></pre>"}, {"location": "writing_style/", "title": "Writing style", "text": "<p>Writing style is the manner of expressing thought in language characteristic of an individual, period, school, or nation. It's defined by the grammatical choices writers make, the importance of adhering to norms in certain contexts and deviating from them in others, the expression of social identity, and the emotional effects of particular devices on audiences.</p> <p>Beyond the essential elements of spelling, grammar, and punctuation, writing style is the choice of words, sentence structure, and paragraph structure, used to convey the meaning effectively.</p> <p>The point of good writing style is to:</p> <ul> <li>Express the message to the reader simply, clearly, and convincingly.</li> <li>Keep the reader attentive, engaged, and interested.</li> </ul> <p>Not to</p> <ul> <li>Display the writer's personality.</li> <li>Demonstrate the writer's skills, knowledge, or abilities.</li> </ul>"}, {"location": "writing_style/#general-writing-principles", "title": "General writing principles", "text": ""}, {"location": "writing_style/#make-it-pleasant-to-the-reader", "title": "Make it pleasant to the reader", "text": "<p>Writing is a medium of communication, so avoid introducing elements that push away the reader, such as:</p> <ul> <li>Spelling mistakes.</li> <li>Gender favoring, polarizing, race related, religion inconsiderate, or other unequal     phrasing.</li> <li>Ugly environment: Present your texts through a pleasant medium such as     a mkdocs webpage.</li> <li>Write like you talk: Ask yourself, is this the way I'd say this if I were talking to     a friend?. If it isn't, imagine what you would     say, and use that instead.</li> <li>Format errors: If you're writing in markdown, make sure that the result has no display     bugs.</li> <li>Write short articles: Even though I love Gwern     site, I find it daunting most of times.     Instead of a big post, I'd rather use multiple well connected articles.</li> </ul>"}, {"location": "writing_style/#saying-more-with-less", "title": "Saying more with less", "text": "<p>Never use a long word where a short one will do. Replace words like <code>really like</code> with <code>love</code> or other more appropriate words that save space writing and are more meaningful.</p> <p>Don't use filler words like really.</p>"}, {"location": "writing_style/#be-aware-of-pacing", "title": "Be aware of pacing", "text": "<p>Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence.</p> <p>For example, change Due to the fact that to because.</p>"}, {"location": "writing_style/#one-purpose", "title": "One purpose", "text": "<p>A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing.</p> <p>This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it.</p> <p>A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted.</p>"}, {"location": "writing_style/#avoid-using-cliches", "title": "Avoid using clich\u00e9s", "text": "<p>Clich\u00e9s prevent readers from visualization, making them an obstacle to creating memorable writing.</p>"}, {"location": "writing_style/#citing-the-sources", "title": "Citing the sources", "text": "<ul> <li>If it's a small phrase or a refactor, link the source inside the phrase or at     the header of the section.</li> <li>If it's a big refactor, add it to a references section.</li> <li>If it's a big block without editing use admonition quotes</li> </ul>"}, {"location": "writing_style/#take-all-the-guidelines-as-suggestions", "title": "Take all the guidelines as suggestions", "text": "<p>All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them.</p>"}, {"location": "writing_style/#unconnected-thoughts", "title": "Unconnected thoughts", "text": "<ul> <li>Replace adjectives with data. Nearly all of -&gt; 84% of.</li> <li>Remove weasel words.</li> <li>Most adverbs are superfluous. When you say \"generally\" or   \"usually\" you're probably undermining your point and the use of \"very\" or   \"extremely\" are hyperbolic and breathless and make it easier to regard what   you're writing as not serious.</li> <li>Examine every word: a surprising number don't serve any purpose.</li> <li>While wrapping your content into a story you may find yourself talking about   your achievements more than giving actionable advice. If that happens, try to   get to the bottom of how you achieved these achievements and break this process   down, then focus on the process more than on your personal achievement.</li> <li>Set up a system that prompts people to review the material.</li> <li>Don't be egocentric, limit the use of <code>I</code>, use the implied subject instead:     It's where I go to -&gt; It's the place to go. I take different actions -&gt; Taking different actions.</li> <li>Don't be possessive, use <code>the</code> instead of <code>my</code>.</li> <li>If you don't know how to express something use services like     deepl.</li> <li>Use synonyms instead of repeating the same word over and over.</li> <li>Think who are you writing to.</li> <li>Use active voice: Active voice ensures that the actors are identified and it   generally leaves less open questions. The exception is if you want to   emphasize the object of the sentence.</li> </ul>"}, {"location": "writing_style/#how-to-end-a-letter", "title": "How to end a letter", "text": "<p>How you end a letter is important. It\u2019s your last chance to leave the reader with positive feelings about you and the letter you have written. To make the matter more difficult, each different closing phrase has subtle connotations attached to them that you need to know to use them well.</p> <p>Most formal letter closing options are reserved, but note that there are degrees of warmth and familiarity among the options. Your relationship with the person to whom you\u2019re writing will shape which closing you choose:</p> <ul> <li>If you don\u2019t know the individual to whom you\u2019re writing, stick with     a professional formal closing.</li> <li>If you\u2019re writing to a colleague, business connection, or someone else you     know well, it\u2019s fine to close your letter less formally.</li> </ul> <p>Above all, your closing should be appropriate. Ideally, your message will resonate instead of your word choice.</p> <p>TL;DR: You can select from:</p> <ul> <li> <p>Simplest, most useful:</p> <ul> <li>Sincerely</li> <li>Regards</li> <li>Yours truly</li> <li>Yours sincerely</li> </ul> </li> <li> <p>Slightly more personal:</p> <ul> <li>Best regards</li> <li>Cordially</li> <li>Yours respectfully</li> </ul> </li> <li> <p>More personal: Only use when appropriate to the letter's content.</p> <ul> <li>Warm regards</li> <li>Best wishes</li> <li>With appreciation</li> </ul> </li> <li> <p>Letter closings to avoid:</p> <ul> <li>Always</li> <li>Cheers</li> <li>Love</li> <li>Take Care</li> <li>XOXO</li> </ul> </li> </ul> <p>The following are letter closings that are appropriate for business and employment related letters.</p> <ul> <li> <p>Sincerely, Regards, Yours truly, and Yours sincerely: These are the simplest     and most useful letter closings to use in a formal business setting. These     are appropriate in almost all instances and are excellent ways to close     a cover letter or an inquiry.</p> </li> <li> <p>Best regards, Cordially, and Yours respectfully: These letter closings fill     the need for something slightly more personal. They are appropriate once you     have some knowledge of the person to whom you are writing. You may have     corresponded via email a few times, had a face-to-face or phone interview,     or met at a networking event.</p> </li> <li> <p>Warm regards, Best wishes, and With appreciation: These letter closings are     also appropriate once you have some knowledge or connection to the person to     whom you are writing. Because they can relate back to the content of the     letter, they can give closure to the point of the letter. Only use these if     they make sense with the content of your letter.</p> </li> </ul>"}, {"location": "writing_style/#letter-closings-to-avoid", "title": "Letter closings to avoid", "text": "<p>There are certain closings that you want to avoid in any business letter. Most of these are simply too informal. Some examples of closings to avoid are listed below:</p> <p>Always, Cheers, Love, Take care, XOXO, Talk soon, See ya, Hugs</p> <p>Some closings (such as \u201cLove\u201d and \u201cXOXO\u201d) imply a level of closeness that is not appropriate for a business letter.</p> <p>Rule of thumb: if you would use the closing in a note to a close friend, it\u2019s probably not suitable for business correspondence.</p>"}, {"location": "writing_style/#punctuating-farewell-phrases", "title": "Punctuating Farewell Phrases", "text": "<p>When writing your sign-off, it's important to remember to use proper capitalization and punctuation. Only the first word should be capitalized (e.g., Yours truly), and the sign-off should be followed by a comma (or an exclamation mark in some informal settings), not a period.</p>"}, {"location": "writing_style/#postscripts", "title": "Postscripts", "text": "<p>A P.S. (or postscript) comes after your sign-off and name. It is meant to include material that is supplementary, subordinated, or not vital to your letter. It is best to avoid postscripts in formal writing, as the information may go unnoticed or ignored; in those cases, try to include all information in the body text of the letter.</p> <p>n casual and personal correspondences, a postscript is generally acceptable. However, try to limit it to include only humorous or unnecessary material.</p>"}, {"location": "writing_style/#letter-closings-in-detail", "title": "Letter closings in detail", "text": ""}, {"location": "writing_style/#sincerely", "title": "Sincerely", "text": "<p>Sincerely (or sincerely yours) is often the go-to sign off for formal letters, and with good reason. This ending restates the sincerity of your letter's intent; it is a safe choice if you are not overly familiar with the letter's recipient, as it's preferable to use a sign-off that is both common and formal in such a situation.</p>"}, {"location": "writing_style/#best", "title": "Best", "text": "<p>Ending your letter with best, all the best, all best, or best wishes indicates that you hope the recipient experiences only good things in the future. Although it is not quite as formal as sincerely, it is still acceptable as a polite, formal/semi-formal letter ending, proper for business contacts as well as friends.</p>"}, {"location": "writing_style/#best-regards", "title": "Best regards", "text": "<p>Quite like the previous sign-off, best regards expresses that you are thinking of the recipient with the best of feelings and intentions. Despite its similarity to best, this sign-off is a little more formal, meant for business letters and unfamiliar contacts. A semi-formal variation is warm regards, and an even more formal variation is simply regards.</p>"}, {"location": "writing_style/#speak-to-you-soon", "title": "Speak to you soon", "text": "<p>Variations to this farewell phrase include see you soon, talk to you later, and looking forward to speaking with you soon. These sign-offs indicate that you are expecting to continue the conversation with your contact. It can be an effective ending to a letter or email when confirming or planning a specific date for a face-to-face meeting.</p> <p>Although these endings can be used in either formal or casual settings, they typically carry a more formal tone. The exception here is talk to you later, which errs on the more casual side.</p>"}, {"location": "writing_style/#thanks", "title": "Thanks", "text": "<p>This is an effective ending to a letter when you are sincerely expressing gratitude. If you are using it as your standard letter ending, however, it can fall flat; the reader will be confused if there is no reason for you to be thanking them. Try to use thanks (or variations such as thanks so much, thank you, or thanks!) and its variations only when you think you haven't expressed your gratitude enough; otherwise, it can come across as excessive.</p> <p>Furthermore, when you're issuing an order, thanks might not be the best sign-off because it can seem presumptuous to offer thanks before the task has even been accepted or begun.</p>"}, {"location": "writing_style/#no-sign-off", "title": "[No sign-off]", "text": "<p>Having no sign-off for your letter is a little unusual, but it is acceptable in some cases. Omitting the sign-off is most appropriately used in cases where you are replying to an email chain. However, in a first email, including neither a sign-off nor your name will make your letter seem to end abruptly. It should be avoided in those situations or when you are not very familiar with the receiver.</p>"}, {"location": "writing_style/#yours-truly", "title": "Yours truly", "text": "<p>This is where the line between formal and informal begins to blur. Yours truly implies the integrity of the message that precedes your name, but it also implies that you are related to the recipient in some way.</p> <p>This ending can be used in various situations, when writing letters to people both familiar and unfamiliar to you; however, yours truly carries a more casual and familiar tone, making it most appropriate for your friends and family. It's best used when you want to emphasize that you mean the contents of your letter.</p>"}, {"location": "writing_style/#take-care", "title": "Take care", "text": "<p>Take care is also a semi-formal way to end your letter. Like the sign-off all the best, this ending wishes that no harm come to the reader; however, like ending your letter with yours truly, the word choice is less formal and implies that the writer is at least somewhat familiar with the reader.</p>"}, {"location": "writing_style/#cheers", "title": "Cheers", "text": "<p>Cheers is a lighthearted ending that expresses your best wishes for the reader. Due to its association with drinking alcohol, it's best to save this sign-off for cases where you are familiar with the reader and when the tone is optimistic and casual. Also note that because cheers is associated with British English, it may seem odd to readers who speak other styles of English and are not very familiar with the term.</p>"}, {"location": "writing_style/#style-issues", "title": "Style issues", "text": ""}, {"location": "writing_style/#avoid-there-is-at-the-start-of-the-sentence", "title": "Avoid there is at the start of the sentence", "text": "<p>Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.</p>"}, {"location": "writing_style/#writing-style-books", "title": "Writing style books", "text": "<p>After you start writing every day professionally, you will see that you will face some hard problems that will haunt you every time you sit down to write.</p> <p>The simplest way to overcome these issues and adopt a philosophy of writing that will make you a more professional, resilient, and wiser writer is to read the books about writing that masters of the craft have published.</p> <p>After reviewing 1, 2 and 3 I've come to the following list of books I'd like to read.</p>"}, {"location": "writing_style/#the-elements-of-style", "title": "The elements of style", "text": "<p>A classic book on grammar, style, and punctuation. If you feel like you need to improve any of those three aspects of your writing, then this book is a great start.</p> <p>With only 85 pages it covers both the grammar basics, rules that affect the style composition, writing toolbox description, and styling recommendations.</p>"}, {"location": "writing_style/#on-writing-well", "title": "On writing well", "text": "<p>They say this book is specially useful to find one's style, develop it, polish it and learn how to write with it.</p> <p>The author doesn't get too philosophical or cutesy in his concepts, neither he gets too technical. In a way, it provides the right balance between The Elements of Style and Bird by Bird. Reading the book feels like you\u2019re being mentored by a wise, highly experienced writer.</p>"}, {"location": "writing_style/#bird-by-bird", "title": "Bird by bird", "text": "<p>Supposedly the most touching, poetic, and psychological book of the collection.</p> <p>The first part of the book lays around the life of Anne Lamott, a relatively popular fiction writer, who happens to have had a quite interesting life. Just like On Writing (the first book mentioned in here), the author manages to share enough of her life to enlighten the story and thesis of the book.</p> <p>The author explains what it takes to be a writer, what it means to be one, and how you can develop a narrative for a fiction book or story.</p> <p>It looks like it's a real pleasure to read it at the same time as it\u2019s still a wonderful experience that will help you understand how you can overcome your own fears, doubts, and pains of writing.</p> <p>Whether you want to write fiction or nonfiction, Bird by Bird provides a beautiful reading experience that will teach you what it takes to be a writer and how to find your demons.</p>"}, {"location": "writing_style/#on-writing", "title": "On writing", "text": "<p>It's a book written by Stephen King that even though I haven't read any of his books I know he is known for being a specialist in capturing the reader. I don't know if it's going to be too much oriented to writing novels, but it looks promising.</p> <p>I'll leave it there for now, but keep on reading on Ivan Kreimer's article for more suggestions.</p>"}, {"location": "writing_style/#references", "title": "References", "text": "<ul> <li>Ivan Kreimer's article</li> </ul>"}, {"location": "yamlfix/", "title": "Yamlfix", "text": "<p>Yamlfix is a simple opinionated yaml formatter that keeps your comments.</p>"}, {"location": "yamlfix/#install", "title": "Install", "text": "<pre><code>pip install yamlfix\n</code></pre>"}, {"location": "yamlfix/#usage", "title": "Usage", "text": "<p>Imagine we've got the following source code:</p> <pre><code>book_library:\n- title: Why we sleep\n  author: Matthew Walker\n- title: Harry Potter and the Methods of Rationality\n  author: Eliezer Yudkowsky\n</code></pre> <p>It has the following errors:</p> <ul> <li>There is no <code>---</code> at the top.</li> <li>The indentation is all wrong.</li> </ul> <p>After running <code>yamlfix</code> the resulting source code will be:</p> <pre><code>---\nbook_library:\n  - title: Why we sleep\n    author: Matthew Walker\n  - title: Harry Potter and the Methods of Rationality\n    author: Eliezer Yudkowsky\n</code></pre> <p><code>yamlfix</code> can be used both as command line tool and as a library.</p> <ul> <li> <p>As a command line tool:</p> <pre><code>$: yamlfix file.yaml\n</code></pre> </li> <li> <p>As a library:</p> <pre><code>from yamlfix import fix_files\n\nfix_files(['file.py'])\n</code></pre> </li> </ul>"}, {"location": "yamlfix/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "uebungen/beispiel/", "title": "beispiel", "text": ""}, {"location": "uebungen/beispiel/#ueberschrift-1", "title": "ueberschrift 1", "text": ""}, {"location": "uebungen/beispiel/#bla", "title": "bla", "text": "<p>Black Bean Beef</p>"}]}